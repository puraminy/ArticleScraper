
    <html>
      <head><title>HTML Pandas Dataframe with CSS</title></head>
      <link rel="stylesheet" type="text/css" href="df_style.css"/>
      <body>
        
    <table border="1" class="dataframe mystyle">
    <tr>
        <td colspan=2>Conference</td><td><a href="https://link.springer.com/conference/semweb">ISWC</a></td>
    </tr>
    <tr>
        <td colspan=2>Search terms</td><td>none</td>
    </tr>
    <tr>
        <td colspan=2>Percentage</td><td>1122 of 1122 = 100.0%</td>
    </tr>
    <tr>
        <td colspan=2>From</td><td>all</td>
    </tr>
    <tr>
    <td colspan=3>Papers</td>
    </tr>
    <tr><td>0</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_42">Browsing Schedules - An Agent-Based Approach to Navigating the Semantic Web</a></td></tr><tr><td colspan=3><p>The Semantic Web promises to change the way agents navigate, harvest and utilize information on the internet. By providing a structured, distributed representation for expressing concepts and relationships defined by multiple ontologies, it is now possible for agents to </p></td></tr><tr><td>1</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_41">WebTheme™: Understanding Web Information through Visual Analytics</a></td></tr><tr><td colspan=3><p>WebTheme combines the power of software agent-based information retrieval with visual analytics to provide users with a new tool for understanding web information. WebTheme allows users to both quickly comprehend large collections of information from the Web and drill down into interesting portions of a collection. Software agents work for users to perform controlled harvesting of web material of interest. Visualization and analysis tools allow exploration of the resulting document space. Information spaces are organized and presented according to their topical context. Tools that display how documents were collected by the agents, where they were gathered, and how they are linked further enhance users’ understanding of information and its context. WebTheme is a significant tool in the pursuit of the Semantic Web. In particular, it supports enhanced user insight into semantics of large, prestructured or ad-hoc, web information collections.</p></td></tr><tr><td>2</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_40">Consistency Checking of Semantic Web Ontologies</a></td></tr><tr><td colspan=3><p>Ensuring that ontologies are consistent is an important part of ontology development and testing. This is especially important when autonomous software agents are to use ontologies in their reasoning. Reasoning with inconsistent ontologies may lead to erroneous conclusions. In this paper we introduce the ConsVISor tool for consistency checking of ontologies. This tool is a consistency checker for formal ontologies, including both traditional data modeling languages and the more recent ontology languages. ConsVISor checks consistency by verifying axioms. ConsVISor is part of the UBOT toolkit that uses a variety of techniques such as theorem proving and logic programming. Some examples of the use of these tools are given.</p></td></tr><tr><td>3</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_39">Content Based Fake News Detection Using Knowledge Graphs</a></td></tr><tr><td colspan=3><p>This paper addresses the problem of fake news detection. There are many works already in this space; however, most of them are for social media and not using news content for the decision making. In this paper, we propose some novel approaches, including the B-TransE model, to detecting fake news based on news content using knowledge graphs. In our solutions, we need to address a few technical challenges. Firstly, computational-oriented fact checking is not comprehensive enough to cover all the relations needed for fake news detection. Secondly, it is challenging to validate the correctness of the extracted triples from news articles. Our approaches are evaluated with the Kaggle’s ‘Getting Real about Fake News’ dataset and some true articles from main stream media. The evaluations show that some of our approaches have over 0.80 F1-scores.</p></td></tr><tr><td>4</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_8">Using Semantic Web Technologies for Representing E-science Provenance</a></td></tr><tr><td colspan=3><p>Life science researchers increasingly rely on the web as a primary source of data, forcing them to apply the same rigor to its use as to an experiment in the laboratory. The </p></td></tr><tr><td>5</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_39">Is Participation in the Semantic Web Too Difficult?</a></td></tr><tr><td colspan=3><p>As long as there is not a sufficient base of RDF-annotated pages, the benefits of participating in the Semantic Web are barely visible. This is true in particular for content providers like individuals or small institutions. These potential participants can’t afford the additional work necessary for the Semantic Web, yet they’re needed for the Semantic Web to reach the critical mass that will make it a success. This paper discusses problems that may prevent small content providers from participating in the Semantic Web, as well as a possible way to lower the barrier for entry using tools like our own Information Layer system.</p></td></tr><tr><td>6</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_49">Specifying Ontology Views by Traversal</a></td></tr><tr><td colspan=3><p>One of the original motivations behind ontology research was the belief that ontologies can help with reuse in knowledge representation. However, many of the ontologies that are developed with reuse in mind, such as standard reference ontologies and controlled terminologies, are extremely large, while the users often need to reuse only a small part of these resources in their work. Specifying various views of an ontology enables users to limit the set of concepts that they see. In this paper, we develop the concept of a </p></td></tr><tr><td>7</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_28">A Conceptual Architecture for Semantic Web Services</a></td></tr><tr><td colspan=3><p>In this paper, we present an abstract conceptual architecture for semantic web services. We define requirements on the architecture by analyzing a set of case studies developed as part of the EU Semantic Web-enabled Web Services project. The architecture is developed as a refinement and extension of the W3C Web Services Architecture. We assess our architecture against the requirements, and provide an analysis of OWL-S.</p></td></tr><tr><td>8</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_24">Optimizing QoS-Aware Semantic Web Service Composition</a></td></tr><tr><td colspan=3><p>Ranking and optimization of web service compositions are some of the most interesting challenges at present. Since web services can be enhanced with formal semantic descriptions, forming the “semantic web services”, it becomes conceivable to exploit the quality of semantic links between services (of any composition) as one of the optimization criteria. For this we propose to use the semantic similarities between output and input parameters of web services. Coupling this with other criteria such as quality of service (QoS) allow us to rank and optimize compositions achieving the same goal. Here we suggest an innovative and extensible optimization model designed to balance semantic fit (or functional quality) with non-functional QoS metrics. To allow the use of this model in the context of a large number of services as foreseen by the strategic EC-funded project SOA4All we propose and test the use of Genetic Algorithms.</p></td></tr><tr><td>9</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_49">An Unsupervised Model for Exploring Hierarchical Semantics from Social Annotations</a></td></tr><tr><td colspan=3><p>This paper deals with the problem of exploring hierarchical semantics from social annotations. Recently, social annotation services have become more and more popular in Semantic Web. It allows users to arbitrarily annotate web resources, thus, largely lowers the barrier to cooperation. Furthermore, through providing abundant meta-data resources, social annotation might become a key to the development of Semantic Web. However, on the other hand, social annotation has its own apparent limitations, for instance, 1) ambiguity and synonym phenomena and 2) lack of hierarchical information. In this paper, we propose an unsupervised model to automatically derive hierarchical semantics from social annotations. Using a social bookmark service Del.icio.us as example, we demonstrate that the derived hierarchical semantics has the ability to compensate those shortcomings. We further apply our model on another data set from Flickr to testify our model’s applicability on different environments. The experimental results demonstrate our model’s efficiency.</p></td></tr><tr><td>10</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_45">Complete Query Answering over Horn Ontologies Using a Triple Store</a></td></tr><tr><td colspan=3><p>In our previous work, we showed how a scalable OWL 2 RL reasoner can be used to compute both lower and upper bound query answers over very large datasets and arbitrary OWL 2 ontologies. However, when these bounds do not coincide, there still remain a number of possible answer tuples whose status is not determined. In this paper, we show how in the case of Horn ontologies one can exploit the lower and upper bounds computed by the RL reasoner to efficiently identify a subset of the data and ontology that is large enough to resolve the status of these tuples, yet small enough so that the status can be computed using a fully-fledged OWL 2 reasoner. The resulting hybrid approach has enabled us to compute exact answers to queries over datasets and ontologies where previously only approximate query answering was possible.</p></td></tr><tr><td>11</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_1">Leveraging the Semantics of Tweets for Adaptive Faceted Search on Twitter</a></td></tr><tr><td colspan=3><p>In the last few years, Twitter has become a powerful tool for publishing and discussing information. Yet, content exploration in Twitter requires substantial effort. Users often have to scan information streams by hand. In this paper, we approach this problem by means of faceted search. We propose strategies for inferring facets and facet values on Twitter by enriching the semantics of individual Twitter messages (tweets) and present different methods, including personalized and context-adaptive methods, for making faceted search on Twitter more effective. We conduct a large-scale evaluation of faceted search strategies, show significant improvements over keyword search and reveal significant benefits of those strategies that (i) further enrich the semantics of tweets by exploiting links posted in tweets, and that (ii) support users in selecting facet value pairs by adapting the faceted search interface to the specific needs and preferences of a user.</p></td></tr><tr><td>12</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_38">How Semantic Technologies Can Enhance Data Access at Siemens Energy</a></td></tr><tr><td colspan=3><p>We present a description and analysis of the data access challenge in the Siemens Energy. We advocate for Ontology Based Data Access (OBDA) as a suitable Semantic Web driven technology to address the challenge. We derive requirements for applying OBDA in Siemens, review existing OBDA systems and discuss their limitations with respect to the Siemens requirements. We then introduce the Optique platform as a suitable OBDA solution for Siemens. Finally, we describe our preliminary installation and evaluation of the platform in Siemens.</p></td></tr><tr><td>13</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_25">Learning Relational Bayesian Classifiers from RDF Data</a></td></tr><tr><td colspan=3><p>The increasing availability of large RDF datasets offers an exciting opportunity to use such data to build predictive models using machine learning algorithms. However, the massive size and distributed nature of RDF data calls for approaches to learning from RDF data in a setting where the data can be accessed only through a query interface, e.g., the SPARQL endpoint of the RDF store. In applications where the data are subject to frequent updates, there is a need for algorithms that allow the predictive model to be incrementally updated in response to changes in the data. Furthermore, in some applications, the attributes that are relevant for specific prediction tasks are not known a priori and hence need to be discovered by the algorithm. We present an approach to learning Relational Bayesian Classifiers (RBCs) from RDF data that addresses such scenarios. Specifically, we show how to build RBCs from RDF data using statistical queries through the SPARQL endpoint of the RDF store. We compare the communication complexity of our algorithm with one that requires direct centralized access to the data and hence has to retrieve the entire RDF dataset from the remote location for processing. We establish the conditions under which the RBC models can be incrementally updated in response to addition or deletion of RDF data. We show how our approach can be extended to the setting where the attributes that are relevant for prediction are not known a priori, by selectively crawling the RDF data for attributes of interest. We provide open source implementation and evaluate the proposed approach on several large RDF datasets.</p></td></tr><tr><td>14</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_38">Publishing Without Publishers: A Decentralized Approach to Dissemination, Retrieval, and Archiving of Data</a></td></tr><tr><td colspan=3><p>Making available and archiving scientific results is for the most part still considered the task of classical publishing companies, despite the fact that classical forms of publishing centered around printed narrative articles no longer seem well-suited in the digital age. In particular, there exist currently no efficient, reliable, and agreed-upon methods for publishing scientific datasets, which have become increasingly important for science. Here we propose to design scientific data publishing as a Web-based bottom-up process, without top-down control of central authorities such as publishing companies. Based on a novel combination of existing concepts and technologies, we present a server network to decentrally store and archive data in the form of nanopublications, an RDF-based format to represent scientific data. We show how this approach allows researchers to publish, retrieve, verify, and recombine datasets of nanopublications in a reliable and trustworthy manner, and we argue that this architecture could be used for the Semantic Web in general. Evaluation of the current small network shows that this system is efficient and reliable.</p></td></tr><tr><td>15</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_23">Collaborative Filtering by Analyzing Dynamic User Interests Modeled by Taxonomy</a></td></tr><tr><td colspan=3><p>Tracking user interests over time is important for making accurate recommendations. However, the widely-used time-decay-based approach worsens the sparsity problem because it deemphasizes old item transactions. We introduce two ideas to solve the sparsity problem. First, we divide the users’ transactions into epochs i.e. time periods, and identify epochs that are dominated by interests similar to the current interests of the active user. Thus, it can eliminate dissimilar transactions while making use of similar transactions that exist in prior epochs. Second, we use a taxonomy of items to model user item transactions in each epoch. This well captures the interests of users in each epoch even if there are few transactions. It suits the situations in which the items transacted by users dynamically change over time; the semantics behind classes do not change so often while individual items often appear and disappear. Fortunately, many taxonomies are now available on the web because of the spread of the Linked Open Data vision. We can now use those to understand dynamic user interests semantically. We evaluate our method using a dataset, a music listening history, extracted from users’ tweets and one containing a restaurant visit history gathered from a gourmet guide site. The results show that our method predicts user interests much more accurately than the previous time-decay-based method.</p></td></tr><tr><td>16</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_39">Provenance for SPARQL Queries</a></td></tr><tr><td colspan=3><p>Determining trust of data available in the Semantic Web is fundamental for applications and users, in particular for linked open data obtained from SPARQL endpoints. There exist several proposals in the literature to annotate SPARQL query results with values from abstract models, adapting the seminal works on provenance for annotated relational databases. We provide an approach capable of providing provenance information for a large and significant fragment of SPARQL 1.1, including for the first time the major non-monotonic constructs under multiset semantics. The approach is based on the translation of SPARQL into relational queries over annotated relations with values of the most general m-semiring, and in this way also refuting a claim in the literature that the </p></td></tr><tr><td>17</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_37">Schema-Agnostic Query Rewriting in SPARQL 1.1</a></td></tr><tr><td colspan=3><p>SPARQL 1.1 supports the use of ontologies to enrich query results with logical entailments, and OWL 2 provides a dedicated fragment OWL QL for this purpose. Typical implementations use the OWL QL schema to rewrite a conjunctive query into an equivalent set of queries, to be answered against the non-schema part of the data. With the adoption of the recent SPARQL 1.1 standard, however, RDF databases are capable of answering much more expressive queries directly, and we ask how this can be exploited in query rewriting. We find that SPARQL 1.1 is powerful enough to “implement” a full-fledged OWL QL reasoner in a single query. Using additional SPARQL 1.1 features, we develop a new method of schema-agnostic query rewriting, where arbitrary conjunctive queries over OWL QL are rewritten into equivalent SPARQL 1.1 queries in a way that is fully independent of the actual schema. This allows us to query RDF data under OWL QL entailment without extracting or preprocessing OWL axioms.</p></td></tr><tr><td>18</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_38">Hybrid SPARQL Queries: Fresh vs. Fast Results</a></td></tr><tr><td colspan=3><p>For Linked Data query engines, there are inherent trade-offs between centralised approaches that can efficiently answer queries over data cached from parts of the Web, and live decentralised approaches that can provide fresher results over the entire Web at the cost of slower response times. Herein, we propose a </p></td></tr><tr><td>19</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_37">Blank Node Matching and RDF/S Comparison Functions</a></td></tr><tr><td colspan=3><p>In RDF, a </p></td></tr><tr><td>20</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_5">LDQL: A Query Language for the Web of Linked Data</a></td></tr><tr><td colspan=3><p>The Web of Linked Data is composed of tons of RDF documents interlinked to each other forming a huge repository of distributed semantic data. Effectively querying this distributed data source is an important open problem in the Semantic Web area. In this paper, we propose LDQL, a declarative language to query Linked Data on the Web. One of the novelties of LDQL is that it expresses separately (i) patterns that describe the expected query result, and (ii)Web navigation paths that select the data sources to be used for computing the result. We present a formal syntax and semantics, prove equivalence rules, and study the expressiveness of the language. In particular, we show that LDQL is strictly more expressive than the query formalisms that have been proposed previously for Linked Data on the Web. The high expressiveness allows LDQL to define queries for which a complete execution is not computationally feasible over the Web. We formally study this issue and provide a syntactic sufficient condition to avoid this problem; queries satisfying this condition are ensured to have a procedure to be effectively evaluated over the Web of Linked Data.</p></td></tr><tr><td>21</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_49">WWW: WSMO, WSML, and WSMX in a Nutshell</a></td></tr><tr><td colspan=3><p>This paper presents, in a nutshell, a unifying framework for conceptually modeling, formally representing, and executing Semantic Web services. We first introduce a conceptual model for representing Semantic Web services and its design principles, then we present a language based on different logical formalisms used to express Semantic Web services that are compliant with our conceptual model. Finally, a high level overview of an execution environment, and its relations to the conceptual model and the language introduced in this paper, are presented.</p></td></tr><tr><td>22</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_48">Web Services Analysis: Making Use of Web Service Composition and Annotation</a></td></tr><tr><td colspan=3><p>Automated Web service composition and automated Web service annotation could be seen as complimentary methodologies. While automated annotation allows to extract Web service semantics from existing WSDL documents, automated composition uses this semantics for integrating applications. Therefore applicability of both methodologies is essential for increasing the productivity of information system integration. Although several papers have proposed methods for automated annotation, there is a lack of studies providing analysis of the general structure of Web services. We argue that having an overview of general Web services structures would greatly improve design of new annotation methods. At the same time, progress in automated composition has resulted in several methods for automating Web services orchestration. In this paper we propose application of automated composition also for analysing Web services domain. We identify and analyse some general Web services properties and provide their interpretation in an industrial context.</p></td></tr><tr><td>23</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_47">A Semantic Rewriting Approach to Automatic Information Providing Web Service Composition</a></td></tr><tr><td colspan=3><p>Much work has been done on automatic information providing Web Service discovery and composition, such as the query rewriting approaches proposed by the database community and planning methods in semantic web research. This paper studies the problem of semantic information providing Web Service composition. More specifically, we propose a new method to represent the semantic information providing Web Services in the CARIN language, which seamlessly integrates both database and semantic web technologies. Then, a semantic rewriting based framework and algorithm are proposed to compose the Web Services. Through a case study we show that the new method could find more compositions compared with both query rewriting and planning based Web Service composition methods.</p></td></tr><tr><td>24</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_45">Toward Automatic Discovery and Invocation of Information-Providing Web Services</a></td></tr><tr><td colspan=3><p>Semantic Web makes the automatic discovery and invocation of Web Services become possible. But existing methods perform the capability matching, which is crucial for service discovery, either only according to inputs and outputs (IO), which results in a not very precise matching, or trying to tackle arbitrary services, which results in an undecidable reasoning. In this paper, targeting merely the information-providing type of Web Services, we present a precise and decidable matching method based on the Description Logic reasoner. An outstanding property of our method is that it can determinate the accurate binding of IO between requested and advertised services, which is necessary for automatic invocation yet rarely addressed in previous work. Besides, this paper also describes a useful use case for automatic Web Services discovery and invocation.</p></td></tr><tr><td>25</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_44">Mediation Enabled Semantic Web Services Usage</a></td></tr><tr><td colspan=3><p>The Semantic Web services has become a challenging research topic in the last half of decade. Various frameworks offer means to semantically describe all the related aspects of Semantic Web services, but the solutions to the heterogeneity problems, inherent in a distributed environment such as the Web, are still to be properly integrated and referred to from the main phases of the Web services usage. Both data and process heterogeneity, as well as the multitude of functionalities required and offered by semantic Web services’ requesters and providers hamper the usability of Web services, making this technology difficult to use. This paper emphasizes the role of mediators in a Semantic Web services architecture, illustrating how the mediators can enable the Semantic Web services usages in operations like discovery, invocation and composition.</p></td></tr><tr><td>26</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_46">Automatic Composition of Semantic Web Services – A Theorem Proof Approach</a></td></tr><tr><td colspan=3><p>This paper proposes a method to automatically generate composite services. The function of a service is specified by its Inputs, Output, Preconditions, and Result (IOPR). These functional descriptions are translated into a first-order logic (FOL) formula. An Automatic Theorem Prover (ATP) is used to generate a proof from known services (as axioms) to the composite service (as an object formula). Based on the deductive program synthesis theory, the implementation of the composite service is extracted from the proof. The “proof to program” method used here guarantees the completeness and correctness of the result. An brief introduction of the prototype system is given.</p></td></tr><tr><td>27</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_41">Scalable Geo-thematic Query Answering</a></td></tr><tr><td colspan=3><p>First order logic (FOL) rewritability is a desirable feature for query answering over geo-thematic ontologies because in most geo-processing scenarios one has to cope with large data volumes. Hence, there is a need for combined geo-thematic logics that provide a sufficiently expressive query language allowing for FOL rewritability. The DL-Lite family of description logics is tailored towards FOL rewritability of query answering for unions of conjunctive queries, hence it is a suitable candidate for the thematic component of a combined geo-thematic logic. We show that a weak coupling of DL-Lite with the expressive region connection calculus RCC8 allows for FOL rewritability under a spatial completeness condition for the ABox. Stronger couplings allowing for FOL rewritability are possible only for spatial calculi as weak as the low-resolution calculus RCC2. Already a strong combination of DL-Lite with the low-resolution calculus RCC3 does not allow for FOL rewritability.</p></td></tr><tr><td>28</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_43">Exploring the Flexible Workflow Technology to Automate Service Composition</a></td></tr><tr><td colspan=3><p>Most of the current workflow-based service composition frameworks and systems require processes to be predefined and services to be statically-bound, thus lacking necessary flexibility to adapt to frequent changes arising from domain/business/user rules and the dynamic Internet environment. This paper proposes a service composition framework based on a flexible workflow method, which enables a part of a process to be created by automatic service composition. In this paper, we propose a semi-automatic service composition framework which enables a part of a process to be created by automatic service composition. In this framework, we encapsulate those uncertain, dynamic and variable parts of a process into black-boxes with a set of rules at the modeling phase. While at the executing phrase, black-boxes are concretized by composing services according to the predefined rules automatically. This framework has been implemented in DartFlow-a service composition platform for the sharing of the TCM (Tradition Chinese Medicine) knowledge and services.</p></td></tr><tr><td>29</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_41">HStar – A Semantic Repository for Large Scale OWL Documents</a></td></tr><tr><td colspan=3><p>HStar is implemented to support large scale OWL documents management. Physical storage model is designed on file system based on semantic model of OWL data. Inference and query are implemented on such physical storage model. Now HStar supports characters of OWL Lite and we try to adopt strategy of partial materializing inference data, which is different from most of existing semantic repository systems. In this paper we first give the data model which HStar supports, then give an analysis of our inference strategy; storage model and query process are discussed in detail; experiments for comparing HStar and related systems are given at last.</p></td></tr><tr><td>30</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_42">Minerva: A Scalable OWL Ontology Storage and Inference System</a></td></tr><tr><td colspan=3><p>With the increasing use of ontologies in Semantic Web and enterprise knowledge management, it is critical to develop scalable and efficient ontology management systems. In this paper, we present Minerva, a storage and inference system for large-scale OWL ontologies on top of relational databases. It aims to meet scalability requirements of real applications and provide practical reasoning capability as well as high query performance. The method combines Description Logic reasoners for the TBox inference with logic rules for the ABox inference. Furthermore, it customizes the database schema based on inference requirements. User queries are answered by directly retrieving materialized results from the back-end database. The effective integration of ontology inference and storage is expected to improve reasoning efficiency, while querying without runtime inference guarantees satisfactory response time. Extensive experiments on University Ontology Benchmark show the high efficiency and scalability of Minerva system.</p></td></tr><tr><td>31</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_40">Automatic Creation and Simplified Querying of Semantic Web Content: An Approach Based on Information-Extraction Ontologies</a></td></tr><tr><td colspan=3><p>The semantic web represents a major advance in web utility, but it is currently difficult to create semantic-web content because pages must be semantically annotated through processes that are mostly manual and require a high degree of engineering skill. Furthermore, users need an effective way to query the semantic web, but any burden placed on users to learn a query language is unlikely to garner sufficient user support and interest. Unfortunately, both the creation and use of semantic-web pages are difficult, and these are precisely the processes that must be made simple in order for the semantic web to truly succeed. We propose using information-extraction ontologies to handle both of these challenges. In this paper we show how a successful ontology-based data-extraction technique can (1) automatically generate semantic annotations for ordinary web pages, and (2) support free-form, textual queries that will be relatively simple for end users to write.</p></td></tr><tr><td>32</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_38">Document Filtering for Domain Ontology Based on Concept Preferences</a></td></tr><tr><td colspan=3><p>For domain ontology construction and expansion, data-driven approaches based on web resources have been actively investigated. Despite the importance of document filtering for domain ontology management, however, few studies have sought to develop a method for automatically filtering out domain-relevant documents from the web. To address this situation, here we propose a document filtering scheme that identifies documents relevant to a domain ontology based on concept preferences. Testing of the proposed filtering scheme with a business domain ontology on 1,409 YahooPicks web pages yielded promising filtering results that outperformed the baseline system.</p></td></tr><tr><td>33</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_40">SRBench: A Streaming RDF/SPARQL Benchmark</a></td></tr><tr><td colspan=3><p>We introduce </p></td></tr><tr><td>34</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_39">Qualitative Spatial Relation Database for Semantic Web</a></td></tr><tr><td colspan=3><p>Geospatial Semantic Web (GSW) has become one of the most prominent research themes in geographic information science over the last few years. The traditional spatial database stores the quantitative data such as coordinate, while GSW needs much qualitative information such as spatial relation. The previous qualitative spatio-temporal systems were all prototype systems which did not support general spatio-temporal relation model and data input. We design the qualitative spatial relation database (QSRDB) based on spatial reasoning. GML data can be converted to QSRDB as data input. OWL ontologies can be generated from QSRDB and applied to GSW systems.</p></td></tr><tr><td>35</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_37">A Semantic Search Conceptual Model and Application in Security Access Control</a></td></tr><tr><td colspan=3><p>We propose a conceptual model for semantic search and implement it in security access control. The model provides security access control to extend the search capabilities. The scalable model can integrate other ontology providing the general ontology as the transformation interface. We combine text Information Retrieval (IR) with semantic inference in the model. So it can not only search the resources and the relationships between them according to the user’s privileges, but also locate the exact resource using text IR. We build a security ontology based on Role-Based Access Control (RBAC) policy. A semantic search system Onto-SSSE is implemented based on the model. The system can perform some complex queries using ontology reasoning, especially about association queries such as the relationships between resources. The evaluation shows that the new system performs better than exiting methods.</p></td></tr><tr><td>36</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_35">A Semantics-Based Protocol for Business Process Transactions</a></td></tr><tr><td colspan=3><p>A Business Process Management System (BPMS) requires transaction management to guarantee reliability of the business process transactions. Several transaction protocols have been suggested for the transaction management, but they are heterogeneous. This heterogeneity interrupts message exchanges among BPMSs which use different transaction protocols, so that the interoperability among the BPMSs cannot be guaranteed. To solve this problem, this paper suggests a semantics-based protocol for business process transactions. The suggested protocol is composed of the static semantics and the operational semantics. In the context of the static semantics, transaction states and messages are defined using the Web Ontology Language (OWL). In the context of the operational semantics, state transitions of business process transactions are defined using the Abstract State Machine (ASM). The suggested approach is expected to enhance interoperability among heterogeneous BPMSs, to increase the understandability for the transaction protocols, and to support automatic transaction execution and systematic transaction monitoring.</p></td></tr><tr><td>37</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_36">Fuzzy View-Based Semantic Search</a></td></tr><tr><td colspan=3><p>This paper presents a fuzzy version of the semantic view-based search paradigm. Our framework contributes to previous work in two ways: First, the fuzzification introduces the notion of relevance to view-based search by enabling the ranking of search results. Second, the framework makes it possible to separate the end-user’s views from content indexer’s taxonomies or ontologies. In this way, search queries can be formulated and results organized using intuitive categories that are different from the semantically complicated indexing concepts. The fuzziness is the result of allowing more accurate weighted annotations and fuzzy mappings between search categories and annotation ontologies. A prototype implementation of the framework is presented and its application to a data set in a semantic eHealth portal discussed.</p></td></tr><tr><td>38</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_34">Applying CommonKADS and Semantic Web Technologies to Ontology-Based E-Government Knowledge Systems</a></td></tr><tr><td colspan=3><p>Government agencies are the largest owners of knowledge assets such as regulations, documents, forms. To build a knowledge-based system (KBS) for e-government has proved to be an effective way to enhance the efficiency of handling governmental services. However, few efforts are made to address automatic reasoning of knowledge-intensive tasks within e-government processes. For this purpose, we present an approach to building an e-government KBS by using the CommonKADS, a knowledge-engineering methodology, and semantic web technologies (OWL, SWRL, OWL-S), with the aiming of automatically solving knowledge-intensive tasks within e-governmental services. Our experiences show that the CommonKADS is crucial to the analysis and identification of knowledge-intensive tasks within government processes, whereas the semantic web technologies enable the refinement of domain ontologies, domain rules and task methods.</p></td></tr><tr><td>39</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_33">Ontology Driven Visualisation of Maps with SVG – Technical Aspects</a></td></tr><tr><td colspan=3><p>This article describes the technical aspects of a particular use of ontologies for visualising maps in a browser window. Geographic data are represented as instances of concepts in an ontology of transportation networks which was designed in close relation to GDF. These data are transformed into SVG, whereas the transformation is specified symbolically as instances of a </p></td></tr><tr><td>40</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_29">A Reasoning Algorithm for pD*</a></td></tr><tr><td colspan=3><p>pD* semantics extends the ‘if-semantics’ of RDFS to a subset of the OWL vocabulary. It leads to simple entailment rules and relatively low computational complexity for reasoning. In this paper, we propose a forward-chaining reasoning algorithm to support RDFS entailments under the pD* semantics. This algorithm extends the Sesame algorithm to cope with the pD* entailments. In particular, an optimization to the dependent table between entailment rules is presented to eliminate much redundant inferring steps. Finally, some test results are given to illustrate the correctness and performance of this algorithm.</p></td></tr><tr><td>41</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_32">Dental Decision Making on Missing Tooth Represented in an Ontology and Rules</a></td></tr><tr><td colspan=3><p>The Web Ontology Language (OWL), which is a Description Logic based ontology language, is widely used to represent formal definitions of vocabularies for domain knowledge, especially in the medical domain. The Semantic Web Rule Language (SWRL), which is a Rule based ontology language, allows users to take advantage of inferencing of new knowledge from existing OWL knowledge base. In this paper, we describe a use case focused on building SWRL rule base on top of the tooth positional ontology represented in OWL so as to assist a dental decision-making on a missing tooth. Then, we discuss limitations of a current SWRL specification, through our experiences on converting dental knowledge into SWRL rules.</p></td></tr><tr><td>42</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_30">Triple Space Computing: Adding Semantics to Space-Based Computing</a></td></tr><tr><td colspan=3><p>Triple Space Computing (TSC) is a very simple and powerful paradigm that inherits the communication model from Tuple Space Computing and projects it in the context of the Semantic Web. In this paper, we propose Triple Space Computing as a new communication and coordination framework for Semantic Web and Semantic Web Services. We identify the value added by TSC and propose the overall architecture of TSC and the interactions among different components.</p></td></tr><tr><td>43</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_31">Full-Automatic High-Level Concept Extraction from Images Using Ontologies and Semantic Inference Rules</a></td></tr><tr><td colspan=3><p>One of the big issues facing current content-based image retrieval is how to automatically extract the semantic information from images. In this paper, we propose an efficient method that automatically extracts the semantic information from images by using ontologies and the semantic inference rules. In our method, MPEG-7 visual descriptors are used to extract the visual features of image which are mapped to the semi-concept values. We also introduce the visual and animal ontology which are built to bridge the semantic gap. The visual ontology facilitates the mapping between visual features and semi-concept values, and allows the definition of relationships between the classes describing the visual features. The animal ontology representing the animal taxonomy can be exploited to identify the object in an image. We also propose the semantic inference rules that can be used to automatically extract high-level concepts from images by applying them to the visual and animal ontology. Finally, we discuss the limitations of the proposed method and the future work.</p></td></tr><tr><td>44</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_26">Representing and Reasoning with Application Profiles Based on OWL and OWL/XDD</a></td></tr><tr><td colspan=3><p>An application profile specifies a set of terms, drawn from one or more standard namespaces, for annotation of data, and constrains their usage and interpretations in a particular local application. A framework for representing and reasoning with application profiles using the OWL and OWL/XDD languages is proposed. The former is a standard Web ontology language and the latter is a definite-clause-style rule language that employs XML expressions as its underlying data structure. Constraints are defined in terms of rules, which are represented as XDD clauses. Application of the approach to defining an application profile with fine-grained semantic constraints is illustrated. A prototype library metadata validation system has been implemented.</p></td></tr><tr><td>45</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_24">Application Integration Using Conceptual Spaces (CSpaces)</a></td></tr><tr><td colspan=3><p>Application integration is a complex problem that consumes a significant share of the IT budget of many companies and organizations. </p></td></tr><tr><td>46</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_28">Visualizing Defeasible Logic Rules for the Semantic Web</a></td></tr><tr><td colspan=3><p>Defeasible reasoning is a rule-based approach for efficient reasoning with incomplete and conflicting information. Such reasoning is useful in many Semantic Web applications, like policies, business rules, brokering, bargaining and agent negotiations. Nevertheless, defeasible logic is based on solid mathematical formulations and is, thus, not fully comprehensible by end users, who often need graphical trace and explanation mechanisms for the derived conclusions. Directed graphs can assist in confronting this drawback. They are a powerful and flexible tool of information visualization, offering a convenient and comprehensible way of representing relationships between entities. Their applicability, however, is balanced by the fact that it is difficult to associate data of a variety of types with the nodes and the arcs in the graph. In this paper we try to utilize digraphs in the graphical representation of defeasible rules, by exploiting the expressiveness and comprehensibility they offer, but also trying to leverage their major disadvantage, by defining two distinct node types, for rules and atomic formulas, and four distinct connection types for each rule type in defeasible logic and for superiority relationships. The paper also briefly presents a tool that implements this representation methodology.</p></td></tr><tr><td>47</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_23">Semantic Integration of Enterprise Information: Challenges and Basic Principles</a></td></tr><tr><td colspan=3><p>To overcome the challenges of EII (Enterprise Information integration), we propose SGII which is the first system undertaking research at the intersection of semantic grid and P2P data integration, exploiting their strengths in a common framework, and expanding their applicability in the area of EII. We first discuss how the P2P and semantic grid technologies can drive current EII systems to a new decentralized, flexible, scalable system based on a short survey of the state of the art of EII and its current challenges. Then, through a discussion of the fundamental formal architecture in general and its components in particular, we depict the basic integration principles from both P2P and semantic grid perspectives. The key contributions of this paper are a P2P semantic grid service oriented framework for EII, which mainly consists of three semantic grid services (data peer, semantic peer and application peer services); basic integration principles, which is compatible with OGSA-DAI infrastructure and P2P data integration paradigm; and added value over the state of the art of EII.</p></td></tr><tr><td>48</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_25">A New Evaluation Method for Ontology Alignment Measures</a></td></tr><tr><td colspan=3><p>Various methods using different measures have been proposed for ontology alignment. Therefore, it is necessary to evaluate the effectiveness of such measures to select better ones for more quality alignment. Current approaches for comparing these measures, are highly dependent on alignment frameworks, which may cause unreal results. In this paper, we propose a framework independent evaluation method, and discuss results of applying it to famous existing string measures.</p></td></tr><tr><td>49</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_21">An Ontology Architecture for Integration of Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies are expected in various areas as promising tools to improve communication among people and to achieve interoperability among systems. For communications between different business domains, building an ontology through integrating existing ontologies is more efficient way than building the ontology without them. However, integration of ontologies is very struggling work since languages, domains, and structures of ontologies are different from each other. In this paper, we suggest an Ontology Architecture which solves this problem by providing a systematic framework to classify ontologies from three kinds of viewpoints: language, domain range, constructs. The Ontology Architecture consists of three axes according to the three viewpoints: Ontology Meta Layering axis, Semantic Domain Layering axis, and Ontology Constructs Layering axis. Because three axes in the Ontology Architecture are designed to improve the syntactic and semantic interoperability among ontologies, the integration of ontologies can be readily achieved.</p></td></tr><tr><td>50</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_22">Automatic Alignment of Ontology Eliminating the Probable Misalignments</a></td></tr><tr><td colspan=3><p>This paper describes a novel approach of detecting misalignment at the time of aligning two different ontologies, and of eliminating the misalignments. Our objective is to reduce limitation of a specific technique of ontology alignment. Two aligned sets extracted by different alignment techniques from the same pair of ontology, are fed to the misalignment detection and elimination process to produce better alignments. Our experiments demonstrate that our method, taking advantage of misalignment detection and elimination, shows a good recall and precision.</p></td></tr><tr><td>51</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_20">Business Process Collaboration Using Semantic Interoperability: Review and Framework</a></td></tr><tr><td colspan=3><p>Business process collaboration is one of the most significant factors driving today’s global business development. Researches and applications such as business process modeling, workflow interoperability, web service and ambient intelligence have been involved in this area. However, a holistic understanding is missing. To clarify the requirements and build a research foundation for business process collaboration, a conceptual model is provided in this paper. Then the state of the art and the future trend of business process modeling and process interoperability are reviewed based on this model. Furthermore, inspired by the novel semantic web technologies, a semantic agent based framework to facilitate business process collaboration is given.</p></td></tr><tr><td>52</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_27">OWL-Full Reasoning from an Object Oriented Perspective</a></td></tr><tr><td colspan=3><p>Bridging the gap between OWL and Object-Oriented Programming (OOP) languages is an indispensable condition to enable the Object-Oriented Modeling in Software Engineering by OWL. However it is very difficult in case of static OOP languages like Java and C#. We have developed SWCLOS, which is an OWL processor seamlessly built on top of Common Lisp Object System (CLOS), a dynamic OOP language. SWCLOS allows programmers to develop application domain models by OWL and enables OOP upon the models. In this paper, we explain the semantic gap between OWL and OOP languages, introduce the RDFS and OWL realization at SWCLOS, and discuss the OWL features from OOP perspectives. Finally we demonstrate the OWL-Full level performance in SWCLOS.</p></td></tr><tr><td>53</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_19">Ontology-Based RBAC Specification for Interoperation in Distributed Environment</a></td></tr><tr><td colspan=3><p>Today, the formulation, specification, and verification of adequate data protection policies in open distributed environment appear as the main challenge to address concerning authorization. Role-based access control models have attracted considerable research interest in recent years due to their innate ability to model organizational structure and their potential to reduce administrative overheads. This paper proposes ontology specification to describe Role-based Access Control model and extend it with a general context expression. Based on these definitions, the specification for interoperation in distributed environment is introduced. The works include a definition of ontology to describe the concepts and a declaration of rules to explicit the relationship between concepts. The ontology based approach can express security policy with semantic information and provide a machine interpretation for descriptions of policy in open distributed environment.</p></td></tr><tr><td>54</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_17">Behavioral Analysis Based on Relations in Weblogs</a></td></tr><tr><td colspan=3><p>This paper analyze the influence of the relations among blogs on users’ browsing behavior assuming that users’ activities can be predict by the relations on the blog network. We define the measure for two-hop relations as a factor to influence activities, and check the correlation between them or with users’ behavior by using blog data including visiting behavior. Attempting to determine the relations on which users read the blogs with interest as a helpful information for page recommendation, we conduct the experiments with a machine learning. As a result, even though the performance is not very high, we get the effective factors for prediction.</p></td></tr><tr><td>55</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_16">Community Focused Social Network Extraction</a></td></tr><tr><td colspan=3><p>A social networking service can become the basis for the information infrastructure of the future. For that purpose, it is important to extract social networks that reflect actual social networks which users have already had. Providing a simple means for users to register their social relations is also important. We propose a method that combines various approaches to extract social networks. Especially, three kinds of networks are extracted: user-registered </p></td></tr><tr><td>56</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_18">UniRSS: A New RSS Framework Supporting Dynamic Plug-In of RSS Extension Modules</a></td></tr><tr><td colspan=3><p>Due to the proliferation of information exchange via Internet, users suffer from information overload. For this reason, RSS is now widely adopted to deliver latest information to users without human intervention. It is also used to deliver customized data using extension modules. However an inter-operability issue among RSS applications using different extension modules has been raised because the processing of extension module has to be performed in the source code level of RSS applications. To resolve this interoperability issue, we propose a new RSS framework, UniRSS, which can support extension modules via a unified interface. UniRSS suggests an architecture composed of a pair of describing schema and a delegation code model to support any kind of extension modules in the code level. It supports both RSS 1.0 and RSS 2.0, and it also provides intelligent syndication using reasoning code insertion for RSS 1.0.</p></td></tr><tr><td>57</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_15">D-FOAF: Distributed Identity Management with Access Rights Delegation</a></td></tr><tr><td colspan=3><p>Todays WWW consists of more than just information. The WWW provides a large number of services, which often require identification of it’s users. This has lead to the fact that today users have to maintain a large number of different credentials for different websites – distributed or shared identification system are not widely deployed. Furthermore current authorisation systems requires strict centralisation of the authorisation procedure – users themselves are usually not enabled to authorise their trusted friends to access services, although often this would be beneficial for services and businesses on the Web.</p></td></tr><tr><td>58</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_14">FTT Algorithm of Web Pageviews for Personalized Recommendation</a></td></tr><tr><td colspan=3><p>As the need for personalized services sharply increases caused by the booming of Internet, Web-based data-mining is becoming a valuable sources of thoughts and theory to satisfy the personalized system function. The characters of personalized data-mining is reviewed and discussed in the beginning, and then an innovative algorithm (FP-Tree time – validity algorithm ) of Web pageviews, based on personalization, is raised. More authentic information can be efficiently got by adding time-validity coefficient to FTT-Tree storage structure to implement increment mining.</p></td></tr><tr><td>59</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_13">A Formalism of XML Restructuring Operations</a></td></tr><tr><td colspan=3><p>We present a set of primitive restructuring operators that, when combined, are sufficiently powerful to convert an XML document under a source schema into an XML document under an arbitrary target schema. We initially define the operators at the schema level, and then show how each operator induces a corresponding transformation on any XML document under the schema. Finally, we note that our operators can be implemented in a high level language such as XQuery, and thus our approach can be used as the basis for automating the conversion of one XML document to another XML document.</p></td></tr><tr><td>60</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_11">Finding Important Vocabulary Within Ontology</a></td></tr><tr><td colspan=3><p>In current Semantic Web community, some researches have been done on ranking ontologies, while very little is paid to ranking vocabularies within ontology. However, finding important vocabularies within a given ontology will bring benefits to ontology indexing, ontology understanding and even ranking vocabularies from a global view. In this paper, Vocabulary Dependency Graph (VDG) is proposed to model the dependencies among vocabularies within an ontology, and Textual Score of Vocabulary (TSV) is established based on the idea of virtual documents. And then a Double Focused PageRank algorithm is applied on VDG and TSV to rank vocabulary within ontology. Primary experiments demonstrate that our approach turns out to be useful in finding important vocabularies within ontology.</p></td></tr><tr><td>61</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_10">Matching Large Scale Ontology Effectively</a></td></tr><tr><td colspan=3><p>Ontology matching has played a great role in many well-known applications. It can identify the elements corresponding to each other. At present, with the rapid development of ontology applications, domain ontologies became very large in scale. Solving large scale ontology matching problems is beyond the reach of the existing matching methods. To improve this situation a modularization-based approach (called MOM) was proposed in this paper. It tries to decompose a large matching problem into several smaller ones and use a method to reduce the complexity dramatically. Several large and complex ontologies have been chosen and tested to verify this approach. The results show that the MOM method can significantly reduce the time cost while keeping the high matching accuracy.</p></td></tr><tr><td>62</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_12">Ontology-Based Similarity Between Text Documents on Manifold</a></td></tr><tr><td colspan=3><p>This paper firstly utilizes the ontology such as WordNet to build the semantic structures of text documents, and then enhance the semantic similarity among them. Because the correlations between documents make them lie on or close to a smooth low-dimensional manifold so that documents can be well characterized by a manifold within the space of documents, we calculate the similarity between any two semantically structured documents with respect to the intrinsic global manifold structure. This idea has been validated in the conducted text categorization experiments on patent documents.</p></td></tr><tr><td>63</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_7">Semantic Wiki as a Lightweight Knowledge Management System</a></td></tr><tr><td colspan=3><p>Since its birth in 1995, Wiki has become more and more popular. This paper presents a Semantic Wiki, a Wiki extended to include the ideas of Semantic Web. The proposed Semantic Wiki uses a simple Wiki syntax to write labeled links which represent RDF triples. By enabling the writing of labeled links, Semantic Wiki may provide an easy-to-use and flexible environment for an integrated management of content and metadata, so that Semantic Wiki may be used as a lightweight knowledge management system.</p></td></tr><tr><td>64</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_5">Automatic Annotation Using Citation Links and Co-citation Measure: Application to the Water Information System</a></td></tr><tr><td colspan=3><p>This paper describes an approach to automatically annotate documents for the Euro-Mediterranean Water Information System. This approach uses the citation links and co-citation measure in order to refine annotations extracted from an indexation method. An experiment of this approach with the CiteSeer database is presented and discussed.</p></td></tr><tr><td>65</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_8">Partition-Based Block Matching of Large Class Hierarchies</a></td></tr><tr><td colspan=3><p>Ontology matching is a crucial task of enabling interoperation between Web applications using different but related ontologies. Due to the size and the monolithic nature, large-scale ontologies regarding real world domains cause a new challenge to current ontology matching techniques. In this paper, we propose a method for partition-based block matching that is practically applicable to large class hierarchies, which are one of the most common kinds of large-scale ontologies. Based on both structural affinities and linguistic similarities, two large class hierarchies are partitioned into small blocks respectively, and then blocks from different hierarchies are matched by combining the two kinds of relatedness found via predefined anchors as well as virtual documents between them. Preliminary experiments demonstrate that the partition-based block matching method performs well on our test cases derived from Web directory structures.</p></td></tr><tr><td>66</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_3">Next Generation Semantic Web Applications</a></td></tr><tr><td colspan=3><p>In this short paper, we examine current Semantic Web application and we highlight what we see as a shift away from first generation Semantic Web applications, towards a new generation of applications, designed to exploit the large amounts of heterogeneous semantic markup, which are increasingly becoming available. Our analysis aims both to highlight the main features that can be used to compare and contrast current Semantic Web applications, as well as providing an initial blueprint for characterizing the nature of Semantic Web applications. Indeed, our ultimate goal is to specify a number of criteria, which Semantic Web applications ought to satisfy, if we want to move away from conventional semantic systems and develop a </p></td></tr><tr><td>67</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_6">Semantic Annotation Using Horizontal and Vertical Contexts</a></td></tr><tr><td colspan=3><p>This paper addresses the issue of semantic annotation using horizontal and vertical contexts. Semantic annotation is a task of annotating web pages with ontological information. As information on a web page is usually two-dimensionally laid out, previous semantic annotation methods that view a web page as an ‘object’ sequence have limitations. In this paper, to better incorporate the two-dimensional contexts, semantic annotation is formalized as a problem of block detection and text annotation. Block detection is aimed at detecting the text block by making use of context in one dimension and text annotation is aimed at detecting the ‘targeted instance’ in the identified blocks using the other dimensional context. A two-stage method for semantic annotation using machine learning has been proposed. Experimental results indicate that the proposed method can significantly outperform the baseline method as well as the sequence-based method for semantic annotation.</p></td></tr><tr><td>68</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_9">Towards Quick Understanding and Analysis of Large-Scale Ontologies</a></td></tr><tr><td colspan=3><p>With the development of semantic web technologies, large and complex ontologies are constructed and applied to many practical applications. In order for users to quickly understand and acquire information from these huge information “oceans”, we propose a novel ontology visualization approach accompanied by “anatomies” of classes and properties. With the holistic “imaging”, users can both quickly locate the interesting “hot” classes or properties and understand the evolution of the ontology; with the anatomies, they can acquire more detailed information of classes or properties that is arduous to collect by browsing and navigation. Specifically, we produce the ontology’s holistic “imaging” which contains a semantic layout on classes and distributions of instances. Additionally, the evolution of the ontology is illustrated by the changes on the “imaging”. Furthermore, detailed anatomies of classes and properties, which are enhanced by techniques in database field (e.g. data mining), are ready for users.</p></td></tr><tr><td>69</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_2">Transformation from OWL Description to Resource Space Model</a></td></tr><tr><td colspan=3><p>Semantics shows diversity in real world, document world, mental abstraction world and machine world. Transformation between semantics pursues the uniformity in the diversity. The Resource Space Model (RSM) is a semantic data model for organizing resources based on the classification semantics that human often use in understanding the real world. The design of a resource space relies on knowledge about domain and the RSM. Automatically creating resource space can relieve such reliance in RSM applications. This paper proposes an approach to automatically transform Web Ontology Language description into resource space. The normal forms of the generated resource space are investigated to ensure its normalization characteristic. The Dunhuang culture resource space is used to illustrate the approach.</p></td></tr><tr><td>70</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_1">The Semantic Web: A Network of Understanding</a></td></tr><tr><td colspan=3><p>If you visit my Web page, which is not much different than most other people’s in many ways, you would find many fields which are highlighted as links to other pages. In the list of my students you can find links to their pages, in the links of my papers you can find downloadable files or links to various digital libraries, and in the lists of my classes you can find links both to the Web resources I used in my classes and to University pages that describe when the classes were given, what the prerequisites were, etc. In short, a great deal of the information “on my page” is not actually on my page at all, it is provided by the linking mechanisms of the Web. It is, in fact, exactly this network effect in which I can gain advantage by linking to information created by other people, rather than recreating it myself, that makes the Web so powerful.</p></td></tr><tr><td>71</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_34">Domain-Aware Ontology Matching</a></td></tr><tr><td colspan=3><p>The inherent heterogeneity of datasets on the Semantic Web has created a need to interlink them, and several tools have emerged that automate this task. In this paper we are interested in what happens if we enrich these matching tools with knowledge of the domain of the ontologies. We explore how to express the notion of a domain in terms usable for an ontology matching tool, and we examine various methods to decide what constitutes the domain of a given dataset. We show how we can use this in a matching tool, and study the effect of domain knowledge on the quality of the alignment.</p></td></tr><tr><td>72</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_36">An Evidence-Based Verification Approach to Extract Entities and Relations for Knowledge Base Population</a></td></tr><tr><td colspan=3><p>This paper presents an approach to automatically extract entities and relationships from textual documents. The main goal is to populate a knowledge base that hosts this structured information about domain entities. The extracted entities and their expected relationships are verified using two evidence based techniques: classification and linking. This last process also enables the linking of our knowledge base to other sources which are part of the Linked Open Data cloud. We demonstrate the benefit of our approach through series of experiments with real-world datasets.</p></td></tr><tr><td>73</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_35">Rapidly Integrating Services into the Linked Data Cloud</a></td></tr><tr><td colspan=3><p>The amount of data available in the Linked Data cloud continues to grow. Yet, few services consume and produce linked data. There is recent work that allows a user to define a linked service from an online service, which includes the specifications for consuming and producing linked data, but building such models is time consuming and requires specialized knowledge of RDF and SPARQL. This paper presents a new approach that allows domain experts to rapidly create semantic models of services by demonstration in an interactive web-based interface. First, the user provides examples of the service request URLs. Then, the system automatically proposes a service model the user can refine interactively. Finally, the system saves a service specification using a new expressive vocabulary that includes lowering and lifting rules. This approach empowers end users to rapidly model existing services and immediately use them to consume and produce linked data.</p></td></tr><tr><td>74</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_32">Semantic Sentiment Analysis of Twitter</a></td></tr><tr><td colspan=3><p>Sentiment analysis over Twitter offer organisations a fast and effective way to monitor the publics’ feelings towards their brand, business, directors, etc. A wide range of features and methods for training sentiment classifiers for Twitter datasets have been researched in recent years with varying results. In this paper, we introduce a novel approach of adding semantics as additional features into the training set for sentiment analysis. For each extracted entity (e.g. iPhone) from tweets, we add its semantic concept (e.g. “Apple product”) as an additional feature, and measure the correlation of the representative concept with negative/positive sentiment. We apply this approach to predict sentiment for three different Twitter datasets. Our results show an average increase of F harmonic accuracy score for identifying both negative and positive sentiment of around 6.5% and 4.8% over the baselines of unigrams and part-of-speech features respectively. We also compare against an approach based on sentiment-bearing topic analysis, and find that semantic features produce better Recall and F score when classifying negative sentiment, and better Precision with lower Recall and F score in positive sentiment classification.</p></td></tr><tr><td>75</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_31">On the Diversity and Availability of Temporal Information in Linked Open Data</a></td></tr><tr><td colspan=3><p>An increasing amount of data is published and consumed on the Web according to the Linked Data paradigm. In consideration of both publishers and consumers, the temporal dimension of data is important. In this paper we investigate the characterisation and availability of temporal information in Linked Data at large scale. Based on an abstract definition of temporal information we conduct experiments to evaluate the availability of such information using the data from the 2011 Billion Triple Challenge (BTC) dataset. Focusing in particular on the representation of temporal meta-information, i.e., temporal information associated with RDF statements and graphs, we investigate the approaches proposed in the literature, performing both a quantitative and a qualitative analysis and proposing guidelines for data consumers and publishers. Our experiments show that the amount of temporal information available in the LOD cloud is still very small; several different models have been used on different datasets, with a prevalence of approaches based on the annotation of RDF documents.</p></td></tr><tr><td>76</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_33">: Crowdsourcing Ontology Alignment with Microtasks</a></td></tr><tr><td colspan=3><p>The last decade of research in ontology alignment has brought a variety of computational techniques to discover correspondences between ontologies. While the accuracy of automatic approaches has continuously improved, human contributions remain a key ingredient of the process: this input serves as a valuable source of domain knowledge that is used to train the algorithms and to validate and augment automatically computed alignments. In this paper, we introduce </p></td></tr><tr><td>77</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_28">Ontology Constraints in Incomplete and Complete Data</a></td></tr><tr><td colspan=3><p>Ontology and other logical languages are built around the idea that axioms enable the inference of new facts about the available data. In some circumstances, however, the data is meant to be complete in certain ways, and deducing new facts may be undesirable. Previous approaches to this issue have relied on syntactically specifying certain axioms as constraints or adding in new constructs for constraints, and providing a different or extended meaning for constraints that reduces or eliminates their ability to infer new facts without requiring the data to be complete. We propose to instead directly state that the extension of certain concepts and roles are complete by making them DBox predicates, which eliminates the distinction between regular axioms and constraints for these concepts and roles. This proposal eliminates the need for special semantics and avoids problems of previous proposals.</p></td></tr><tr><td>78</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_30">Who Will Follow Whom? Exploiting Semantics for Link Prediction in Attention-Information Networks</a></td></tr><tr><td colspan=3><p>Existing approaches for link prediction, in the domain of network science, exploit a network’s topology to predict future connections by assessing existing edges and connections, and inducing links given the presence of mutual nodes. Despite the rise in popularity of Attention-Information Networks (i.e. microblogging platforms) and the production of content within such platforms, no existing work has attempted to exploit the semantics of published content when predicting network links. In this paper we present an approach that fills this gap by a) predicting </p></td></tr><tr><td>79</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11836025_4">Hierarchical Topic Term Extraction for Semantic Annotation in Chinese Bulletin Board System</a></td></tr><tr><td colspan=3><p>With the current growing interest in the Semantic Web, the demand for ontological data has been on the verge of emergency. Currently many structured and semi-structured documents have been applied for ontology learning and annotation. However, most of the electronic documents on the web are plain-text, and these texts are still not well utilized for the Semantic Web. In this paper, we propose a novel method to automatically extract topic terms to generate a concept hierarchy from the data of Chinese Bulletin Board System (BBS), which is a collection of plain-text. In addition, our work provides the text source associated with the extracted concept as well, which could be a perfect fit for the semantic search application that makes a fusion of both formal and implicit semantics. The experimental results indicate that our method is effective and the extracted concept hierarchy is meaningful.</p></td></tr><tr><td>80</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_29">A Machine Learning Approach for Instance Matching Based on Similarity Metrics</a></td></tr><tr><td colspan=3><p>The Linking Open Data (LOD) project is an ongoing effort to construct a global data space, i.e. the Web of Data. One important part of this project is to establish </p></td></tr><tr><td>81</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_27">Discovering Concept Coverings in Ontologies of Linked Data Sources</a></td></tr><tr><td colspan=3><p>Despite the increase in the number of linked instances in the Linked Data Cloud in recent times, the absence of links at the concept level has resulted in heterogenous schemas, challenging the interoperability goal of the Semantic Web. In this paper, we address this problem by finding alignments between concepts from multiple Linked Data sources. Instead of only considering the existing concepts present in each ontology, we hypothesize new composite concepts defined as disjunctions of conjunctions of (RDF) types and value restrictions, which we call </p></td></tr><tr><td>82</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_26">Mining Semantic Relations between Research Areas</a></td></tr><tr><td colspan=3><p>For a number of years now we have seen the emergence of repositories of research data specified using OWL/RDF as representation languages, and conceptualized according to a variety of ontologies. This class of solutions promises both to facilitate the integration of research data with other relevant sources of information and also to support more intelligent forms of querying and exploration. However, an issue which has only been partially addressed is that of generating and characterizing semantically the relations that exist between research areas. This problem has been traditionally addressed by manually creating taxonomies, such as the ACM classification of research topics. However, this manual approach is inadequate for a number of reasons: these taxonomies are very coarse-grained and they do not cater for the fine-grained research topics, which define the level at which typically researchers (and even more so, PhD students) operate. Moreover, they evolve slowly, and therefore they tend not to cover the most recent research trends. In addition, as we move towards a semantic characterization of these relations, there is arguably a need for a more sophisticated characterization than a homogeneous taxonomy, to reflect the different ways in which research areas can be related. In this paper we propose Klink, a new approach to i) automatically generating relations between research areas and ii) populating a bibliographic ontology, which combines both machine learning methods and external knowledge, which is drawn from a number of resources, including Google Scholar and Wikipedia. We have tested a number of alternative algorithms and our evaluation shows that a method relying on both external knowledge and the ability to detect temporal relations between research areas performs best with respect to a manually constructed standard.</p></td></tr><tr><td>83</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_25">Hitting the Sweetspot: Economic Rewriting of Knowledge Bases</a></td></tr><tr><td colspan=3><p>Three conflicting requirements arise in the context of knowledge base (KB) extraction: the size of the extracted KB, the size of the corresponding signature and the syntactic similarity of the extracted KB with the original one. Minimal module extraction and uniform interpolation assign an absolute priority to one of these requirements, thereby limiting the possibilities to influence the other two. We propose a novel technique for </p></td></tr><tr><td>84</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_24">Link Discovery with Guaranteed Reduction Ratio in Affine Spaces with Minkowski Measures</a></td></tr><tr><td colspan=3><p>Time-efficient algorithms are essential to address the complex linking tasks that arise when trying to discover links on the Web of Data. Although several lossless approaches have been developed for this exact purpose, they do not offer theoretical guarantees with respect to their performance. In this paper, we address this drawback by presenting the first Link Discovery approach with theoretical quality guarantees. In particular, we prove that given an achievable reduction ratio </p></td></tr><tr><td>85</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_21">Feature LDA: A Supervised Topic Model for Automatic Detection of Web API Documentations from the Web</a></td></tr><tr><td colspan=3><p>Web APIs have gained increasing popularity in recent Web service technology development owing to its simplicity of technology stack and the proliferation of mashups. However, efficiently discovering Web APIs and the relevant documentations on the Web is still a challenging task even with the best resources available on the Web. In this paper we cast the problem of detecting the Web API documentations as a text classification problem of classifying a given Web page as Web API associated or not. We propose a supervised generative topic model called feature latent Dirichlet allocation (feaLDA) which offers a generic probabilistic framework for automatic detection of Web APIs. feaLDA not only captures the correspondence between data and the associated class labels, but also provides a mechanism for incorporating side information such as labelled features automatically learned from data that can effectively help improving classification performance. Extensive experiments on our Web APIs documentation dataset shows that the feaLDA model outperforms three strong supervised baselines including naive Bayes, support vector machines, and the maximum entropy model, by over 3% in classification accuracy. In addition, feaLDA also gives superior performance when compared against other existing supervised topic models.</p></td></tr><tr><td>86</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_22">Efficient Execution of Top-K SPARQL Queries</a></td></tr><tr><td colspan=3><p>Top-k queries, i.e. queries returning the top </p></td></tr><tr><td>87</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_20">DeFacto - Deep Fact Validation</a></td></tr><tr><td colspan=3><p>One of the main tasks when creating and maintaining knowledge bases is to validate facts and provide sources for them in order to ensure correctness and traceability of the provided knowledge. So far, this task is often addressed by human curators in a three-step process: issuing appropriate keyword queries for the statement to check using standard search engines, retrieving potentially relevant documents and screening those documents for relevant content. The drawbacks of this process are manifold. Most importantly, it is very time-consuming as the experts have to carry out several search processes and must often read several documents. In this article, we present DeFacto (Deep Fact Validation) – an algorithm for validating facts by finding trustworthy sources for it on the Web. DeFacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of webpages as well as useful additional information including a score for the confidence DeFacto has in the correctness of the input fact.</p></td></tr><tr><td>88</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_19">Strabon: A Semantic Geospatial DBMS</a></td></tr><tr><td colspan=3><p>We present Strabon, a new RDF store that supports the state of the art semantic geospatial query languages stSPARQL and GeoSPARQL. To illustrate the expressive power offered by these query languages and their implementation in Strabon, we concentrate on the new version of the data model stRDF and the query language stSPARQL that we have developed ourselves. Like GeoSPARQL, these new versions use OGC standards to represent geometries where the original versions used linear constraints. We study the performance of Strabon experimentally and show that it scales to very large data volumes and performs, most of the times, better than all other geospatial RDF stores it has been compared with.</p></td></tr><tr><td>89</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_14">Formal Verification of Data Provenance Records</a></td></tr><tr><td colspan=3><p>Data provenance is the history of derivation of a data artifact from its original sources. As the real-life provenance records can likely cover thousands of data items and derivation steps, one of the pressing challenges becomes development of formal frameworks for their automated verification.</p></td></tr><tr><td>90</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_18">The Not-So-Easy Task of Computing Class Subsumptions in OWL RL</a></td></tr><tr><td colspan=3><p>The lightweight ontology language OWL RL is used for reasoning with large amounts of data. To this end, the W3C standard provides a simple system of deduction rules, which operate directly on the RDF syntax of OWL. Several similar systems have been studied. However, these approaches are usually complete for instance retrieval only. This paper asks if and how such methods could also be used for computing entailed subclass relationships. Checking entailment for arbitrary OWL RL class subsumptions is co-NP-hard, but tractable rule-based reasoning is possible when restricting to subsumptions between atomic classes. Surprisingly, however, this cannot be achieved in any RDF-based rule system, i.e., the W3C calculus cannot be extended to compute all atomic class subsumptions. We identify syntactic restrictions to mitigate this problem, and propose a rule system that is sound and complete for many OWL RL ontologies.</p></td></tr><tr><td>91</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_13">Predicting Reasoning Performance Using Ontology Metrics</a></td></tr><tr><td colspan=3><p>A key issue in semantic reasoning is the computational complexity of inference tasks on expressive ontology languages such as OWL DL and OWL 2 DL. Theoretical works have established worst-case complexity results for reasoning tasks for these languages. However, hardness of reasoning about individual ontologies has not been adequately characterised. In this paper, we conduct a systematic study to tackle this problem using machine learning techniques, covering over 350 real-world ontologies and four state-of-the-art, widely-used OWL 2 reasoners. Our main contributions are two-fold. Firstly, we learn various classifiers that accurately predict classification time for an ontology based on its metric values. Secondly, we identify a number of metrics that can be used to effectively predict reasoning performance. Our prediction models have been shown to be highly effective, achieving an accuracy of over 80%.</p></td></tr><tr><td>92</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_17">Large-Scale Learning of Relation-Extraction Rules with Distant Supervision from the Web</a></td></tr><tr><td colspan=3><p>We present a large-scale relation extraction (RE) system which learns grammar-based RE rules from the Web by utilizing large numbers of relation instances as seed. Our goal is to obtain rule sets large enough to cover the actual range of linguistic variation, thus tackling the long-tail problem of real-world applications. A variant of distant supervision learns several relations in parallel, enabling a new method of rule filtering. The system detects both binary and </p></td></tr><tr><td>93</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_15">Cost Based Query Ordering over OWL Ontologies</a></td></tr><tr><td colspan=3><p>The paper presents an approach for cost-based query planning for SPARQL queries issued over an OWL ontology using the OWL Direct Semantics entailment regime of SPARQL 1.1. The costs are based on information about the instances of classes and properties that are extracted from a model abstraction built by an OWL reasoner. A static and a dynamic algorithm are presented which use these costs to find optimal or near optimal execution orders for the atoms of a query. For the dynamic case, we improve the performance by exploiting an individual clustering approach that allows for computing the cost functions based on one individual sample from a cluster. Our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available. This changes, however, when the statistics are less accurate, e.g., due to non-deterministic reasoning decisions.</p></td></tr><tr><td>94</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_16">Robust Runtime Optimization and Skew-Resistant Execution of Analytical SPARQL Queries on Pig</a></td></tr><tr><td colspan=3><p>We describe a system that incrementally translates SPARQL queries to Pig Latin and executes them on a Hadoop cluster. This system is designed to work efficiently on complex queries with many self-joins over huge datasets, avoiding job failures even in the case of joins with unexpected high-value skew. To be robust against cost estimation errors, our system </p></td></tr><tr><td>95</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_9">RDFS Reasoning on Massively Parallel Hardware</a></td></tr><tr><td colspan=3><p>Recent developments in hardware have shown an increase in parallelism as opposed to clock rates. In order to fully exploit these new avenues of performance improvement, computationally expensive workloads have to be expressed in a way that allows for fine-grained parallelism. In this paper, we address the problem of describing RDFS entailment in such a way. Different from previous work on parallel RDFS reasoning, we assume a shared memory architecture. We analyze the problem of duplicates that naturally occur in RDFS reasoning and develop strategies towards its mitigation, exploiting all levels of our architecture. We implement and evaluate our approach on two real-world datasets and study its performance characteristics on different levels of parallelization. We conclude that RDFS entailment lends itself well to parallelization but can benefit even more from careful optimizations that take into account intricacies of modern parallel hardware.</p></td></tr><tr><td>96</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_11">Semantic Enrichment by Non-experts: Usability of Manual Annotation Tools</a></td></tr><tr><td colspan=3><p>Most of the semantic content available has been generated automatically by using annotation services for existing content. Automatic annotation is not of sufficient quality to enable focused search and retrieval: either too many or too few terms are semantically annotated. User-defined semantic enrichment allows for a more targeted approach. We developed a tool for semantic annotation of digital documents and conducted an end-user study to evaluate its acceptance by and usability for non-expert users. This paper presents the results of this user study and discusses the lessons learned about both the semantic enrichment process and our methodology of exposing non-experts to semantic enrichment.</p></td></tr><tr><td>97</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_10">An Efficient Bit Vector Approach to Semantics-Based Machine Perception in Resource-Constrained Devices</a></td></tr><tr><td colspan=3><p>The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception – explanation and discrimination – and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale.</p></td></tr><tr><td>98</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_5">Automatic Typing of DBpedia Entities</a></td></tr><tr><td colspan=3><p>We present </p></td></tr><tr><td>99</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_6">Performance Heterogeneity and Approximate Reasoning in Description Logic Ontologies</a></td></tr><tr><td colspan=3><p>Due to the high worst case complexity of the core reasoning problem for the expressive profiles of OWL 2, ontology engineers are often surprised and confused by the performance behaviour of reasoners on their ontologies. Even very experienced modellers with a sophisticated grasp of reasoning algorithms do not have a good mental model of reasoner performance behaviour. Seemingly innocuous changes to an OWL ontology can degrade classification time from instantaneous to too long to wait for. Similarly, switching reasoners (e.g., to take advantage of specific features) can result in wildly different classification times. In this paper we investigate performance variability phenomena in OWL ontologies, and present methods to identify subsets of an ontology which are performance-degrading for a given reasoner. When such (ideally small) subsets are removed from an ontology, and the remainder is much easier for the given reasoner to reason over, we designate them “hot spots”. The identification of these hot spots allows users to isolate difficult portions of the ontology in a principled and systematic way. Moreover, we devise and compare various methods for approximate reasoning and knowledge compilation based on hot spots. We verify our techniques with a select set of varyingly difficult ontologies from the NCBO BioPortal, and were able to, firstly, successfully identify performance hot spots against the major freely available DL reasoners, and, secondly, significantly improve classification time using approximate reasoning based on hot spots.</p></td></tr><tr><td>100</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_7">Concept-Based Semantic Difference in Expressive Description Logics</a></td></tr><tr><td colspan=3><p>Detecting, much less understanding, the difference between two description logic based ontologies is challenging for ontology engineers due, in part, to the possibility of complex, non-local logic effects of axiom changes. First, it is often quite difficult to even determine which concepts have had their meaning altered by a change. Second, once a concept change is pinpointed, the problem of distinguishing whether the concept is directly or indirectly affected by a change has yet to be tackled. To address the first issue, various principled notions of “semantic diff” (based on deductive inseparability) have been proposed in the literature and shown to be computationally practical for the expressively restricted case of </p></td></tr><tr><td>101</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_2">A Formal Semantics for Weighted Ontology Mappings</a></td></tr><tr><td colspan=3><p>Ontology mappings are often assigned a weight or confidence factor by matchers. Nonetheless, few semantic accounts have been given so far for such weights. This paper presents a formal semantics for weighted mappings between different ontologies. It is based on a classificational interpretation of mappings: if </p></td></tr><tr><td>102</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_1">: Modular Combination of OWL Reasoners for Ontology Classification</a></td></tr><tr><td colspan=3><p>Classification is a fundamental reasoning task in ontology design, and there is currently a wide range of reasoners highly optimised for classification of OWL 2 ontologies. There are also several reasoners that are complete for restricted fragments of OWL 2 , such as the OWL 2 EL profile. These reasoners are much more efficient than fully-fledged OWL 2 reasoners, but they are not complete for ontologies containing (even if just a few) axioms outside the relevant fragment. In this paper, we propose a novel classification technique that combines an OWL 2 reasoner and an efficient reasoner for a given fragment in such a way that the bulk of the workload is assigned to the latter. Reasoners are combined in a black-box modular manner, and the specifics of their implementation (and even of their reasoning technique) are irrelevant to our approach.</p></td></tr><tr><td>103</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_3">Personalised Graph-Based Selection of Web APIs</a></td></tr><tr><td colspan=3><p>Modelling and understanding various contexts of users is important to enable personalised selection of Web APIs in directories such as Programmable Web. Currently, relationships between users and Web APIs are not clearly understood and utilized by existing selection approaches. In this paper, we present a semantic model of a Web API directory graph that captures relationships such as Web APIs, mashups, developers, and categories. We describe a novel configurable graph-based method for selection of Web APIs with personalised and temporal aspects. The method allows users to get more control over their preferences and recommended Web APIs while they can exploit information about their social links and preferences. We evaluate the method on a real-world dataset from ProgrammableWeb.com, and show that it provides more contextualised results than currently available popularity-based rankings.</p></td></tr><tr><td>104</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_4">Instance-Based Matching of Large Ontologies Using Locality-Sensitive Hashing</a></td></tr><tr><td colspan=3><p>In this paper, we describe a mechanism for ontology alignment using instance based matching of types (or classes). Instance-based matching is known to be a useful technique for matching ontologies that have different names and different structures. A key problem in instance matching of types, however, is scaling the matching algorithm to (a) handle types with a large number of instances, and (b) efficiently match a large number of type pairs. We propose the use of state-of-the art locality-sensitive hashing (LSH) techniques to vastly improve the scalability of instance matching across multiple types. We show the feasibility of our approach with DBpedia and Freebase, two different type systems with hundreds and thousands of types, respectively. We describe how these techniques can be used to estimate containment or equivalence relations between two type systems, and we compare two different LSH techniques for computing instance similarity.</p></td></tr><tr><td>105</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_12">Ontology-Based Access to Probabilistic Data with </a></td></tr><tr><td colspan=3><p>We propose a framework for querying probabilistic instance data in the presence of an </p></td></tr><tr><td>106</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_39">Unsupervised Entity Resolution on Multi-type Graphs</a></td></tr><tr><td colspan=3><p>Entity resolution is the task of identifying all mentions that represent the same real-world entity within a knowledge base or across multiple knowledge bases. We address the problem of performing entity resolution on RDF graphs containing multiple types of nodes, using the links between instances of different types to improve the accuracy. For example, in a graph of products and manufacturers the goal is to resolve all the products and all the manufacturers. We formulate this problem as a multi-type graph summarization problem, which involves clustering the nodes in each type that refer to the same entity into one super node and creating weighted links among super nodes that summarize the inter-cluster links in the original graph. Experiments show that the proposed approach outperforms several state-of-the-art generic entity resolution approaches, especially in data sets with missing values and one-to-many, many-to-many relations.</p></td></tr><tr><td>107</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_37">A Knowledge Base Approach to Cross-Lingual Keyword Query Interpretation</a></td></tr><tr><td colspan=3><p>The amount of entities in large knowledge bases available on the Web has been increasing rapidly, making it possible to propose new ways of intelligent information access. In addition, there is an impending need for technologies that can enable cross-lingual information access. As a simple and intuitive way of specifying information needs, keyword queries enjoy widespread usage, but suffer from the challenges including </p></td></tr><tr><td>108</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-35176-1_8">SPLODGE: Systematic Generation of SPARQL Benchmark Queries for Linked Open Data</a></td></tr><tr><td colspan=3><p>The distributed and heterogeneous nature of Linked Open Data requires flexible and federated techniques for query evaluation. In order to evaluate current federation querying approaches a general methodology for conducting benchmarks is mandatory. In this paper, we present a classification methodology for federated </p></td></tr><tr><td>109</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_38">Context-Free Path Queries on RDF Graphs</a></td></tr><tr><td colspan=3><p>Navigational graph queries are an important class of queries that can extract implicit binary relations over the nodes of input graphs. Most of the navigational query languages used in the RDF community, e.g. property paths in W3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the regular expressions. It is known that regular expressions have limited expressivity; for instance, some natural queries, like </p></td></tr><tr><td>110</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_34">Integrating Medical Scientific Knowledge with the Semantically Quantified Self</a></td></tr><tr><td colspan=3><p>The assessment of risk in medicine is a crucial task, and depends on scientific knowledge derived by systematic clinical studies on factors affecting health, as well as on particular knowledge about the current status of a particular patient. Existing non-semantic risk prediction tools are typically based on hardcoded scientific knowledge, and only cover a very limited range of patient states. This makes them rapidly out of date, and limited in application, particularly for patients with multiple co-occurring conditions. In this work we propose an integration of Semantic Web and Quantified Self technologies to create a framework for calculating clinical risk predictions for patients based on self-gathered biometric data. This framework relies on generic, reusable ontologies for representing clinical risk, and sensor readings, and reasoning to support the integration of data represented according to these ontologies. The implemented framework shows a wide range of advantages over existing risk calculation.</p></td></tr><tr><td>111</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_33">Leveraging Linked Data to Discover Semantic Relations Within Data Sources</a></td></tr><tr><td colspan=3><p>Mapping data to a shared domain ontology is a key step in publishing semantic content on the Web. Most of the work on automatically mapping structured and semi-structured sources to ontologies focuses on semantic labeling, i.e., annotating data fields with ontology classes and/or properties. However, a precise mapping that fully recovers the intended meaning of the data needs to describe the semantic relations between the data fields too. We present a novel approach to automatically discover the semantic relations within a given data source. We mine the small graph patterns occurring in Linked Open Data and combine them to build a graph that will be used to infer semantic relations. We evaluated our approach on datasets from different domains. Mining patterns of maximum length five, our method achieves an average precision of 75 % and recall of 77 % for a dataset with very complex mappings to the domain ontology, increasing up to 86 % and 82 %, respectively, for simpler ontologies and mappings.</p></td></tr><tr><td>112</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_35">Learning to Assess Linked Data Relationships Using Genetic Programming</a></td></tr><tr><td colspan=3><p>The goal of this work is to learn a measure supporting the detection of strong relationships between Linked Data entities. Such relationships can be represented as paths of entities and properties, and can be obtained through a blind graph search process traversing Linked Data. The challenge here is therefore the design of a cost-function that is able to detect the strongest relationship between two given entities, by objectively assessing the value of a given path. To achieve this, we use a Genetic Programming approach in a supervised learning method to generate path evaluation functions that compare well with human evaluations. We show how such a cost-function can be generated only using basic topological features of the nodes of the paths as they are being traversed (i.e. without knowledge of the whole graph), and how it can be improved through introducing a very small amount of knowledge about the vocabularies of the properties that connect nodes in the graph.</p></td></tr><tr><td>113</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_36">A Probabilistic Model for Time-Aware Entity Recommendation</a></td></tr><tr><td colspan=3><p>In recent years, there has been an increasing effort to develop techniques for related entity recommendation, where the task is to retrieve a ranked list of related entities given a keyword query. Another trend in the area of information retrieval (IR) is to take temporal aspects of a given query into account when assessing the relevance of documents. However, while this has become an established functionality in document search engines, the significance of time has not yet been recognized for entity recommendation. In this paper, we address this gap by introducing the task of </p></td></tr><tr><td>114</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_29">Distributed RDF Query Answering with Dynamic Data Exchange</a></td></tr><tr><td colspan=3><p>Evaluating joins over RDF data stored in a shared-nothing server cluster is key to processing truly large RDF datasets. To the best of our knowledge, the existing approaches use a variant of the </p></td></tr><tr><td>115</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_30">RDF2Vec: RDF Graph Embeddings for Data Mining</a></td></tr><tr><td colspan=3><p>Linked Open Data has been recognized as a valuable source for background information in data mining. However, most data mining tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. Our evaluation shows that such vector representations outperform existing techniques for the propositionalization of RDF graphs on a variety of different predictive machine learning tasks, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.</p></td></tr><tr><td>116</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_31">SPARQL-to-SQL on Internet of Things Databases and Streams</a></td></tr><tr><td colspan=3><p>To realise a semantic Web of Things, the challenge of achieving efficient Resource Description Format (RDF) storage and SPARQL query performance on Internet of Things (IoT) devices with limited resources has to be addressed. State-of-the-art SPARQL-to-SQL engines have been shown to outperform RDF stores on some benchmarks. In this paper, we describe an optimisation to the SPARQL-to-SQL approach, based on a study of time-series IoT data structures, that employs metadata abstraction and efficient translation by reusing existing SPARQL engines to produce Linked Data ‘just-in-time’. We evaluate our approach against RDF stores, state-of-the-art SPARQL-to-SQL engines and streaming SPARQL engines, in the context of IoT data and scenarios. We show that storage efficiency, with succinct row storage, and query performance can be improved from 2 times to 3 orders of magnitude.</p></td></tr><tr><td>117</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_28">Exploiting Emergent Schemas to Make RDF Systems More Efficient</a></td></tr><tr><td colspan=3><p>We build on our earlier finding that more than 95 % of the triples in actual RDF triple graphs have a remarkably tabular structure, whose schema does not necessarily follow from explicit metadata such as ontologies, but for which an RDF store can automatically derive by looking at the data using so-called “emergent schema” detection techniques. In this paper we investigate how computers and in particular RDF stores can take advantage from this emergent schema to more compactly store RDF data and more efficiently optimize and execute SPARQL queries. To this end, we contribute techniques for efficient emergent schema aware RDF storage and new query operator algorithms for emergent schema aware scans and joins. In all, these techniques allow RDF schema processors fully catch up with relational database techniques in terms of rich physical database design options and efficiency, without requiring a rigid upfront schema structure definition.</p></td></tr><tr><td>118</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_32">Can You Imagine... A Language for Combinatorial Creativity?</a></td></tr><tr><td colspan=3><p>Combinatorial creativity combines existing concepts in a novel way in order to produce new concepts. For example, we can imagine jewelry that measures blood pressure. For this, we would combine the concept of jewelry with the capabilities of medical devices. In this paper, we concentrate on creating new concepts in the description logic </p></td></tr><tr><td>119</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_27">Semantic Labeling: A Domain-Independent Approach</a></td></tr><tr><td colspan=3><p>Semantic labeling is the process of mapping attributes in data sources to classes in an ontology and is a necessary step in heterogeneous data integration. Variations in data formats, attribute names and even ranges of values of data make this a very challenging task. In this paper, we present a novel domain-independent approach to automatic semantic labeling that uses machine learning techniques. Previous approaches use machine learning to learn a model that extracts features related to the data of a domain, which requires the model to be re-trained for every new domain. Our solution uses similarity metrics as features to compare against labeled domain data and learns a matching function to infer the correct semantic labels for data. Since our approach depends on the learned similarity metrics but not the data itself, it is domain-independent and only needs to be trained once to work effectively across multiple domains. In our evaluation, our approach achieves higher accuracy than other approaches, even when the learned models are trained on domains other than the test domain.</p></td></tr><tr><td>120</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_26">Multi-level Semantic Labelling of Numerical Values</a></td></tr><tr><td colspan=3><p>With the success of Open Data a huge amount of tabular data sources became available that could potentially be mapped and linked into the Web of (Linked) Data. Most existing approaches to “semantically label” such tabular data rely on mappings of textual information to classes, properties, or instances in RDF knowledge bases in order to link – and eventually transform – tabular data into RDF. However, as we will illustrate, Open Data tables typically contain a large portion of numerical columns and/or non-textual headers; therefore solutions that solely focus on textual “cues” are only partially applicable for mapping such data sources. We propose an approach to find and rank candidates of semantic labels and context descriptions for a given bag of numerical values. To this end, we apply a hierarchical clustering over information taken from DBpedia to build a background knowledge graph of possible “semantic contexts” for bags of numerical values, over which we perform a nearest neighbour search to rank the most likely candidates. Our evaluation shows that our approach can assign fine-grained semantic labels, when there is enough supporting evidence in the background knowledge graph. In other cases, our approach can nevertheless assign high level contexts to the data, which could potentially be used in combination with other approaches to narrow down the search space of possible labels.</p></td></tr><tr><td>121</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_21">Optimizing Aggregate SPARQL Queries Using Materialized RDF Views</a></td></tr><tr><td colspan=3><p>During recent years, more and more data has been published as native RDF datasets. In this setup, both the size of the datasets and the need to process aggregate queries represent challenges for standard SPARQL query processing techniques. To overcome these limitations, materialized views can be created and used as a source of precomputed partial results during query processing. However, materialized view techniques as proposed for relational databases do not support RDF specifics, such as incompleteness and the need to support implicit (derived) information. To overcome these challenges, this paper proposes MARVEL (MAterialized Rdf Views with Entailment and incompLetness). The approach consists of a view selection algorithm based on an associated RDF-specific cost model, a view definition syntax, and an algorithm for rewriting SPARQL queries using materialized RDF views. The experimental evaluation shows that MARVEL can improve query response time by more than an order of magnitude while effectively handling RDF specifics.</p></td></tr><tr><td>122</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_25">Semantic Sensitive Simultaneous Tensor Factorization</a></td></tr><tr><td colspan=3><p>The semantics distributed over large-scale knowledge bases can be used to intermediate heterogeneous users’ activity logs created in services; such information can be used to improve applications that can help users to decide the next activities/services. Since user activities can be represented in terms of relationships involving three or more things (e.g. a user tags movie items on a webpage), tensors are an attractive approach to represent them. The recently introduced Semantic Sensitive Tensor Factorization (SSTF) is promising as it achieves high accuracy in predicting users’ activities by basing tensor factorization on the semantics behind objects (e.g. item categories). However, SSTF currently focuses on the factorization of a tensor for a single service and thus has two problems: (1) </p></td></tr><tr><td>123</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_22">Algebraic Calculi for Weighted Ontology Alignments</a></td></tr><tr><td colspan=3><p>Alignments between ontologies usually come with numerical attributes expressing the confidence of each correspondence. Semantics supporting such confidences must generalise the semantics of alignments without confidence. There exists a semantics which satisfies this but introduces a discontinuity between weighted and non-weighted interpretations. Moreover, it does not provide a calculus for reasoning with weighted ontology alignments. This paper introduces a calculus for such alignments. It is given by an infinite relation-type algebra, the elements of which are weighted taxonomic relations. In addition, it approximates the non-weighted case in a continuous manner.</p></td></tr><tr><td>124</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_23">Ontologies for Knowledge Graphs: Breaking the Rules</a></td></tr><tr><td colspan=3><p>Large-scale knowledge graphs (KGs) are widely used in industry and academia, and provide excellent use-cases for ontologies. We find, however, that popular ontology languages, such as OWL and Datalog, cannot express even the most basic relationships on the normalised data format of KGs. Existential rules are more powerful, but may make reasoning undecidable. Normalising them to suit KGs often also destroys syntactic restrictions that ensure decidability and low complexity. We study this issue for several classes of existential rules and derive new syntactic criteria to recognise well-behaved rule-based ontologies over KGs.</p></td></tr><tr><td>125</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_24">An Extensible Linear Approach for Holistic Ontology Matching</a></td></tr><tr><td colspan=3><p>Resolving the semantic heterogeneity in the semantic web requires finding correspondences between ontologies describing resources. In particular, with the explosive growth of data sets in the Linked Open Data, linking multiple vocabularies and ontologies simultaneously, known as holistic matching problem, becomes necessary. Currently, most state-of-the-art matching approaches are limited to pairwise matching. In this paper, we propose a holistic ontology matching approach that is modeled through a linear program extending the maximum-weighted graph matching problem with linear constraints (cardinality, structural, and coherence constraints). Our approach guarantees the optimal solution with mostly coherent alignments. To evaluate our proposal, we discuss the results of experiments performed on the Conference track of the OAEI 2015, under both holistic and pairwise matching settings.</p></td></tr><tr><td>126</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_19">Walking Without a Map: Ranking-Based Traversal for Querying Linked Data</a></td></tr><tr><td colspan=3><p>The traversal-based approach to execute queries over Linked Data on the WWW fetches data by traversing data links and, thus, is able to make use of up-to-date data from initially unknown data sources. While the downside of this approach is the delay before the query engine completes a query execution, user perceived response time may be improved significantly by returning as many elements of the result set as soon as possible. To this end, the query engine requires a traversal strategy that enables the engine to fetch result-relevant data as early as possible. The challenge for such a strategy is that the query engine does not know a priori which of the data sources discovered during the query execution will contain result-relevant data. In this paper, we investigate 14 different approaches to rank traversal steps and achieve a variety of traversal strategies. We experimentally study their impact on response times and compare them to a baseline that resembles a breadth-first traversal. While our experiments show that some of the approaches can achieve noteworthy improvements over the baseline in a significant number of cases, we also observe that for every approach, there is a non-negligible chance to achieve response times that are worse than the baseline.</p></td></tr><tr><td>127</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_20">CubeQA—Question Answering on RDF Data Cubes</a></td></tr><tr><td colspan=3><p>Statistical data in the form of RDF Data Cubes is becoming increasingly valuable as it influences decisions in areas such as health care, policy and finance. While a growing amount is becoming freely available through the open data movement, this data is opaque to laypersons. Semantic Question Answering (SQA) technologies provide intuitive access via free-form natural language queries but general SQA systems cannot process RDF Data Cubes. On the intersection between RDF Data Cubes and SQA, we create a new subfield of SQA, called RDCQA. We create an RDQCA benchmark as task 3 of the QALD-6 evaluation challenge, to stimulate further research and enable quantitative comparison between RDCQA systems. We design and evaluate the domain independent CubeQA algorithm, which is the first RDCQA system and achieves a global </p></td></tr><tr><td>128</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_14">Seed, an End-User Text Composition Tool for the Semantic Web</a></td></tr><tr><td colspan=3><p>Despite developments of Semantic Web-enabling technologies, the gap between non-expert end-users and the Semantic Web still exists. In the field of semantic content authoring, tools for interacting with semantic content remain directed at highly trained individuals. This adds to the challenges of bringing user-generated content into the Semantic Web. In this paper, we present </p></td></tr><tr><td>129</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_18">Predicting Energy Consumption of Ontology Reasoning over Mobile Devices</a></td></tr><tr><td colspan=3><p>The unprecedented growth in mobile devices, combined with advances in Semantic Web (SW) Technologies, has given birth to opportunities for more intelligent systems on-the-go. Limited resources of mobile devices demand approaches that make mobile reasoning more applicable. While Mobile-Cloud integration is a promising method for harnessing the power of semantic technologies in the mobile infrastructure, it is an open question how to decide when to reason over ontologies on mobile devices. In this paper, we introduce an energy consumption prediction mechanism for ontology reasoning on mobile devices that allows an analysis of the feasibility of performing an ontology reasoning on a mobile device with respect to energy consumption. The developed prediction model contributes to mobile–cloud integration and helps to improve further developments in semantic reasoning in general.</p></td></tr><tr><td>130</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_15">Exception-Enriched Rule Learning from Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. To overcome this problem, we present a method for effective revision of learned Horn rules by adding exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction.</p></td></tr><tr><td>131</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_17">Explicit Query Interpretation and Diversification for Context-Driven Concept Search Across Ontologies</a></td></tr><tr><td colspan=3><p>Finding relevant concepts from a corpus of ontologies is useful in many scenarios, such as document classification, web page annotation, and automatic ontology population. Many millions of concepts are contained in a large number of ontologies across diverse domains. A SPARQL-based query demands the knowledge of the structure of ontologies and the query language, whereas user-friendlier and, simpler keyword-based approaches suffer from false positives. This is because concept descriptions in ontologies may be ambiguous and may overlap. In this paper, we propose a keyword-based concept search framework, which (1) exploits the structure and semantics in ontologies, by constructing </p></td></tr><tr><td>132</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_12">Are Names Meaningful? Quantifying Social Meaning on the Semantic Web</a></td></tr><tr><td colspan=3><p>According to its model-theoretic semantics, Semantic Web IRIs are individual constants or predicate letters whose names are chosen arbitrarily and carry no formal meaning. At the same time it is a well-known aspect of Semantic Web </p></td></tr><tr><td>133</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_13">User Validation in Ontology Alignment</a></td></tr><tr><td colspan=3><p>User validation is one of the challenges facing the ontology alignment community, as there are limits to the quality of automated alignment algorithms. In this paper we present a broad study on user validation of ontology alignments that encompasses three distinct but interrelated aspects: the profile of the user, the services of the alignment system, and its user interface. We discuss key issues pertaining to the alignment validation process under each of these aspects, and provide an overview of how current systems address them. Finally, we use experiments from the Interactive Matching track of the Ontology Alignment Evaluation Initiative (OAEI) 2015 to assess the impact of errors in alignment validation, and how systems cope with them as function of their services.</p></td></tr><tr><td>134</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_16">Planning Ahead: Stream-Driven Linked-Data Access Under Update-Budget Constraints</a></td></tr><tr><td colspan=3><p>Data stream applications are becoming increasingly popular on the web. In these applications, one query pattern is especially prominent: a join between a continuous data stream and some background data (BGD). Oftentimes, the target BGD is large, maintained externally, changing slowly, and costly to query (both in terms of time and money). Hence, practical applications usually maintain a local (cached) view of the relevant BGD. Given that these caches are not updated as the original BGD, they should be refreshed under realistic budget constraints (in terms of latency, computation time, and possibly financial cost) to avoid stale data leading to wrong answers. This paper proposes to model the join between streams and the BGD as a bipartite graph. By exploiting the graph structure, we keep the quality of results good enough without refreshing the entire cache for each evaluation. We also introduce two extensions to this method: first, we consider a continuous join between recent portions of a data stream and some BGD to focus on updates that have the longest effect. Second, we consider the future impact of a query to the BGD by proposing to delay some updates to provide fresher answers in future. By extending an existing stream processor with the proposed policies, we empirically show that we can improve result freshness by 93 % over baseline algorithms such as Random Selection or Least Recently Updated.</p></td></tr><tr><td>135</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_11">Updating DL-Lite Ontologies Through First-Order Queries</a></td></tr><tr><td colspan=3><p>In this paper we study instance-level update in </p></td></tr><tr><td>136</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_10">Knowledge Representation on the Web Revisited: The Case for Prototypes</a></td></tr><tr><td colspan=3><p>Recently, RDF and OWL have become the most common knowledge representation languages in use on the Web, propelled by the recommendation of the W3C. In this paper we examine an alternative way to represent knowledge based on Prototypes. This Prototype-based representation has different properties, which we argue to be more suitable for data sharing and reuse on the Web. Prototypes avoid the distinction between classes and instances and provide a means for object-based data sharing and reuse.</p></td></tr><tr><td>137</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_9">A Reuse-Based Annotation Approach for Medical Documents</a></td></tr><tr><td colspan=3><p>Annotations are useful to semantically enrich documents and other datasets with concepts of standardized vocabularies and ontologies. In the medical domain, many documents are not annotated at all and manual annotation is a difficult process making automatic annotation methods highly desirable to support human annotators. We propose a reuse-based annotation approach that utilizes previous annotations to annotate similar medical documents. The approach clusters items in documents such as medical forms according to previous ontology-based annotations and uses these clusters to determine candidate annotations for new items. The final annotations are selected according to a new context-based strategy that considers the co-occurrence and semantic relatedness of annotating concepts. The evaluation based on previous UMLS annotations of medical forms shows that the new approaches outperform a baseline approach as well as the use of the MetaMap tool for finding UMLS concepts in medical documents.</p></td></tr><tr><td>138</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_8">Efficient Algorithms for Association Finding and Frequent Association Pattern Mining</a></td></tr><tr><td colspan=3><p>Finding associations between entities is a common information need in many areas. It has been facilitated by the increasing amount of graph-structured data on the Web describing relations between entities. In this paper, we define an association connecting multiple entities in a graph as a minimal connected subgraph containing all of them. We propose an efficient graph search algorithm for finding associations, which prunes the search space by exploiting distances between entities computed based on a distance oracle. Having found a possibly large group of associations, we propose to mine frequent association patterns as a conceptual abstract summarizing notable subgroups to be explored, and present an efficient mining algorithm based on canonical codes and partitions. Extensive experiments on large, real RDF datasets demonstrate the efficiency of the proposed algorithms.</p></td></tr><tr><td>139</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_7">WebBrain: Joint Neural Learning of Large-Scale Commonsense Knowledge</a></td></tr><tr><td colspan=3><p>Despite the emergence and growth of numerous large knowledge graphs, many basic and important facts about our everyday world are not readily available on the Web. To address this, we present WebBrain, a new approach for harvesting commonsense knowledge that relies on joint learning from Web-scale data to fill gaps in the knowledge acquisition. We train a neural network model to learn relations based on large numbers of textual patterns found on the Web. At the same time, the model learns vector representations of general word semantics. This joint approach allows us to generalize beyond the explicitly extracted information. Experiments show that we can obtain representations of words that reflect their semantics, yet also allow us to capture conceptual relationships and commonsense knowledge.</p></td></tr><tr><td>140</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_49">dipLODocus</a></td></tr><tr><td colspan=3><p>The proliferation of semantic data on the Web requires RDF database systems to constantly improve their scalability and transactional efficiency. At the same time, users are increasingly interested in investigating or visualizing large collections of online data by performing complex analytic queries. This paper introduces a novel database system for RDF data management called dipLODocus</p></td></tr><tr><td>141</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_6">Containment of Expressive SPARQL Navigational Queries</a></td></tr><tr><td colspan=3><p>Query containment is one of the building block of query optimization techniques. In the relational world, query containment is a well-studied problem. At the same time it is well-understood that relational queries are not enough to cope with graph-structured data, where one is interested in expressing queries that capture </p></td></tr><tr><td>142</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_3">Ontop of Geospatial Databases</a></td></tr><tr><td colspan=3><p>We propose an OBDA approach for accessing geospatial data stored in relational databases, using the OGC standard GeoSPARQL and R2RML or OBDA mappings. We introduce extensions to an existing SPARQL-to-SQL translation method to support GeoSPARQL features. We describe the implementation of our approach in the system ontop-spatial, an extension of the OBDA system Ontop for creating virtual geospatial RDF graphs on top of geospatial relational databases. We present an experimental evaluation of our system using and extending a state-of-the-art benchmark. To measure the performance of our system, we compare it to a state-of-the-art geospatial RDF store and confirm its efficiency.</p></td></tr><tr><td>143</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_4">Expressive Multi-level Modeling for the Semantic Web</a></td></tr><tr><td colspan=3><p>In several subject domains, classes themselves may be subject to categorization, resulting in classes of classes (or “metaclasses”). When representing these domains, one needs to capture not only entities of different classification levels, but also their (intricate) relations. We observe that this is challenging in current Semantic Web languages, as there is little support to guide the modeler in producing correct multi-level ontologies, especially because of the nuances in the constraints that apply to entities of different classification levels and their relations. In order to address these representation challenges, we propose a vocabulary that can be used as a basis for multi-level ontologies in OWL along with a number of integrity constraints to prevent the construction of inconsistent models. In this process we employ an axiomatic theory called MLT (a Multi-Level Modeling Theory).</p></td></tr><tr><td>144</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_48">Enabling Fine-Grained HTTP Caching of SPARQL Query Results</a></td></tr><tr><td colspan=3><p>As SPARQL endpoints are increasingly used to serve linked data, their ability to scale becomes crucial. Although much work has been done to improve query evaluation, little has been done to take advantage of caching. Effective solutions for caching query results can improve scalability by reducing latency, network IO, and CPU overhead. We show that simple augmentation of the database indexes found in common SPARQL implementations can directly lead to effective caching at the HTTP protocol level. Using tests from the Berlin SPARQL benchmark, we evaluate the potential of such caching to improve overall efficiency of SPARQL query evaluation.</p></td></tr><tr><td>145</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_5">A Practical Acyclicity Notion for Query Answering Over </a></td></tr><tr><td colspan=3><p>Conjunctive query answering over expressive Horn Description Logic ontologies is a relevant and challenging problem which, in some cases, can be addressed by application of the chase algorithm. In this paper, we define a novel acyclicity notion which provides a sufficient condition for termination of the restricted chase over </p></td></tr><tr><td>146</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_47">Practical RDF Schema Reasoning with Annotated Semantic Web Data</a></td></tr><tr><td colspan=3><p>Semantic Web data with annotations is becoming available, being YAGO knowledge base a prominent example. In this paper we present an approach to perform the closure of large RDF Schema annotated semantic web data using standard database technology. In particular, we exploit several alternatives to address the problem of computing transitive closure with real fuzzy semantic data extracted from YAGO in the PostgreSQL database management system. We benchmark the several alternatives and compare to classical RDF Schema reasoning, providing the first implementation of annotated RDF schema in persistent storage.</p></td></tr><tr><td>147</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_1">Structuring Linked Data Search Results Using Probabilistic Soft Logic</a></td></tr><tr><td colspan=3><p>On-the-fly generation of integrated representations of Linked Data (LD) search results is challenging because it requires successfully automating a number of complex subtasks, such as structure inference and matching of both instances and concepts, each of which gives rise to uncertain outcomes. Such uncertainty is unavoidable given the semantically heterogeneous nature of web sources, including LD ones. This paper approaches the problem of structuring LD search results as an evidence-based one. In particular, the paper shows how one formalism (viz., probabilistic soft logic (PSL)) can be exploited to assimilate different sources of evidence in a principled way and to beneficial effect for users. The paper considers syntactic evidence derived from matching algorithms, semantic evidence derived from LD vocabularies, and user evidence, in the form of feedback. The main contributions are: sets of PSL rules that model the uniform assimilation of diverse kinds of evidence, an empirical evaluation of how the resulting PSL programs perform in terms of their ability to infer structure for integrating LD search results, and, finally, a concrete example of how populating such inferred structures for presentation to the end user is beneficial, besides enabling the collection of feedback whose assimilation further improves search result presentation.</p></td></tr><tr><td>148</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_44">Watermarking for Ontologies</a></td></tr><tr><td colspan=3><p>In this paper, we study watermarking methods to prove the ownership of an ontology. Different from existing approaches, we propose to watermark not by altering existing statements, but by removing them. Thereby, our approach does not introduce false statements into the ontology. We show how ownership of ontologies can be established with provably tight probability bounds, even if only parts of the ontology are being re-used. We finally demonstrate the viability of our approach on real-world ontologies.</p></td></tr><tr><td>149</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_45">Link Prediction for Annotation Graphs Using Graph Summarization</a></td></tr><tr><td colspan=3><p>Annotation graph datasets are a natural representation of scientific knowledge. They are common in the life sciences where genes or proteins are annotated with controlled vocabulary terms (CV terms) from ontologies. The W3C Linking Open Data (LOD) initiative and semantic Web technologies are playing a leading role in making such datasets widely available. Scientists can mine these datasets to discover patterns of annotation. While ontology alignment and integration across datasets has been explored in the context of the semantic Web, there is no current approach to mine such patterns in annotation graph datasets. In this paper, we propose a novel approach for link prediction; it is a preliminary task when discovering more complex patterns. Our prediction is based on a complementary methodology of graph summarization (GS) and dense subgraphs (DSG). GS can exploit and summarize knowledge captured within the ontologies and in the annotation patterns. DSG uses the ontology structure, in particular the distance between CV terms, to filter the graph, and to find promising subgraphs. We develop a scoring function based on multiple heuristics to rank the predictions. We perform an extensive evaluation on Arabidopsis thaliana genes.</p></td></tr><tr><td>150</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_46">QueryPIE: Backward Reasoning for OWL Horst over Very Large Knowledge Bases</a></td></tr><tr><td colspan=3><p>Both materialization and backward-chaining as different modes of performing inference have complementary advantages and disadvantages.</p></td></tr><tr><td>151</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_43">Repairing Ontologies for Incomplete Reasoners</a></td></tr><tr><td colspan=3><p>The need for scalable query answering often forces Semantic Web applications to use </p></td></tr><tr><td>152</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_41">Automatically Generating Data Linkages Using a Domain-Independent Candidate Selection Approach</a></td></tr><tr><td colspan=3><p>One challenge for Linked Data is scalably establishing high-quality </p></td></tr><tr><td>153</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_2">The Multiset Semantics of SPARQL Patterns</a></td></tr><tr><td colspan=3><p>The paper determines the algebraic and logic structure of the multiset semantics of the core patterns of SPARQL. We prove that the fragment formed by AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both, the intuitive multiset relational algebra (projection, selection, natural join, arithmetic union and except), and the multiset non-recursive Datalog with safe negation.</p></td></tr><tr><td>154</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_36">strukt—A Pattern System for Integrating Individual and Organizational Knowledge Work</a></td></tr><tr><td colspan=3><p>Expert-driven business process management is an established means for improving efficiency of organizational knowledge work. Implicit procedural knowledge in the organization is made explicit by defining processes. This approach is not applicable to individual knowledge work due to its high complexity and variability. However, without explicitly described processes there is no analysis and efficient communication of best practices of individual knowledge work within the organization. In addition, the activities of the individual knowledge work cannot be synchronized with the activities in the organizational knowledge work.</p></td></tr><tr><td>155</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_39">Local Closed World Semantics: Grounded Circumscription for OWL</a></td></tr><tr><td colspan=3><p>We present a new approach to adding closed world reasoning to the Web Ontology Language OWL. It transcends previous work on circumscriptive description logics which had the drawback of yielding an undecidable logic unless severe restrictions were imposed. In particular, it was not possible, in general, to apply local closure to roles.</p></td></tr><tr><td>156</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_38">FedX: Optimization Techniques for Federated Query Processing on Linked Data</a></td></tr><tr><td colspan=3><p>Motivated by the ongoing success of Linked Data and the growing amount of semantic data sources available on the Web, new challenges to query processing are emerging. Especially in distributed settings that require joining data provided by multiple sources, sophisticated optimization techniques are necessary for efficient query processing. We propose novel join processing and grouping techniques to minimize the number of remote requests, and develop an effective solution for source selection in the absence of preprocessed metadata. We present FedX, a practical framework that enables efficient SPARQL query processing on heterogeneous, </p></td></tr><tr><td>157</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_37">FedBench: A Benchmark Suite for Federated Semantic Data Query Processing</a></td></tr><tr><td colspan=3><p>In this paper we present </p></td></tr><tr><td>158</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_40">Extending Logic Programs with Description Logic Expressions for the Semantic Web</a></td></tr><tr><td colspan=3><p>Recently much attention has been directed to extending logic programming with description logic (DL) expressions, so that logic programs have access to DL knowledge bases and thus are able to reason with ontologies in the Semantic Web. In this paper, we propose a new extension of logic programs with DL expressions, called normal DL logic programs. In a normal DL logic program arbitrary DL expressions are allowed to appear in rule bodies and atomic DL expressions (i.e., atomic concepts and atomic roles) allowed in rule heads. We extend the key condition of well-supportedness for normal logic programs under the standard answer set semantics to normal DL logic programs and define an answer set semantics for DL logic programs which satisfies the extended well-supportedness condition. We show that the answer set semantics for normal DL logic programs is decidable if the underlying description logic is decidable (e.g. </p></td></tr><tr><td>159</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_42">A Machine Learning Approach to Multilingual and Cross-Lingual Ontology Matching</a></td></tr><tr><td colspan=3><p>Ontology matching is a task that has attracted considerable attention in recent years. With very few exceptions, however, research in ontology matching has focused primarily on the development of monolingual matching algorithms. As more and more resources become available in more than one language, novel algorithms are required which are capable of matching ontologies which share more than one language, or ontologies which are multilingual but do not share any languages. In this paper, we discuss several approaches to learning a matching function between two ontologies using a small set of manually aligned concepts, and evaluate them on different pairs of financial accounting standards, showing that multilingual information can indeed improve the matching quality, even in cross-lingual scenarios. In addition to this, as current research on ontology matching does not make a satisfactory distinction between multilingual and cross-lingual ontology matching, we provide precise definitions of these terms in relation to monolingual ontology matching, and quantify their effects on different matching algorithms.</p></td></tr><tr><td>160</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_34">An Ontology Design Pattern for Referential Qualities</a></td></tr><tr><td colspan=3><p>Referential qualities are qualities of an entity taken with reference to another entity. For example the vulnerability of a coast to sea level rise. In contrast to most non-relational qualities which only depend on their host, referential qualities require a referent additional to their host, i.e. a quality Q of an entity X taken with reference to another entity R. These qualities occur frequently in ecological systems, which make concepts from these systems challenging to model in formal ontology. In this paper, we discuss exemplary resilience, vulnerability and affordance as qualities of an entity taken with reference to an external factor. We suggest an ontology design pattern for referential qualities. The design pattern is anchored in the foundational ontology DOLCE and evaluated using implementations for the notions affordance, resilience and vulnerability.</p></td></tr><tr><td>161</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_35">Connecting the Dots: A Multi-pivot Approach to Data Exploration</a></td></tr><tr><td colspan=3><p>The purpose of data browsers is to help users identify and query data effectively without being overwhelmed by large complex graphs of data. A proposed solution to identify and query data in graph-based datasets is </p></td></tr><tr><td>162</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_33">Encyclopedic Knowledge Patterns from Wikipedia Links</a></td></tr><tr><td colspan=3><p>What is the most intuitive way of organizing concepts for describing things? What are the most relevant types of things that people use for describing other things? Wikipedia and Linked Data offer knowledge engineering researchers a chance to empirically identifying invariances in conceptual organization of knowledge i.e. knowledge patterns. In this paper, we present a resource of Encyclopedic Knowledge Patterns that have been discovered by analyizing the Wikipedia page links dataset, describe their evaluation with a user study, and discuss why it enables a number of research directions contributing to the realization of a meaningful Semantic Web.</p></td></tr><tr><td>163</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_31">Wheat and Chaff – Practically Feasible Interactive Ontology Revision</a></td></tr><tr><td colspan=3><p>When ontological knowledge is acquired automatically, quality control is essential. We consider the tightest possible approach – an exhaustive manual inspection of the acquired data. By using automated reasoning, we partially automate the process: after each expert decision, axioms that are entailed by the already approved statements are automatically approved, whereas axioms that would lead to an inconsistency are declined. Adequate axiom ranking strategies are essential in this setting to minimize the amount of expert decisions.</p></td></tr><tr><td>164</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_30">A Novel Approach to Visualizing and Navigating Ontologies</a></td></tr><tr><td colspan=3><p>Observational studies in the literature have highlighted low levels of user satisfaction in relation to the support for ontology visualization and exploration provided by current ontology engineering tools. These issues are particularly problematic for non-expert users, who rely on effective tool support to abstract from representational details and to be able to make sense of the contents and the structure of ontologies. To address these issues, we have developed a novel solution for visualizing and navigating ontologies, </p></td></tr><tr><td>165</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_26">Large Scale Fuzzy </a></td></tr><tr><td colspan=3><p>The MapReduce framework has proved to be very efficient for data-intensive tasks. Earlier work has tried to use MapReduce for large scale reasoning for </p></td></tr><tr><td>166</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_32">Getting the Meaning Right: A Complementary Distributional Layer for the Web Semantics</a></td></tr><tr><td colspan=3><p>We aim at providing a complementary layer for the web semantics, catering for bottom-up phenomena that are empirically </p></td></tr><tr><td>167</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_27">On Blank Nodes</a></td></tr><tr><td colspan=3><p>Blank nodes are defined in RDF as ‘existential variables’ in the same way that has been used before in mathematical logic. However, evidence suggests that actual usage of RDF does not follow this definition. In this paper we thoroughly cover the issue of blank nodes, from incomplete information in database theory, over different treatments of blank nodes across the W3C stack of RDF-related standards, to empirical analysis of RDF data publicly available on the Web. We then summarize alternative approaches to the problem, weighing up advantages and disadvantages, also discussing proposals for Skolemization.</p></td></tr><tr><td>168</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_29">DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data</a></td></tr><tr><td colspan=3><p>Triple stores are the backbone of increasingly many Data Web applications. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in general. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been converted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more useful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarks.</p></td></tr><tr><td>169</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_24">A Native and Adaptive Approach for Unified Processing of Linked Streams and Linked Data</a></td></tr><tr><td colspan=3><p>In this paper we address the problem of scalable, native and adaptive query processing over Linked Stream Data integrated with Linked Data. Linked Stream Data consists of data generated by stream sources, e.g., sensors, enriched with semantic descriptions, following the standards proposed for Linked Data. This enables the integration of stream data with Linked Data collections and facilitates a wide range of novel applications. Currently available systems use a “black box” approach which delegates the processing to other engines such as stream/event processing engines and SPARQL query processors by translating to their provided languages. As the experimental results described in this paper show, the need for query translation and data transformation, as well as the lack of full control over the query execution, pose major drawbacks in terms of efficiency. To remedy these drawbacks, we present CQELS (</p></td></tr><tr><td>170</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_28">Inspecting Regularities in Ontology Design Using Clustering</a></td></tr><tr><td colspan=3><p>We propose a novel application of clustering analysis to identify regularities in the usage of entities in axioms within an ontology. We argue that such regularities will be able to help to identify parts of the schemas and guidelines upon which ontologies are often built, especially in the absence of explicit documentation. Such analysis can also isolate irregular entities, thus highlighting possible deviations from the initial design. The clusters we obtain can be fully described in terms of generalised axioms that offer a synthetic representation of the detected regularity. In this paper we discuss the results of the application of our analysis to different ontologies and we discuss the potential advantages of incorporating it into future authoring tools.</p></td></tr><tr><td>171</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_21">Capturing Instance Level Ontology Evolution for DL-Lite</a></td></tr><tr><td colspan=3><p>Evolution of Knowledge Bases (KBs) expressed in Description Logics (DLs) proved its importance. Recent studies of the topic mostly focussed on model-based approaches (MBAs), where an evolution (of a KB) results in a set of models. For KBs expressed in tractable DLs, such as </p></td></tr><tr><td>172</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_19">Generating Resource Profiles by Exploiting the Context of Social Annotations</a></td></tr><tr><td colspan=3><p>Typical tagging systems merely capture that part of the tagging interactions that enrich the semantics of tag assignments according to the system’s purposes. The common practice is to build tag-based resource or user profiles on the basis of statistics about tags, disregarding the additional evidence that pertain to the resource, the user or the tag assignment itself. Thus, the main bulk of this valuable information is ignored when generating user or resource profiles.</p></td></tr><tr><td>173</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_23">ShareAlike Your Data: Self-referential Usage Policies for the Semantic Web</a></td></tr><tr><td colspan=3><p>Numerous forms of policies, licensing terms, and related conditions are associated with Web data and services. A natural goal for facilitating the re-use and re-combination of such content is to model usage policies as part of the data so as to enable their exchange and automated processing. This paper thus proposes a concrete policy modelling language. A particular difficulty are </p></td></tr><tr><td>174</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_22">Querying OWL 2 QL and Non-monotonic Rules</a></td></tr><tr><td colspan=3><p>Answering (conjunctive) queries is an important reasoning task in Description Logics (DL), hence also in highly expressive ontology languages, such as OWL. Extending such ontology languages with rules, such as those expressible in RIF-Core, and further with non-monotonic rules, integrating default negation as described in the RIF-FLD, yields an even more expressive language that allows for modeling defaults, exceptions, and integrity constraints.</p></td></tr><tr><td>175</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_17">Visualizing Ontologies: A Case Study</a></td></tr><tr><td colspan=3><p>Concept diagrams were introduced for precisely specifying ontologies in a manner more readily accessible to developers and other stakeholders than symbolic notations. In this paper, we present a case study on the use of concept diagrams in visually specifying the Semantic Sensor Networks (SSN) ontology. The SSN ontology was originally developed by an Incubator Group of the W3C. In the ontology, a sensor is a physical object that implements sensing and an observation is observed by a single sensor. These, and other, roles and concepts are captured visually, but precisely, by concept diagrams. We consider the lessons learnt from developing this visual model and show how to convert description logic axioms into concept diagrams. We also demonstrate how to merge simple concept diagram axioms into more complex axioms, whilst ensuring that diagrams remain relatively uncluttered.</p></td></tr><tr><td>176</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_18">LogMap: Logic-Based and Scalable Ontology Matching</a></td></tr><tr><td colspan=3><p>In this paper, we present LogMap—a highly scalable ontology matching system with ‘built-in’ reasoning and diagnosis capabilities. To the best of our knowledge, LogMap is the only matching system that can deal with semantically rich ontologies containing tens (and even hundreds) of thousands of classes. In contrast to most existing tools, LogMap also implements algorithms for ‘on the fly’ unsatisfiability detection and repair. Our experiments with the ontologies NCI, FMA and SNOMED CT confirm that our system can efficiently match even the largest existing bio-medical ontologies. Furthermore, LogMap is able to produce a ‘clean’ set of output mappings in many cases, in the sense that the ontology obtained by integrating LogMap’s output mappings with the input ontologies is consistent and does not contain unsatisfiable classes.</p></td></tr><tr><td>177</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_15">Verification of the OWL-Time Ontology</a></td></tr><tr><td colspan=3><p>Ontology verification is concerned with the relationship between the intended structures for an ontology and the models of the axiomatization of the ontology. The verification of a particular ontology requires characterization of the models of the ontology up to isomorphism and a proof that these models are equivalent to the intended structures for the ontology. In this paper we provide the verification of the ontology of time introduced by Hobbs and Pan, which is a first-order axiomatization of OWL-Time. We identify five modules within this ontology and present a complete account of the metatheoretic relationships among the modules and between other time ontologies for points and intervals.</p></td></tr><tr><td>178</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_14">Extracting Semantic User Networks from Informal Communication Exchanges</a></td></tr><tr><td colspan=3><p>Nowadays communication exchanges are an integral and time consuming part of people’s job, especially for the so called </p></td></tr><tr><td>179</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_16">The Cognitive Complexity of OWL Justifications</a></td></tr><tr><td colspan=3><p>In this paper, we present an approach to determining the cognitive complexity of justifications for entailments of OWL ontologies. We introduce a simple cognitive complexity model and present the results of validating that model via experiments involving OWL users. The validation is based on test data derived from a large and diverse corpus of naturally occurring justifications. Our contributions include validation for the cognitive complexity model, new insights into justification complexity, a significant corpus with novel analyses of justifications suitable for experimentation, and an experimental protocol suitable for model validation and refinement.</p></td></tr><tr><td>180</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_11">Labels in the Web of Data</a></td></tr><tr><td colspan=3><p>Entities on the Web of Data need to have labels in order to be exposable to humans in a meaningful way. These labels can then be used for exploring the data, i.e., for displaying the entities in a linked data browser or other front-end applications, but also to support keyword-based or natural-language based search over the Web of Data. Far too many applications fall back to exposing the URIs of the entities to the user in the absence of more easily understandable representations such as human-readable labels. In this work we introduce a number of label-related metrics: completeness of the labeling, the efficient accessibility of the labels, unambiguity of labeling, and the multilinguality of the labeling. We report our findings from measuring the Web of Data using these metrics. We also investigate which properties are used for labeling purposes, since many vocabularies define further labeling properties beyond the standard property from RDFS.</p></td></tr><tr><td>181</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_20">Concurrent Classification of </a></td></tr><tr><td colspan=3><p>We describe an optimised consequence-based procedure for classification of ontologies expressed in a polynomial fragment </p></td></tr><tr><td>182</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_12">Semantic Search: Reconciling Expressive Querying and Exploratory Search</a></td></tr><tr><td colspan=3><p>Faceted search and querying are two well-known paradigms to search the Semantic Web. Querying languages, such as SPARQL, offer expressive means for searching RDF datasets, but they are difficult to use. Query assistants help users to write well-formed queries, but they do not prevent empty results. Faceted search supports exploratory search, i.e., guided navigation that returns rich feedbacks to users, and prevents them to fall in dead-ends (empty results). However, faceted search systems do not offer the same expressiveness as query languages. We introduce </p></td></tr><tr><td>183</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_13">Effectively Interpreting Keyword Queries on RDF Databases with a Rear View</a></td></tr><tr><td colspan=3><p>Effective techniques for keyword search over RDF databases incorporate an explicit interpretation phase that maps keywords in a keyword query to structured query constructs. Because of the ambiguity of keyword queries, it is often not possible to generate a unique interpretation for a keyword query. Consequently, heuristics geared toward generating the top-K likeliest user-intended interpretations have been proposed. However, heuristics currently proposed fail to capture any user-dependent characteristics, but rather depend on database-dependent properties such as occurrence frequency of subgraph pattern connecting keywords. This leads to the problem of generating top-K interpretations that are not aligned with user intentions. In this paper, we propose a context-aware approach for keyword query interpretation that personalizes the interpretation process based on a user’s query context. Our approach addresses the novel problem of using a sequence of structured queries corresponding to interpretations of keyword queries in the query history as contextual information for biasing the interpretation of a new query. Experimental results presented over DBPedia dataset show that our approach outperforms the state-of-the-art technique on both efficiency and effectiveness, particularly for ambiguous queries.</p></td></tr><tr><td>184</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_9">Decomposition and Modular Structure of BioPortal Ontologies</a></td></tr><tr><td colspan=3><p>We present the first large scale investigation into the modular structure of a substantial collection of state-of-the-art biomedical ontologies, namely those maintained in the NCBO BioPortal repository. Using the notion of Atomic Decomposition, we partition BioPortal ontologies into logically coherent subsets (atoms), which are related to each other by a notion of dependency. We analyze various aspects of the resulting structures, and discuss their implications on applications of ontologies. In particular, we describe and investigate the usage of these ontology decompositions to extract modules, for instance, to facilitate matchmaking of semantic Web services in SSWAP (Simple Semantic Web Architecture and Protocol). Descriptions of those services use terms from BioPortal so service discovery requires reasoning with respect to relevant fragments of ontologies (i.e., modules). We present a novel algorithm for extracting modules from decomposed BioPortal ontologies which is able to quickly identify atoms that need to be included in a module to ensure logically complete reasoning. Compared to existing module extraction algorithms, it has a number of benefits, including improved performance and the possibility to avoid loading the entire ontology into memory. The algorithm is also evaluated on BioPortal ontologies and the results are presented and discussed.</p></td></tr><tr><td>185</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_8">RELIN: Relatedness and Informativeness-Based Centrality for Entity Summarization</a></td></tr><tr><td colspan=3><p>Linked Data is developing towards a large, global repository for structured, interlinked descriptions of real-world entities. An emerging problem in many Web applications making use of data like Linked Data is how a lengthy description can be tailored to the task of quickly identifying the underlying entity. As a solution to this novel problem of entity summarization, we propose RELIN, a variant of the random surfer model that leverages the relatedness and informativeness of description elements for ranking. We present an implementation of this conceptual model, which captures the semantics of description elements based on linguistic and information theory concepts. In experiments involving real-world data sets and users, our approach outperforms the baselines, producing summaries that better match handcrafted ones and further, shown to be useful in a concrete task.</p></td></tr><tr><td>186</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_10">A Clustering-Based Approach to Ontology Alignment</a></td></tr><tr><td colspan=3><p>Ontology alignment is an important problem for the linked data web, as more and more ontologies and ontology instances get published for specific domains such as government and healthcare. A number of (semi-)automated alignment systems have been proposed in recent years. Most combine a set of similarity functions on lexical, semantic and structural features to align ontologies. Although these functions work well in many cases of ontology alignments, they fail to capture alignments when terms or structure varies vastly across ontologies. In this case, one is forced to rely on manual alignment. In this paper, we study whether it is feasible to re-use such expert provided ontology alignments for new alignment tasks. We focus in particular on many-to-one alignments, where the opportunity for re-use is feasible if alignments are </p></td></tr><tr><td>187</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_6">Effective and Efficient Entity Search in RDF Data</a></td></tr><tr><td colspan=3><p>Triple stores have long provided RDF storage as well as data access using expressive, formal query languages such as SPARQL. The new end users of the Semantic Web, however, are mostly unaware of SPARQL and overwhelmingly prefer imprecise, informal keyword queries for searching over data. At the same time, the amount of data on the Semantic Web is approaching the limits of the architectures that provide support for the full expressivity of SPARQL. These factors combined have led to an increased interest in semantic search, i.e. access to RDF data using Information Retrieval methods. In this work, we propose a method for effective and efficient entity search over RDF data. We describe an adaptation of the BM25F ranking function for RDF data, and demonstrate that it outperforms other state-of-the-art methods in ranking RDF resources. We also propose a set of new index structures for efficient retrieval and ranking of results. We implement these results using the open-source MG4J framework.</p></td></tr><tr><td>188</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_7">An Empirical Study of Vocabulary Relatedness and Its Application to Recommender Systems</a></td></tr><tr><td colspan=3><p>When thousands of vocabularies having been published on the Semantic Web by various authorities, a question arises as to how they are related to each other. Existing work has mainly analyzed their similarity. In this paper, we inspect the more general notion of relatedness, and characterize it from four angles: well-defined semantic relatedness, lexical similarity in contents, closeness in expressivity and distributional relatedness. We present an empirical study of these measures on a large, real data set containing 2,996 vocabularies, and 15 million RDF documents that use them. Then, we propose to apply vocabulary relatedness to the problem of post-selection vocabulary recommendation. We implement such a recommender service as part of a vocabulary search engine, and test its effectiveness against a handcrafted gold standard.</p></td></tr><tr><td>189</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_3">Modelling and Analysis of User Behaviour in Online Communities</a></td></tr><tr><td colspan=3><p>Understanding and forecasting the health of an online community is of great value to its owners and managers who have vested interests in its longevity and success. Nevertheless, the association between community evolution and the behavioural patterns and trends of its members is not clearly understood, which hinders our ability of making accurate predictions of whether a community is flourishing or diminishing. In this paper we use statistical analysis, combined with a semantic model and rules for representing and computing behaviour in online communities. We apply this model on a number of forum communities from Boards.ie to categorise behaviour of community members over time, and report on how different behaviour compositions correlate with positive and negative community growth in these forums.</p></td></tr><tr><td>190</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_4">Alignment-Based Trust for Resource Finding in Semantic P2P Networks</a></td></tr><tr><td colspan=3><p>In a semantic P2P network, peers use separate ontologies and rely on alignments between their ontologies for translating queries. Nonetheless, alignments may be limited —unsound or incomplete— and generate flawed translations, leading to unsatisfactory answers. In this paper we present a trust mechanism that can assist peers to select those in the network that are better suited to answer their queries. The trust that a peer has towards another peer depends on a specific query and represents the probability that the latter peer will provide a satisfactory answer. In order to compute trust, we exploit both alignments and peers’ direct experience, and perform Bayesian inference. We have implemented our technique and conducted an evaluation. Experimental results showed that trust values converge as more queries are sent and answers received. Furthermore, the use of trust improves both precision and recall.</p></td></tr><tr><td>191</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_5">The Justificatory Structure of the NCBO BioPortal Ontologies</a></td></tr><tr><td colspan=3><p>Current ontology development tools offer debugging support by presenting justifications for entailments of OWL ontologies. While these minimal subsets have been shown to support debugging and understanding tasks, the occurrence of </p></td></tr><tr><td>192</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_44">Statistical Knowledge Patterns: Identifying Synonymous Relations in Large Linked Datasets</a></td></tr><tr><td colspan=3><p>The Web of Data is a rich common resource with billions of triples available in thousands of datasets and individual Web documents created by both expert and non-expert ontologists. A common problem is the imprecision in the use of vocabularies: annotators can misunderstand the semantics of a class or property or may not be able to find the right objects to annotate with. This decreases the quality of data and may eventually hamper its usability over large scale. This paper describes Statistical Knowledge Patterns (SKP) as a means to address this issue. SKPs encapsulate key information about ontology classes, including synonymous properties in (and across) datasets, and are automatically generated based on statistical data analysis. SKPs can be effectively used to automatically normalise data, and hence increase recall in querying. Both pattern extraction and pattern usage are completely automated. The main benefits of SKPs are that: (1) their structure allows for both accurate query expansion and restriction; (2) they are context dependent, hence they describe the usage and meaning of properties in the context of a particular class; and (3) they can be generated offline, hence the equivalence among relations can be used efficiently at run time.</p></td></tr><tr><td>193</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_43">Infrastructure for Efficient Exploration of Large Scale Linked Data via Contextual Tag Clouds</a></td></tr><tr><td colspan=3><p>In this paper we present the infrastructure of the contextual tag cloud system which can execute large volumes of queries about the number of instances that use particular ontological terms. The contextual tag cloud system is a novel application that helps users explore a large scale RDF dataset: the tags are ontological terms (classes and properties), the context is a set of tags that defines a subset of instances, and the font sizes reflect the number of instances that use each tag. It visualizes the patterns of instances specified by the context a user constructs. Given a request with a specific context, the system needs to quickly find what other tags the instances in the context use, and how many instances in the context use each tag. The key question we answer in this paper is how to scale to Linked Data; in particular we use a dataset with 1.4 billion triples and over 380,000 tags. This is complicated by the fact that the calculation should, when directed by the user, consider the entailment of taxonomic and/or domain/range axioms in the ontology. We combine a scalable preprocessing approach with a specially-constructed inverted index and use three approaches to prune unnecessary counts for faster intersection computations. We compare our system with a state-of-the-art triple store, examine how pruning rules interact with inference and analyze our design choices.</p></td></tr><tr><td>194</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-25073-6_2">ANAPSID: An Adaptive Query Processing Engine for SPARQL Endpoints</a></td></tr><tr><td colspan=3><p>Following the design rules of Linked Data, the number of available SPARQL endpoints that support remote query processing is quickly growing; however, because of the lack of adaptivity, query executions may frequently be unsuccessful. First, fixed plans identified following the traditional optimize-then-execute paradigm, may timeout as a consequence of endpoint availability. Second, because blocking operators are usually implemented, endpoint query engines are not able to incrementally produce results, and may become blocked if data sources stop sending data. We present ANAPSID, an adaptive query engine for SPARQL endpoints that adapts query execution schedulers to data availability and run-time conditions. ANAPSID provides physical SPARQL operators that detect when a source becomes blocked or data traffic is bursty, and opportunistically, the operators produce results as quickly as data arrives from the sources. Additionally, ANAPSID operators implement main memory replacement policies to move previously computed matches to secondary memory avoiding duplicates. We compared ANAPSID performance with respect to RDF stores and endpoints, and observed that ANAPSID speeds up execution time, in some cases, in more than one order of magnitude.</p></td></tr><tr><td>195</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_42">Discovering Missing Semantic Relations between Entities in Wikipedia</a></td></tr><tr><td colspan=3><p>Wikipedia’s infoboxes contain rich structured information of various entities, which have been explored by the DBpedia project to generate large scale Linked Data sets. Among all the infobox attributes, those attributes having hyperlinks in its values identify semantic relations between entities, which are important for creating RDF links between DBpedia’s instances. However, quite a few hyperlinks have not been anotated by editors in infoboxes, which causes lots of relations between entities being missing in Wikipedia. In this paper, we propose an approach for automatically discovering the missing entity links in Wikipedia’s infoboxes, so that the missing semantic relations between entities can be established. Our approach first identifies entity mentions in the given infoboxes, and then computes several features to estimate the possibilities that a given attribute value might link to a candidate entity. A learning model is used to obtain the weights of different features, and predict the destination entity for each attribute value. We evaluated our approach on the English Wikipedia data, the experimental results show that our approach can effectively find the missing relations between entities, and it significantly outperforms the baseline methods in terms of both precision and recall.</p></td></tr><tr><td>196</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_41">DynamiTE: Parallel Materialization of Dynamic RDF Data</a></td></tr><tr><td colspan=3><p>One of the main advantages of using semantically annotated data is that machines can </p></td></tr><tr><td>197</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_37">On the Status of Experimental Research on the Semantic Web</a></td></tr><tr><td colspan=3><p>Experimentation is an important way to validate results of Semantic Web and Computer Science research in general. In this paper, we investigate the development and the current status of experimental work on the Semantic Web. Based on a corpus of 500 papers collected from the International Semantic Web Conferences (ISWC) over the past decade, we analyse the importance and the quality of experimental research conducted and compare it to general Computer Science. We observe that the amount and quality of experiments are steadily increasing over time. Unlike hypothesised, we cannot confirm a statistically significant correlation between a paper’s citations and the amount of experimental work reported. Our analysis, however, shows that papers comparing themselves to other systems are more often cited than other papers.</p></td></tr><tr><td>198</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_39">QODI: Query as Context in Automatic Data Integration</a></td></tr><tr><td colspan=3><p>QODI is an automatic ontology-based data integration system (OBDI). QODI is distinguished in that the ontology mapping algorithm dynamically determines a partial mapping specific to the reformulation of each query. The query provides application context not available in the ontologies alone; thereby the system is able to disambiguate mappings for different queries. The mapping algorithm decomposes the query into a set of paths, and compares the set of paths with a similar decomposition of a source ontology. Using test sets from three real world applications, QODI achieves favorable results compared with AgreementMaker, a leading ontology matcher, and an ontology-based implementation of the mapping methods detailed for Clio, the state-of-the-art relational data integration and data exchange system.</p></td></tr><tr><td>199</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_38">A Graph-Based Approach to Learn Semantic Descriptions of Data Sources</a></td></tr><tr><td colspan=3><p>Semantic models of data sources and services provide support to automate many tasks such as source discovery, data integration, and service composition, but writing these semantic descriptions by hand is a tedious and time-consuming task. Most of the related work focuses on automatic annotation with classes or properties of source attributes or input and output parameters. However, constructing a source model that includes the relationships between the attributes in addition to their semantic types remains a largely unsolved problem. In this paper, we present a graph-based approach to hypothesize a rich semantic description of a new target source from a set of known sources that have been modeled over the same domain ontology. We exploit the domain ontology and the known source models to build a graph that represents the space of plausible source descriptions. Then, we compute the top </p></td></tr><tr><td>200</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_36">DAW: Duplicate-AWare Federated Query Processing over the Web of Data</a></td></tr><tr><td colspan=3><p>Over the last years the Web of Data has developed into a large compendium of interlinked data sets from multiple domains. Due to the decentralised architecture of this compendium, several of these datasets contain duplicated data. Yet, so far, only little attention has been paid to the effect of duplicated data on federated querying. This work presents DAW, a novel duplicate-aware approach to federated querying over the Web of Data. DAW is based on a combination of min-wise independent permutations and compact data summaries. It can be directly combined with existing federated query engines in order to achieve the same query recall values while querying fewer data sources. We extend three well-known federated query processing engines – DARQ, SPLENDID, and FedX – with DAW and compare our extensions with the original approaches. The comparison shows that DAW can greatly reduce the number of queries sent to the endpoints, while keeping high query recall values. Therefore, it can significantly improve the performance of federated query processing engines. Moreover, DAW provides a source selection mechanism that maximises the query recall, when the query processing is limited to a subset of the sources.</p></td></tr><tr><td>201</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_40">: Ranking Entity Types Using the Web of Data</a></td></tr><tr><td colspan=3><p>Much of Web search and browsing activity is today centered around entities. For this reason, Search Engine Result Pages (SERPs) increasingly contain information about the searched entities such as pictures, short summaries, related entities, and factual information. A key facet that is often displayed on the SERPs and that is instrumental for many applications is the entity </p></td></tr><tr><td>202</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_35">Ontology-Based Data Access: </a></td></tr><tr><td colspan=3><p>We present the architecture and technologies underpinning the OBDA system </p></td></tr><tr><td>203</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_34">Knowledge Graph Identification</a></td></tr><tr><td colspan=3><p>Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a </p></td></tr><tr><td>204</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_32">Type Inference on Noisy RDF Data</a></td></tr><tr><td colspan=3><p>Type information is very valuable in knowledge bases. However, most large open knowledge bases are incomplete with respect to type information, and, at the same time, contain noisy and incorrect data. That makes classic type inference by reasoning difficult. In this paper, we propose the heuristic link-based type inference mechanism </p></td></tr><tr><td>205</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_33">What’s in a ‘nym’? Synonyms in Biomedical Ontology Matching</a></td></tr><tr><td colspan=3><p>To bring the Life Sciences domain closer to a Semantic Web realization it is fundamental to establish meaningful relations between biomedical ontologies. The successful application of ontology matching techniques is strongly tied to an effective exploration of the complex and diverse biomedical terminology contained in biomedical ontologies. In this paper, we present an overview of the lexical components of several biomedical ontologies and investigate how different approaches for their use can impact the performance of ontology matching techniques. We propose novel approaches for exploring the different types of synonyms encoded by the ontologies and for extending them based both on internal synonym derivation and on external ontologies.</p></td></tr><tr><td>206</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_30">Personalized Best Answer Computation in Graph Databases</a></td></tr><tr><td colspan=3><p>Though subgraph matching has been extensively studied as a query paradigm in semantic web and social network data environments, a user can get a large number of answers in response to a query. Just like Google does, these answers can be shown to the user in accordance with an importance ranking. In this paper, we present scalable algorithms to find the top-</p></td></tr><tr><td>207</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_27">FedSearch: Efficiently Combining Structured Queries and Full-Text Search in a SPARQL Federation</a></td></tr><tr><td colspan=3><p>Combining structured queries with full-text search provides a powerful means to access distributed linked data. However, executing hybrid search queries in a federation of multiple data sources presents a number of challenges due to data source heterogeneity and lack of statistical data about keyword selectivity. To address these challenges, we present FedSearch – a novel hybrid query engine based on the SPARQL federation framework FedX. We extend the SPARQL algebra to incorporate keyword search clauses as first-class citizens and apply novel optimization techniques to improve the query processing efficiency while maintaining a meaningful ranking of results. By performing on-the-fly adaptation of the query execution plan and intelligent grouping of query clauses, we are able to reduce significantly the communication costs making our approach suitable for top-</p></td></tr><tr><td>208</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_29">Exploring Scholarly Data with Rexplore</a></td></tr><tr><td colspan=3><p>Despite the large number and variety of tools and services available today for exploring scholarly data, current support is still very limited in the context of sensemaking tasks, which go beyond standard search and ranking of authors and publications, and focus instead on i) understanding the dynamics of research areas, ii) relating authors ‘semantically’ (e.g., in terms of common interests or shared academic trajectories), or iii) performing fine-grained academic expert search along multiple dimensions. To address this gap we have developed a novel tool, Rexplore, which integrates statistical analysis, semantic technologies, and visual analytics to provide effective support for exploring and making sense of scholarly data. Here, we describe the main innovative elements of the tool and we present the results from a task-centric empirical evaluation, which shows that Rexplore is highly effective at providing support for the aforementioned sensemaking tasks. In addition, these results are robust both with respect to the background of the users (i.e., expert analysts vs. ‘ordinary’ users) and also with respect to whether the tasks are selected by the evaluators or proposed by the users themselves.</p></td></tr><tr><td>209</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_25">ORCHID – Reduction-Ratio-Optimal Computation of Geo-spatial Distances for Link Discovery</a></td></tr><tr><td colspan=3><p>The discovery of links between resources within knowledge bases is of crucial importance to realize the vision of the Semantic Web. Addressing this task is especially challenging when dealing with geo-spatial datasets due to their sheer size and the potential complexity of single geo-spatial objects. Yet, so far, little attention has been paid to the characteristics of geo-spatial data within the context of link discovery. In this paper, we address this gap by presenting </p></td></tr><tr><td>210</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_28">Getting Lucky in Ontology Search: A Data-Driven Evaluation Framework for Ontology Ranking</a></td></tr><tr><td colspan=3><p>With hundreds, if not thousands, of ontologies available today in many different domains, ontology search and ranking has become an important and timely problem. When a user searches a collection of ontologies for her terms of interest, there are often dozens of ontologies that contain these terms. How does she know which ontology is the most relevant to her search? Our research group hosts BioPortal, a public repository of more than 330 ontologies in the biomedical domain. When a term that a user searches for is available in multiple ontologies, how do we rank the results and how do we measure how well our ranking works? In this paper, we develop an evaluation framework that enables developers to compare and analyze the performance of different ontology-ranking methods. Our framework is based on processing search logs and determining how often users select the top link that the search engine offers. We evaluate our framework by analyzing the data on BioPortal searches. We explore several different ranking algorithms and measure the effectiveness of each ranking by measuring how often users click on the highest ranked ontology. We collected log data from more than 4,800 BioPortal searches. Our results show that regardless of the ranking, in more than half the searches, users select the first link. Thus, it is even more critical to ensure that the ranking is appropriate if we want to have satisfied users. Our further analysis demonstrates that ranking ontologies based on page view data significantly improves the user experience, with an approximately 26% increase in the number of users who select the highest ranked ontology for the search.</p></td></tr><tr><td>211</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_26">Simplifying Description Logic Ontologies</a></td></tr><tr><td colspan=3><p>We discuss the problem of minimizing TBoxes expressed in the lightweight description logic </p></td></tr><tr><td>212</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_31">Towards an Automatic Creation of Localized Versions of DBpedia</a></td></tr><tr><td colspan=3><p>DBpedia is a large-scale knowledge base that exploits Wikipedia as primary data source. The extraction procedure requires to manually map Wikipedia infoboxes into the DBpedia ontology. Thanks to crowdsourcing, a large number of infoboxes has been mapped in the English DBpedia. Consequently, the same procedure has been applied to other languages to create the localized versions of DBpedia. However, the number of accomplished mappings is still small and limited to most frequent infoboxes. Furthermore, mappings need maintenance due to the constant and quick changes of Wikipedia articles. In this paper, we focus on the problem of automatically mapping infobox attributes to properties into the DBpedia ontology for extending the coverage of the existing localized versions or building from scratch versions for languages not covered in the current version. The evaluation has been performed on the Italian mappings. We compared our results with the current mappings on a random sample re-annotated by the authors. We report results comparable to the ones obtained by a human annotator in term of precision, but our approach leads to a significant improvement in recall and speed. Specifically, we mapped 45,978 Wikipedia infobox attributes to DBpedia properties in 14 different languages for which mappings were not yet available. The resource is made available in an open format.</p></td></tr><tr><td>213</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_24">Bringing Math to LOD: A Semantic Publishing Platform Prototype for Scientific Collections in Mathematics</a></td></tr><tr><td colspan=3><p>We present our work on developing a software platform for mining mathematical scholarly papers to obtain a Linked Data representation. Currently, the Linking Open Data (LOD) cloud lacks up-to-date and detailed information on professional level mathematics. To our mind, the main reason for that is the absence of appropriate tools that could analyze the underlying semantics in mathematical papers and effectively build their consolidated representation. We have developed a holistic approach to analysis of mathematical documents, including ontology based extraction, conversion of the article body as well as its metadata into RDF, integration with some existing LOD data sets, and semantic search. We argue that the platform may be helpful for enriching user experience on modern online scientific collections.</p></td></tr><tr><td>214</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_20">The Combined Approach to OBDA: Taming Role Hierarchies Using Filters</a></td></tr><tr><td colspan=3><p>The basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with infinite materializations. While this approach is efficient and scalable for ontologies that are formulated in the basic version of the description logic DL-Lite, it incurs an exponential blowup during query rewriting when DL-Lite is extended with the popular role hierarchies. In this paper, we show how to replace the query rewriting with a filtering technique. This is natural from an implementation perspective and allows us to handle role hierarchies without an exponential blowup. We also carry out an experimental evaluation that demonstrates the scalability of this approach.</p></td></tr><tr><td>215</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_23">Semantic Message Passing for Generating Linked Data from Tables</a></td></tr><tr><td colspan=3><p>We describe work on automatically inferring the intended meaning of tables and representing it as RDF linked data, making it available for improving search, interoperability and integration. We present implementation details of a joint inference module that uses knowledge from the linked open data (LOD) cloud to jointly infer the semantics of column headers, table cell values (e.g., strings and numbers) and relations between columns. We also implement a novel </p></td></tr><tr><td>216</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_21">A Snapshot of the OWL Web</a></td></tr><tr><td colspan=3><p>Tool development for and empirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as input for testing and evaluation purposes and detailed characterisations of real ontologies. Empirical activities often resort to (somewhat arbitrarily) hand curated corpora available on the web, such as the NCBO BioPortal and the TONES Repository, or manually selected sets of well-known ontologies. Findings of surveys and results of benchmarking activities may be biased, even heavily, towards these datasets. Sampling from a large corpus of ontologies, on the other hand, may lead to more representative results. Current large scale repositories and web crawls are mostly uncurated and suffer from duplication, small and (for many purposes) uninteresting ontology files, and contain large numbers of ontology versions, variants, and facets, and therefore do not lend themselves to random sampling. In this paper, we survey ontologies as they exist on the web and describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling, various forms of de-duplications and manual cleaning, which allows random sampling of ontologies for a variety of empirical applications.</p></td></tr><tr><td>217</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_19">Towards Constructive Evidence of Data Flow-Oriented Web Service Composition</a></td></tr><tr><td colspan=3><p>Automation of service composition is one of the most interesting challenges facing the Semantic Web and the Web of services today. Despite approaches which are able to infer a partial order of services, its data flow remains implicit and difficult to be automatically generated. Enhanced with formal representations, the semantic links between output and input parameters of services can be then exploited to infer their data flow. This work addresses the problem of effectively inferring data flow between services based on their representations. To this end, we introduce the non standard Description Logic reasoning </p></td></tr><tr><td>218</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_22">Semantic Rule Filtering for Web-Scale Relation Extraction</a></td></tr><tr><td colspan=3><p>Web-scale relation extraction is a means for building and extending large repositories of formalized knowledge. This type of automated knowledge building requires a decent level of precision, which is hard to achieve with automatically acquired rule sets learned from unlabeled data by means of distant or minimal supervision. This paper shows how precision of relation extraction can be considerably improved by employing a wide-coverage, general-purpose lexical semantic network, i.e., BabelNet, for effective semantic rule filtering. We apply Word Sense Disambiguation to the content words of the automatically extracted rules. As a result a set of relation-specific relevant concepts is obtained, and each of these concepts is then used to represent the structured semantics of the corresponding relation. The resulting relation-specific subgraphs of BabelNet are used as semantic filters for estimating the adequacy of the extracted rules. For the seven semantic relations tested here, the semantic filter consistently yields a higher precision at any relative recall value in the high-recall range.</p></td></tr><tr><td>219</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_17">A Decision Procedure for </a></td></tr><tr><td colspan=3><p>The Semantic Web makes an extensive use of the OWL DL ontology language, underlied by the </p></td></tr><tr><td>220</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_18">Elastic and Scalable Processing of Linked Stream Data in the Cloud</a></td></tr><tr><td colspan=3><p>Linked Stream Data extends the Linked Data paradigm to dynamic data sources. It enables the integration and joint processing of heterogeneous stream data with quasi-static data from the Linked Data Cloud in near-real-time. Several Linked Stream Data processing engines exist but their scalability still needs to be in improved in terms of (static and dynamic) data sizes, number of concurrent queries, stream update frequencies, etc. So far, none of them supports parallel processing in the Cloud, i.e., elastic load profiles in a hosted environment. To remedy these limitations, this paper presents an approach for elastically parallelizing the continuous execution of queries over Linked Stream Data. For this, we have developed novel, highly efficient, and scalable parallel algorithms for continuous query operators. Our approach and algorithms are implemented in our CQELS Cloud system and we present extensive evaluations of their superior performance on Amazon EC2 demonstrating their high scalability and excellent elasticity in a real deployment.</p></td></tr><tr><td>221</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_15">Incremental Reasoning in OWL EL without Bookkeeping</a></td></tr><tr><td colspan=3><p>We describe a method for updating the classification of ontologies expressed in the </p></td></tr><tr><td>222</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_16">Secure Manipulation of Linked Data</a></td></tr><tr><td colspan=3><p>When it comes to publishing data on the web, the level of access control required (if any) is highly dependent on the type of content exposed. Up until now RDF data publishers have focused on exposing and linking public data. With the advent of SPARQL 1.1, the linked data infrastructure can be used, not only as a means of publishing open data but also, as a general mechanism for managing distributed graph data. However, such a decentralised architecture brings with it a number of additional challenges with respect to both data security and integrity. In this paper, we propose a general authorisation framework that can be used to deliver dynamic query results based on user credentials and to cater for the secure manipulation of linked data. Specifically we describe how graph patterns, propagation rules, conflict resolution policies and integrity constraints can together be used to specify and enforce consistent access control policies.</p></td></tr><tr><td>223</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_14">A Query Tool for </a></td></tr><tr><td colspan=3><p>We present the Protégé plug-in NoHR that allows the user to take an </p></td></tr><tr><td>224</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_12">ProSWIP: Property-Based Data Access for Semantic Web Interactive Programming</a></td></tr><tr><td colspan=3><p>The Semantic Web has matured from a mere theoretical vision to a variety of ready-to-use linked open data sources currently available on the Web. Still, with respect to application development, the Web community is just starting to develop new paradigms in which data as the main driver of applications is promoted to first class status. Relying on properties of resources as an indicator for the type, property-based typing is such a paradigm. In this paper, we inspect the feasibility of property-based typing for accessing data from the linked open data cloud. Problems in terms of transparency and quality of the selected data were noticeable. To alleviate these problems, we developed an iterative approach that builds on human feedback.</p></td></tr><tr><td>225</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_11">Federated Entity Search Using On-the-Fly Consolidation</a></td></tr><tr><td colspan=3><p>Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in different sources, which may provide only limited information about their content and are therefore called “uncooperative”. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of </p></td></tr><tr><td>226</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_10">One License to Compose Them All</a></td></tr><tr><td colspan=3><p>In the domain of Linked Open Data a need is emerging for developing automated frameworks able to generate the licensing terms associated to data coming from heterogeneous distributed sources. This paper proposes and evaluates a deontic logic semantics which allows us to define the deontic components of the licenses, i.e., permissions, obligations, and prohibitions, and generate a composite license compliant with the licensing items of the composed different licenses. Some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs w.r.t. the data she is publishing.</p></td></tr><tr><td>227</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_8">Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation</a></td></tr><tr><td colspan=3><p>Research effort in ontology visualization has largely focused on developing new visualization techniques. At the same time, researchers have paid less attention to investigating the usability of common visualization techniques that many practitioners regularly use to visualize ontological data. In this paper, we focus on two popular ontology visualization techniques: indented tree and graph. We conduct a controlled usability study with an emphasis on the effectiveness, efficiency, workload and satisfaction of these visualization techniques in the context of assisting users during evaluation of ontology mappings. Findings from this study have revealed both strengths and weaknesses of each visualization technique. In particular, while the indented tree visualization is more organized and familiar to novice users, subjects found the graph visualization to be more controllable and intuitive without visual redundancy, particularly for ontologies with multiple inheritance.</p></td></tr><tr><td>228</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_7">The Logic of Extensional RDFS</a></td></tr><tr><td colspan=3><p>The normative version of RDF Schema (RDFS) gives non-standard (intensional) interpretations to some standard notions such as classes and properties, thus departing from standard set-based semantics. In this paper we develop a standard set-based (extensional) semantics for the RDFS vocabulary while preserving the simplicity and computational complexity of deduction of the intensional version. This result can positively impact current implementations, as reasoning in RDFS can be implemented following common set-based intuitions and be compatible with OWL extensions.</p></td></tr><tr><td>229</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_13">Simplified OWL Ontology Editing for the Web: Is WebProtégé Enough?</a></td></tr><tr><td colspan=3><p>Ontology engineering is a task that is notorious for its difficulty. As the group that developed Protégé, the most widely used ontology editor, we are keenly aware of how difficult the users perceive this task to be. In this paper, we present the new version of WebProtégé that we designed with two main goals in mind: (1) create a tool that will be easy to use while still accounting for commonly used OWL constructs; (2) support collaboration and social interaction around distributed ontology editing as part of the core tool design. We designed this new version of the WebProtégé user interface empirically, by analysing the use of OWL constructs in a large corpus of publicly available ontologies. Since the beta release of this new WebProtégé interface in January 2013, our users from around the world have created and uploaded 519 ontologies on our server. In this paper, we describe the key features of the new tool and our empirical design approach. We evaluate language coverage in WebProtégé by assessing how well it covers the OWL constructs that are present in ontologies that users have uploaded to WebProtégé. We evaluate the usability of WebProtégé through a usability survey. Our analysis validates our empirical design, suggests additional language constructors to explore, and demonstrates that an easy-to-use web-based tool that covers most of the frequently used OWL constructs is sufficient for many users to start editing their ontologies.</p></td></tr><tr><td>230</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_5">Completeness Statements about RDF Data Sources and Their Use for Query Answering</a></td></tr><tr><td colspan=3><p>With thousands of RDF data sources available on the Web covering disparate and possibly overlapping knowledge domains, the problem of providing high-level descriptions (in the form of metadata) of their content becomes crucial. In this paper we introduce a theoretical framework for describing data sources in terms of their completeness. We show how existing data sources can be described with completeness statements expressed in RDF. We then focus on the problem of the completeness of query answering over plain and RDFS data sources augmented with completeness statements. Finally, we present an extension of the completeness framework for federated data sources.</p></td></tr><tr><td>231</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_4">Controlled Query Evaluation over OWL 2 RL Ontologies</a></td></tr><tr><td colspan=3><p>We study confidentiality enforcement in ontology-based information systems where ontologies are expressed in OWL 2 RL, a profile of OWL 2 that is becoming increasingly popular in Semantic Web applications. We formalise a natural adaptation of the Controlled Query Evaluation (CQE) framework to ontologies. Our goal is to provide CQE algorithms that (i) ensure confidentiality of sensitive information; (ii) are efficiently implementable by means of RDF triple store technologies; and (iii) ensure maximality of the answers returned by the system to user queries (thus restricting access to information as little as possible). We formally show that these requirements are in conflict and cannot be satisfied without imposing restrictions on ontologies. We propose a fragment of OWL 2 RL for which all three requirements can be satisfied. For the identified fragment, we design a CQE algorithm that has the same computational complexity as standard query answering and can be implemented by relying on state-of-the-art triple stores.</p></td></tr><tr><td>232</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_6">Empirical Study of Logic-Based Modules: Cheap Is Cheerful</a></td></tr><tr><td colspan=3><p>For ontology reuse and integration, a number of approaches have been devised that aim at identifying modules, i.e., suitably small sets of “relevant” axioms from ontologies. Here we consider three logically sound notions of modules: </p></td></tr><tr><td>233</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_3">Pattern Based Knowledge Base Enrichment</a></td></tr><tr><td colspan=3><p>Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. Having such schemata allows more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. In this article, we propose a semi-automatic schemata construction approach addressing this problem: First, the frequency of axiom patterns in existing knowledge bases is discovered. Afterwards, those patterns are converted to SPARQL based pattern detection algorithms, which allow to enrich knowledge base schemata. We argue that we present the first scalable knowledge base enrichment approach based on real schema usage patterns. The approach is evaluated on a large set of knowledge bases with a quantitative and qualitative result analysis.</p></td></tr><tr><td>234</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_9">Real-Time RDF Extraction from Unstructured Data Streams</a></td></tr><tr><td colspan=3><p>The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured data, thus creating a representation of general knowledge. However, most of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and massive extraction of RDF facts from unstructured data, has remained open so far. The availability of such knowledge on the Web of Data would provide significant benefits to manifold applications including news retrieval, sentiment analysis and business intelligence. In this paper, we address the problem of the actuality of the Web of Data by presenting an approach that allows extracting RDF triples from unstructured data streams. We employ statistical methods in combination with deduplication, disambiguation and unsupervised as well as supervised machine learning techniques to create a knowledge base that reflects the content of the input streams. We evaluate a sample of the RDF we generate against a large corpus of news streams and show that we achieve a precision of more than 85%.</p></td></tr><tr><td>235</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_1">TRM – Learning Dependencies between Text and Structure with Topical Relational Models</a></td></tr><tr><td colspan=3><p>Text-rich structured data become more and more ubiquitous on the Web and on the enterprise databases by encoding heterogeneous structural information between entities such as people, locations, or organizations and the associated textual information. For analyzing this type of data, existing topic modeling approaches, which are highly tailored toward document collections, require manually-defined regularization terms to exploit and to bias the topic learning towards structure information. We propose an approach, called </p></td></tr><tr><td>236</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-41335-3_2">A Confidentiality Model for Ontologies</a></td></tr><tr><td colspan=3><p>We illustrate several novel attacks to the confidentiality of knowledge bases (KB). Then we introduce a new confidentiality model, sensitive enough to detect those attacks, and a method for constructing secure KB views. We identify safe approximations of the background knowledge exploited in the attacks; they can be used to reduce the complexity of constructing secure KB views.</p></td></tr><tr><td>237</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_36">Explaining and Suggesting Relatedness in Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Knowledge graphs (KGs) are a key ingredient for searching, browsing and knowledge discovery activities. Motivated by the need to harness knowledge available in a variety of KGs, we face the following two problems. First, given a pair of entities defined in some KG, find an explanation of their relatedness. We formalize the notion of relatedness explanation and introduce different criteria to build explanations based on information-theory, diversity and their combinations. Second, given a pair of entities, find other (pairs of) entities sharing a similar relatedness perspective. We describe an implementation of our ideas in a tool, called </p></td></tr><tr><td>238</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_37">Type-Constrained Representation Learning in Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data.</p></td></tr><tr><td>239</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_35">Content-Based Recommendations via DBpedia and Freebase: A Case Study in the Music Domain</a></td></tr><tr><td colspan=3><p>The </p></td></tr><tr><td>240</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_33">Next Step for NoHR: OWL 2 QL</a></td></tr><tr><td colspan=3><p>The Protégé plug-in NoHR allows the user to combine an OWL 2 EL ontology with a set of non-monotonic (logic programming) rules – suitable, e.g., to express defaults and exceptions – and query the combined knowledge base (KB). The formal approach realized in NoHR is polynomial (w.r.t. data complexity) and it has been shown that even very large health care ontologies, such as SNOMED CT, can be handled. As each of the tractable OWL profiles is motivated by different application cases, extending the tool to the other profiles is of particular interest, also because these preserve the polynomial data complexity of the combined formalism. Yet, a straightforward adaptation of the existing approach to OWL 2 QL turns out to not be viable. In this paper, we provide the non-trivial solution for the extension of NoHR to OWL 2 QL by directly translating the ontology into rules without any prior classification. We have implemented our approach and our evaluation shows encouraging results.</p></td></tr><tr><td>241</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_32">Understanding How Users Edit Ontologies: Comparing Hypotheses About Four Real-World Projects</a></td></tr><tr><td colspan=3><p>Ontologies are complex intellectual artifacts and creating them requires significant expertise and effort. While existing ontology-editing tools and methodologies propose ways of building ontologies in a normative way, empirical investigations of how experts </p></td></tr><tr><td>242</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_34">Concept Forgetting in </a></td></tr><tr><td colspan=3><p>We present a method for forgetting concept symbols in ontologies specified in the description logic </p></td></tr><tr><td>243</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_30">Interest-Based RDF Update Propagation</a></td></tr><tr><td colspan=3><p>Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and process large amounts of requests from diverse applications. Many data products and services rely on full or partial local LOD replications to ensure faster querying and processing. Given the evolving nature of the original and authoritative datasets, to ensure consistent and up-to-date replicas frequent replacements are required at a great cost. In this paper, we introduce an approach for interest-based RDF update propagation, which propagates only interesting parts of updates from the source to the target dataset. Effectively, this enables remote applications to ‘subscribe’ to relevant datasets and consistently reflect the necessary changes locally without the need to frequently replace the entire dataset (or a relevant subset). Our approach is based on a formal definition for graph-pattern-based interest expressions that is used to filter interesting parts of updates from the source. We implement the approach in the iRap framework and perform a comprehensive evaluation based on DBpedia Live updates, to confirm the validity and value of our approach.</p></td></tr><tr><td>244</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_31">General Terminology Induction in OWL</a></td></tr><tr><td colspan=3><p>Automated acquisition, or learning, of ontologies has attracted research attention because it can help ontology engineers build ontologies and give domain experts new insights into their data. However, existing approaches to ontology learning are considerably limited, e.g. focus on learning descriptions for given classes, require intense supervision and human involvement, make assumptions about data, do not fully respect background knowledge. We investigate the problem of general terminology induction, i.e. learning sets of general class inclusions, GCIs, from data and background knowledge. We introduce measures that evaluate logical and statistical quality of a set of GCIs. We present methods to compute these measures and an anytime algorithm that induces sets of GCIs. Our experiments show that we can acquire interesting sets of GCIs and provide insights into the structure of the search space.</p></td></tr><tr><td>245</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_29">A Flexible Framework for Understanding the Dynamics of Evolving RDF Datasets</a></td></tr><tr><td colspan=3><p>The dynamic nature of Web data gives rise to a multitude of problems related to the description and analysis of the evolution of RDF datasets, which are important to a large number of users and domains, such as, the curators of biological information where changes are constant and interrelated. In this paper, we propose a framework that enables identifying, analysing and understanding these dynamics. Our approach is flexible enough to capture the peculiarities and needs of different applications on dynamic data, while being formally robust due to the satisfaction of the completeness and unambiguity properties. In addition, our framework allows the persistent representation of the detected changes between versions, in a manner that enables easy and efficient navigation among versions, automated processing and analysis of changes, cross-snapshot queries (spanning across different versions), as well as queries involving both changes and data. Our work is evaluated using real Linked Open Data, and exhibits good scalability properties.</p></td></tr><tr><td>246</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_28">Improving Entity Retrieval on Structured Data</a></td></tr><tr><td colspan=3><p>The increasing amount of data on the Web, in particular of Linked Data, has led to a diverse landscape of datasets, which make entity retrieval a challenging task. Explicit cross-dataset links, for instance to indicate co-references or related entities can significantly improve entity retrieval. However, only a small fraction of entities are interlinked through explicit statements. In this paper, we propose a two-fold entity retrieval approach. In a first, offline preprocessing step, we cluster entities based on the </p></td></tr><tr><td>247</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_27">SANAPHOR: Ontology-Based Coreference Resolution</a></td></tr><tr><td colspan=3><p>We tackle the problem of resolving coreferences in textual content by leveraging Semantic Web techniques. Specifically, we focus on noun phrases that coreference identifiable entities that appear in the text; the challenge in this context is to improve the coreference resolution by leveraging potential semantic annotations that can be added to the identified mentions. Our system, </p></td></tr><tr><td>248</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_26">Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation</a></td></tr><tr><td colspan=3><p>Semantic relatedness and disambiguation are fundamental problems for linking text documents to the Web of Data. There are many approaches dealing with both problems but most of them rely on word or concept distribution over Wikipedia. They are therefore not applicable to concepts that do not have a rich textual description. In this paper, we show that semantic relatedness can also be accurately computed by analysing only the graph structure of the knowledge base. In addition, we propose a joint approach to entity and word-sense disambiguation that makes use of graph-based relatedness. As opposed to the majority of state-of-the-art systems that target mainly named entities, we use our approach to disambiguate both entities and common nouns. In our experiments, we first validate our relatedness measure on multiple knowledge bases and ground truth datasets and show that it performs better than related state-of-the-art graph based measures. Afterwards, we evaluate the disambiguation algorithm and show that it also achieves superior disambiguation accuracy with respect to alternative state-of-the-art graph-based algorithms.</p></td></tr><tr><td>249</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_22"> Piercing to the Heart of Instance Matching Tools</a></td></tr><tr><td colspan=3><p>One of the main challenges in the Data Web is the identification of instances that refer to the same real-world entity. Choosing the right framework for this purpose remains tedious, as current instance matching benchmarks fail to provide end users and developers with the necessary insights pertaining to how current frameworks behave when dealing with real data. In this paper, we present </p></td></tr><tr><td>250</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_23">Decision-Making Bias in Instance Matching Model Selection</a></td></tr><tr><td colspan=3><p>Instance matching has emerged as an important problem in the Semantic Web, with machine learning methods proving especially effective. To enhance performance, task-specific knowledge is typically used to introduce bias in the model selection problem. Such biases tend to be exploited by practitioners in a piecemeal fashion. This paper introduces a framework where the model selection design process is represented as a factor graph. Nodes in this bipartite graphical model represent opportunities for explicitly introducing bias. The graph is first used to unify and visualize common biases in the design of existing instance matchers. As a direct application, we then use the graph to hypothesize about potential unexploited biases. The hypotheses are evaluated by training 1032 neural networks on three instance matching tasks on Microsoft Azure’s cloud-based platform. An analysis over 25 GB of experimental data indicates that the proposed biases can improve efficiency by over 65% over a baseline configuration, with effectiveness improving by a smaller margin. The findings lead to a promising set of four recommendations that can be integrated into existing supervised instance matchers.</p></td></tr><tr><td>251</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_25">TabEL: Entity Linking in Web Tables</a></td></tr><tr><td colspan=3><p>Web tables form a valuable source of relational data. The Web contains an estimated 154 million HTML tables of relational data, with Wikipedia alone containing 1.6 million high-quality tables. Extracting the semantics of Web tables to produce machine-understandable knowledge has become an active area of research.</p></td></tr><tr><td>252</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_24">Klink-2: Integrating Multiple Web Sources to Generate Semantic Topic Networks</a></td></tr><tr><td colspan=3><p>The amount of scholarly data available on the web is steadily increasing, enabling different types of analytics which can provide important insights into the research activity. In order to make sense of and explore this large-scale body of knowledge we need an accurate, comprehensive and up-to-date ontology of research topics. Unfortunately, human crafted classifications do not satisfy these criteria, as they evolve too slowly and tend to be too coarse-grained. Current automated methods for generating ontologies of research areas also present a number of limitations, such as: i) they do not consider the rich amount of indirect statistical and semantic relationships, which can help to understand the relation between two topics – e.g., the fact that two research areas are associated with a similar set of venues or technologies; ii) they do not distinguish between different kinds of hierarchical relationships; and iii) they are not able to handle effectively ambiguous topics characterized by a noisy set of relationships. In this paper we present Klink-2, a novel approach which improves on our earlier work on automatic generation of semantic topic networks and addresses the aforementioned limitations by taking advantage of a variety of knowledge sources available on the web. In particular, Klink-2 analyses networks of research entities (including papers, authors, venues, and technologies) to infer three kinds of semantic relationships between topics. It also identifies ambiguous keywords (e.g., “ontology”) and separates them into the appropriate distinct topics – e.g., “ontology/philosophy” vs. “ontology/semantic web”. Our experimental evaluation shows that the ability of Klink-2 to integrate a high number of data sources and to generate topics with accurate contextual meaning yields significant improvements over other algorithms in terms of both precision and recall.</p></td></tr><tr><td>253</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_21">Optimizing the Computation of Overriding</a></td></tr><tr><td colspan=3><p>We introduce optimization techniques for reasoning in </p></td></tr><tr><td>254</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_19">R</a></td></tr><tr><td colspan=3><p>It has been shown, both theoretically and empirically, that performing core reasoning tasks on large and expressive ontologies in OWL 1 and OWL 2 is time-consuming and resource-intensive. Moreover, due to the different reasoning algorithms and optimisation techniques employed, each reasoner may be efficient for ontologies with different characteristics. In this paper, we present R</p></td></tr><tr><td>255</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_18">Adding </a></td></tr><tr><td colspan=3><p>Levesque’s proper knowledge bases (proper KBs) correspond to infinite sets of ground positive and negative facts, with the notable property that for FOL formulas in a certain normal form, which includes conjunctive queries and positive queries possibly extended with a controlled form of negation, entailment reduces to formula evaluation. However proper KBs represent extensional knowledge only. In description logic terms, they correspond to ABoxes. In this paper, we augment them with </p></td></tr><tr><td>256</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_20">Rewriting-Based Instance Retrieval for Negated Concepts in Description Logic Ontologies</a></td></tr><tr><td colspan=3><p>Instance retrieval computes all instances of a given concept in a consistent description logic (DL) ontology. Although it is a popular task for ontology reasoning, there is no scalable method for instance retrieval for negated concepts by now. This paper studies a new approach to instance retrieval for negated concepts based on query rewriting. A class of DL ontologies called the </p></td></tr><tr><td>257</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_17">Effective Online Knowledge Graph Fusion</a></td></tr><tr><td colspan=3><p>Recently, Web search engines have empowered their search with knowledge graphs to satisfy increasing demands of complex information needs about entities. Each engine offers an online knowledge graph service to display highly relevant information about the query entity in form of a structured summary called </p></td></tr><tr><td>258</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_16">CogMap: A Cognitive Support Approach to Property and Instance Alignment</a></td></tr><tr><td colspan=3><p>The iterative user interaction approach for data integration proposed by Falconer and Noy can be generalized to consider interactions between integration tools (generators) that generate potential schema mappings and users or analysis tools (analyzers) that select the best mapping. Each such selection then provides high-confidence guidance for the next iteration of the integration tool. We have implemented this generalized approach in </p></td></tr><tr><td>259</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_15">An Algebra of Qualitative Taxonomical Relations for Ontology Alignments</a></td></tr><tr><td colspan=3><p>Algebras of relations were shown useful in managing ontology alignments. They make it possible to aggregate alignments disjunctively or conjunctively and to propagate alignments within a network of ontologies. The previously considered algebra of relations contains taxonomical relations between classes. However, compositional inference using this algebra is sound only if we assume that classes which occur in alignments have nonempty extensions. Moreover, this algebra covers relations only between classes. Here we introduce a new algebra of relations, which, first, solves the limitation of the previous one, and second, incorporates all qualitative taxonomical relations that occur between individuals and concepts, including the relations “is a” and “is not”. We prove that this algebra is coherent with respect to the simple semantics of alignments.</p></td></tr><tr><td>260</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_10">Facilitating Entity Navigation Through Top-K Link Patterns</a></td></tr><tr><td colspan=3><p>Entity navigation over Linked Data often follows semantic links by using Linked Data browsers. With the increasing volume of Linked Data, the rich and diverse links make it difficult for users to traverse the link graph and find target entities. Besides, there is a necessity for navigation paradigm to take into account not only single-entity-oriented transition, but also entity-set-oriented transition. To facilitate entity navigation, we propose a novel concept called link pattern, and introduce link pattern lattice to organize semantic links when browsing an entity or a set of entities. Furthermore, to help users quickly find target entities, top-K link patterns are selected for entity navigation. The proposed approach is implemented in a prototype system and then compared with two Linked Data browsers via a user study. Experimental results show that our approach is effective.</p></td></tr><tr><td>261</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_13">Mapping Analysis in Ontology-Based Data Access: Algorithms and Complexity</a></td></tr><tr><td colspan=3><p>Ontology-based data access (OBDA) is a recent paradigm for accessing data sources through an ontology that acts as a conceptual, integrated view of the data, and declarative mappings that connect the ontology to the data sources. We study the formal analysis of mappings in OBDA. Specifically, we focus on the problem of identifying mapping inconsistency and redundancy, two of the most important anomalies for mappings in OBDA. We consider a wide range of ontology languages that comprises OWL 2 and all its profiles, and examine mapping languages of different expressiveness over relational databases. We provide algorithms and establish tight complexity bounds for the decision problems associated with mapping inconsistency and redundancy. Our results prove that, in our general framework, such forms of mapping analysis enjoy nice computational properties, in the sense that they are not harder than standard reasoning tasks over the ontology or over the relational database schema.</p></td></tr><tr><td>262</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_14">Towards Defeasible Mappings for Tractable Description Logics</a></td></tr><tr><td colspan=3><p>We present a novel approach to denote mappings between </p></td></tr><tr><td>263</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_11">Serving DBpedia with DOLCE – More than Just Adding a Cherry on Top</a></td></tr><tr><td colspan=3><p>Large knowledge bases, such as DBpedia, are most often created heuristically due to scalability issues. In the building process, both random as well as systematic errors may occur. In this paper, we focus on finding </p></td></tr><tr><td>264</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_12">Ontology-Based Integration of Cross-Linked Datasets</a></td></tr><tr><td colspan=3><p>In this paper we tackle the problem of answering SPARQL queries over </p></td></tr><tr><td>265</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_9">LinkDaViz – Automatic Binding of Linked Data to Visualizations</a></td></tr><tr><td colspan=3><p>As the Web of Data is growing steadily, the demand for user-friendly means for exploring, analyzing and visualizing Linked Data is also increasing. The key challenge for visualizing Linked Data consists in providing a clear overview of the data and supporting non-technical users in finding suitable visualizations while hiding technical details of Linked Data and visualization configuration. In order to accomplish this, we propose a largely automatic workflow which guides users through the process of creating visualizations by automatically categorizing and binding data to visualization parameters. The approach is based on a heuristic analysis of the structure of the input data and a comprehensive visualization model facilitating the automatic binding between data and visualization parameters. The resulting assignments are ranked and presented to the user. With LinkDaViz we provide a web-based implementation of the approach and demonstrate the feasibility by an extended user and performance evaluation.</p></td></tr><tr><td>266</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_8">Substring Filtering for Low-Cost Linked Data Interfaces</a></td></tr><tr><td colspan=3><p>Recently, Triple Pattern Fragments (</p></td></tr><tr><td>267</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_7">Networks of Linked Data Eddies: An Adaptive Web Query Processing Engine for RDF Data</a></td></tr><tr><td colspan=3><p>Client-side query processing techniques that rely on the materialization of fragments of the original RDF dataset provide a promising solution for Web query processing. However, because of unexpected data transfers, the traditional optimize-then-execute paradigm, used by existing approaches, is not always applicable in this context, i.e., performance of client-side execution plans can be negatively affected by live conditions where rate at which data arrive from sources changes. We tackle adaptivity for client-side query processing, and present a network of Linked Data Eddies that is able to adjust query execution schedulers to data availability and runtime conditions. Experimental studies suggest that the network of Linked Data Eddies outperforms static Web query schedulers in scenarios with unpredictable transfer delays and data distributions.</p></td></tr><tr><td>268</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_1">SPARQL with Property Paths</a></td></tr><tr><td colspan=3><p>The original SPARQL proposal was often criticized for its inability to navigate through the structure of RDF documents. For this reason property paths were introduced in SPARQL 1.1, but up to date there are no theoretical studies examining how their addition to the language affects main computational tasks such as query evaluation, query containment, and query subsumption. In this paper we tackle all of these problems and show that although the addition of property paths has no impact on query evaluation, they do make the containment and subsumption problems substantially more difficult.</p></td></tr><tr><td>269</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_6">Opportunistic Linked Data Querying Through Approximate Membership Metadata</a></td></tr><tr><td colspan=3><p>Between </p></td></tr><tr><td>270</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_2">Recursion in SPARQL</a></td></tr><tr><td colspan=3><p>In this paper we propose a general purpose recursion operator to be added to SPARQL, formalize its syntax and develop algorithms for evaluating it in practical scenarios. We also show how to implement recursion as a plug-in on top of existing systems and test its performance on several real world datasets.</p></td></tr><tr><td>271</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_4">FEASIBLE: A Feature-Based SPARQL Benchmark Generation Framework</a></td></tr><tr><td colspan=3><p>Benchmarking is indispensable when aiming to assess technologies with respect to their suitability for given tasks. While several benchmarks and benchmark generation frameworks have been developed to evaluate triple stores, they mostly provide a one-fits-all solution to the benchmarking problem. This approach to benchmarking is however unsuitable to evaluate the performance of a triple store for a given application with particular requirements. We address this drawback by presenting FEASIBLE, an automatic approach for the generation of benchmarks out of the query history of applications, i.e., query logs. The generation is achieved by selecting prototypical queries of a user-defined size from the input set of queries. We evaluate our approach on two query logs and show that the benchmarks it generates are accurate approximations of the input query logs. Moreover, we compare four different triple stores with benchmarks generated using our approach and show that they behave differently based on the data they contain and the types of queries posed. Our results suggest that FEASIBLE generates better sample queries than the state of the art. In addition, the better query selection and the larger set of query types used lead to triple store rankings which partly differ from the rankings generated by previous works.</p></td></tr><tr><td>272</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_3">Federated SPARQL Queries Processing with Replicated Fragments</a></td></tr><tr><td colspan=3><p>Federated query engines provide a unified query interface to federations of SPARQL endpoints. Replicating data fragments from different Linked Data sources facilitates data re-organization to better fit federated query processing needs of data consumers. However, existing federated query engines are not designed to support replication and replicated data can negatively impact their performance. In this paper, we formulate the source selection problem with fragment replication (SSP-FR). For a given set of endpoints with replicated fragments and a SPARQL query, the problem is to select the endpoints that minimize the number of tuples to be transferred. We devise the </p></td></tr><tr><td>273</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_36">kyrie2: Query Rewriting under Extensional Constraints in </a></td></tr><tr><td colspan=3><p>In this paper we study query answering and rewriting in ontology-based data access. Specifically, we present an algorithm for computing a perfect rewriting of unions of conjunctive queries posed over ontologies expressed in the description logic </p></td></tr><tr><td>274</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_35">Answering SPARQL Queries over Databases under OWL 2 QL Entailment Regime</a></td></tr><tr><td colspan=3><p>We present an extension of the ontology-based data access platform </p></td></tr><tr><td>275</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_33">Ensemble Learning for Named Entity Recognition</a></td></tr><tr><td colspan=3><p>A considerable portion of the information on the Web is still only available in unstructured form. Implementing the vision of the Semantic Web thus requires transforming this unstructured data into structured data. One key step during this process is the recognition of named entities. Previous works suggest that ensemble learning can be used to improve the performance of named entity recognition tools. However, no comparison of the performance of existing supervised machine learning approaches on this task has been presented so far. We address this research gap by presenting a thorough evaluation of named entity recognition based on ensemble learning. To this end, we combine four different state-of-the approaches by using 15 different algorithms for ensemble learning and evaluate their performace on five different datasets. Our results suggest that ensemble learning can reduce the error rate of state-of-the-art named entity recognition systems by 40%, thereby leading to over 95% f-score in our best run.</p></td></tr><tr><td>276</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_32">Semano: Semantic Annotation Framework for Natural Language Resources</a></td></tr><tr><td colspan=3><p>In this paper, we present Semano — a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies. Semano generalizes the mechanism of JAPE transducers that has been introduced within the General Architecture for Text Engineering (GATE) to enable modular development of annotation rule bases. The core of the Semano rule base model are rule templates called </p></td></tr><tr><td>277</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_30">M-ATOLL: A Framework for the Lexicalization of Ontologies in Multiple Languages</a></td></tr><tr><td colspan=3><p>Many tasks in which a system needs to mediate between natural language expressions and elements of a vocabulary in an ontology or dataset require knowledge about how the elements of the vocabulary (i.e. classes, properties, and individuals) are expressed in natural language. In a multilingual setting, such knowledge is needed for each of the supported languages. In this paper we present M-ATOLL, a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus. The framework exploits a set of language-specific dependency patterns which are formalized as SPARQL queries and run over a parsed corpus. We have instantiated the system for two languages: German and English. We evaluate it in terms of precision, recall and F-measure for English and German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia. In particular, we investigate the contribution of each single dependency pattern and perform an analysis of the impact of different parameters.</p></td></tr><tr><td>278</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_34">OBDA: Query Rewriting or Materialization? In Practice, Both!</a></td></tr><tr><td colspan=3><p>Given a source relational database, a target OWL ontology and a mapping from the source database to the target ontology, Ontology-Based Data Access (OBDA) concerns answering queries over the target ontology using these three components. This paper presents the development of Ultrawrap</p></td></tr><tr><td>279</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_28">Updating RDFS ABoxes and TBoxes in SPARQL</a></td></tr><tr><td colspan=3><p>Updates in RDF stores have recently been standardised in the SPARQL 1.1 Update specification. However, computing entailed answers by ontologies is usually treated orthogonally to updates in triple stores. Even the W3C SPARQL 1.1 Update and SPARQL 1.1 Entailment Regimes specifications explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates. In this paper, we take a first step to close this gap. We define a fragment of SPARQL basic graph patterns corresponding to (the RDFS fragment of) </p></td></tr><tr><td>280</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_31">Towards Efficient and Effective Semantic Table Interpretation</a></td></tr><tr><td colspan=3><p>This paper describes TableMiner, the first semantic Table Interpretation method that adopts an incremental, mutually recursive and bootstrapping learning approach seeded by automatically selected ‘partial’ data from a table. TableMiner labels columns containing named entity mentions with semantic concepts that best describe data in columns, and disambiguates entity content cells in these columns. TableMiner is able to use various types of contextual information outside tables for Table Interpretation, including semantic markups (e.g., RDFa/microdata annotations) that to the best of our knowledge, have never been used in Natural Language Processing tasks. Evaluation on two datasets shows that compared to two baselines, TableMiner consistently obtains the best performance. In the classification task, it achieves significant improvements of between 0.08 and 0.38 F1 depending on different baseline methods; in the disambiguation task, it outperforms both baselines by between 0.19 and 0.37 in Precision on one dataset, and between 0.02 and 0.03 F1 on the other dataset. Observation also shows that the bootstrapping learning approach adopted by TableMiner can potentially deliver computational savings of between 24 and 60% against classic methods that ‘exhaustively’ processes the entire table content to build features for interpretation.</p></td></tr><tr><td>281</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_29">AGDISTIS - Graph-Based Disambiguation of Named Entities Using Linked Data</a></td></tr><tr><td colspan=3><p>Over the last decades, several billion Web pages have been made available on the Web. The ongoing transition from the current Web of unstructured data to the Web of Data yet requires scalable and accurate approaches for the extraction of structured data in RDF (Resource Description Framework) from these websites. One of the key steps towards extracting RDF from text is the disambiguation of named entities. While several approaches aim to tackle this problem, they still achieve poor accuracy. We address this drawback by presenting AGDISTIS, a novel knowledge-base-agnostic approach for named entity disambiguation. Our approach combines the Hypertext-Induced Topic Search (HITS) algorithm with label expansion strategies and string similarity measures. Based on this combination, AGDISTIS can efficiently detect the correct URIs for a given set of named entities within an input text. We evaluate our approach on eight different datasets against state-of-the-art named entity disambiguation frameworks. Our results indicate that we outperform the state-of-the-art approach by up to 29% F-measure.</p></td></tr><tr><td>282</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_26">A Power Consumption Benchmark for Reasoners on Mobile Devices</a></td></tr><tr><td colspan=3><p>We introduce a new methodology for benchmarking the performance per watt of semantic web reasoners and rule engines on smartphones to provide developers with information critical for deploying semantic web tools on power-constrained devices. We validate our methodology by applying it to three well-known reasoners and rule engines answering queries on two ontologies with expressivities in RDFS and OWL DL. While this validation was conducted on smartphones running Google’s Android operating system, our methodology is general and may be applied to different hardware platforms, reasoners, ontologies, and entire applications to determine performance relevant to power consumption. We discuss the implications of our findings for balancing tradeoffs of local computation versus communication costs for semantic technologies on mobile platforms, sensor networks, the Internet of Things, and other power-constrained environments.</p></td></tr><tr><td>283</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_24">Noisy Type Assertion Detection in Semantic Datasets</a></td></tr><tr><td colspan=3><p>Semantic datasets provide support to automate many tasks such as decision-making and question answering. However, their performance is always decreased by the noises in the datasets, among which, noisy type assertions play an important role. This problem has been mainly studied in the domain of data mining but not in the semantic web community. In this paper, we study the problem of noisy type assertion detection in semantic web datasets by making use of concept disjointness relationships hidden in the datasets. We transform noisy type assertion detection into multiclass classification of pairs of type assertions which type an individual to two potential disjoint concepts. The multiclass classification is solved by Adaboost with C4.5 as the base classifier. Furthermore, we propose instance-concept compatability metrics based on instance-instance relationships and instance-concept assertions. We evaluate the approach on both synthetic datasets and DBpedia. Our approach effectively detect noisy type assertions in DBpedia with a high precision of 95%.</p></td></tr><tr><td>284</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_25">A Cross-Platform Benchmark Framework for Mobile Semantic Web Reasoning Engines</a></td></tr><tr><td colspan=3><p>Semantic Web technologies are used in a variety of domains for their ability to facilitate data integration, as well as enabling expressive, standards-based reasoning. Deploying Semantic Web reasoning processes directly on mobile devices has a number of advantages, including robustness to connectivity loss, more timely results, and reduced infrastructure requirements. At the same time, a number of challenges arise as well, related to mobile platform heterogeneity and limited computing resources. To tackle these challenges, it should be possible to benchmark mobile reasoning performance across different mobile platforms, with rule- and datasets of varying scale and complexity and existing reasoning process flows. To deal with the current heterogeneity of rule formats, a uniform rule- and data-interface on top of mobile reasoning engines should be provided as well. In this paper, we present a cross-platform benchmark framework that supplies 1) a generic, standards-based Semantic Web layer on top of existing mobile reasoning engines; and 2) a benchmark engine to investigate and compare mobile reasoning performance.</p></td></tr><tr><td>285</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_21">Col-Graph: Towards Writable and Scalable Linked Open Data</a></td></tr><tr><td colspan=3><p>Linked Open Data faces severe issues of scalability, availability and data quality. These issues are observed by data consumers performing federated queries; SPARQL endpoints do not respond and results can be wrong or out-of-date. If a data consumer finds an error, how can she fix it? This raises the issue of the </p></td></tr><tr><td>286</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_22">Transferring Semantic Categories with Vertex Kernels: Recommendations with SemanticSVD++</a></td></tr><tr><td colspan=3><p>Matrix Factorisation is a recommendation approach that tries to understand what factors interest a user, based on his past ratings for items (products, movies, songs), and then use this factor information to predict future item ratings. A central limitation of this approach however is that it cannot capture how a user’s tastes have evolved beforehand; thereby ignoring if a user’s preference for a factor is likely to change. One solution to this is to include users’ preferences for semantic (i.e. linked data) categories, however this approach is limited should a user be presented with an item for which he has not rated the semantic categories previously; so called </p></td></tr><tr><td>287</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_23">Detecting Errors in Numerical Linked Data Using Cross-Checked Outlier Detection</a></td></tr><tr><td colspan=3><p>Outlier detection used for identifying wrong values in data is typically applied to single datasets to search them for values of unexpected behavior. In this work, we instead propose an approach which combines the outcomes of two independent outlier detection runs to get a more reliable result and to also prevent problems arising from natural outliers which are exceptional values in the dataset but nevertheless correct. Linked Data is especially suited for the application of such an idea, since it provides large amounts of data enriched with hierarchical information and also contains explicit links between instances. In a first step, we apply outlier detection methods to the property values extracted from a single repository, using a novel approach for splitting the data into relevant subsets. For the second step, we exploit </p></td></tr><tr><td>288</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_20">Discovery and Visual Analysis of Linked Data for Humans</a></td></tr><tr><td colspan=3><p>Linked Data has grown to become one of the largest available knowledge bases. Unfortunately, this wealth of data remains inaccessible to those without in-depth knowledge of semantic technologies. We describe a toolchain enabling users without semantic technology background to explore and visually analyse Linked Data. We demonstrate its applicability in scenarios involving data from the Linked Open Data Cloud, and research data extracted from scientific publications. Our focus is on the Web-based front-end consisting of querying and visualisation tools. The performed usability evaluations unveil mainly positive results confirming that the Query Wizard simplifies searching, refining and transforming Linked Data and, in particular, that people using the Visualisation Wizard quickly learn to perform interactive analysis tasks on the resulting Linked Data sets. In making Linked Data analysis effectively accessible to the general public, our tool has been integrated in a number of live services where people use it to analyse, discover and discuss facts with Linked Data.</p></td></tr><tr><td>289</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_19">On Publishing Chinese Linked Open Schema</a></td></tr><tr><td colspan=3><p>Linking Open Data (LOD) is the largest community effort for semantic data publishing which converts the Web from a Web of document to a Web of interlinked knowledge. While the state of the art LOD contains billion of triples describing millions of entities, it has only a limited number of schema information and is lack of schema-level axioms. To close the gap between the lightweight LOD and the expressive ontologies, we contribute to the complementary part of the LOD, that is, Linking Open Schema (LOS). In this paper, we introduce Zhishi.schema, the first effort to publish Chinese linked open schema. We collect navigational categories as well as dynamic tags from more than 50 various most popular social Web sites in China. We then propose a two-stage method to capture equivalence, subsumption and relate relationships between the collected categories and tags, which results in an integrated concept taxonomy and a large semantic network. Experimental results show the high quality of Zhishi.schema. Compared with category systems of DBpedia, Yago, BabelNet, and Freebase, Zhishi.schema has wide coverage of categories and contains the largest number of subsumptions between categories. When substituting Zhishi.schema for the original category system of Zhishi.me, we not only filter out incorrect category subsumptions but also add more finer-grained categories.</p></td></tr><tr><td>290</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_17">Analyzing Schema.org</a></td></tr><tr><td colspan=3><p>Schema.org is a way to add machine-understandable information to web pages that is processed by the major search engines to improve search performance. The definition of schema.org is provided as a set of web pages plus a partial mapping into RDF triples with unusual properties, and is incomplete in a number of places. This analysis of and formal semantics for schema.org provides a complete basis for a plausible version of what schema.org should be.</p></td></tr><tr><td>291</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_16">Adoption of the Linked Data Best Practices in Different Topical Domains</a></td></tr><tr><td colspan=3><p>The central idea of Linked Data is that data publishers support applications in discovering and integrating data by complying to a set of best practices in the areas of linking, vocabulary usage, and metadata provision. In 2011, the </p></td></tr><tr><td>292</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_27">Dynamic Provenance for SPARQL Updates</a></td></tr><tr><td colspan=3><p>While the Semantic Web currently can exhibit provenance information by using the W3C PROV standards, there is a “missing link” in connecting PROV to storing and querying for dynamic changes to RDF graphs using SPARQL. Solving this problem would be required for such clear use-cases as the creation of version control systems for RDF. While some provenance models and annotation techniques for storing and querying provenance data originally developed with databases or workflows in mind transfer readily to RDF and SPARQL, these techniques do not readily adapt to describing changes in dynamic RDF datasets over time. In this paper we explore how to adapt the dynamic copy-paste provenance model of Buneman et al.[2] to RDF datasets that change over time in response to SPARQL updates, how to represent the resulting provenance records themselves as RDF in a manner compatible with W3C PROV, and how the provenance information can be defined by reinterpreting SPARQL updates. The primary contribution of this paper is a semantic framework that enables the semantics of SPARQL Update to be used as the basis for a ‘cut-and-paste’ provenance model in a principled manner.</p></td></tr><tr><td>293</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_18">The WebDataCommons Microdata, RDFa and Microformat Dataset Series</a></td></tr><tr><td colspan=3><p>In order to support web applications to understand the content of HTML pages an increasing number of websites have started to annotate structured data within their pages using markup formats such as Microdata, RDFa, Microformats. The annotations are used by Google, Yahoo!, Yandex, Bing and Facebook to enrich search results and to display entity descriptions within their applications. In this paper, we present a series of publicly accessible Microdata, RDFa, Microformats datasets that we have extracted from three large web corpora dating from 2010, 2012 and 2013. Altogether, the datasets consist of almost 30 billion RDF quads. The most recent of the datasets contains amongst other data over 211 million product descriptions, 54 million reviews and 125 million postal addresses originating from thousands of websites. The availability of the datasets lays the foundation for further research on integrating and cleansing the data as well as for exploring its utility within different application contexts. As the dataset series covers four years, it can also be used to analyze the evolution of the adoption of the markup formats.</p></td></tr><tr><td>294</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_15">Dutch Ships and Sailors Linked Data</a></td></tr><tr><td colspan=3><p>We present the Dutch Ships and Sailors Linked Data Cloud. This heterogeneous dataset brings together four curated datasets on Dutch Maritime history as five-star linked data. The individual datasets use separate datamodels, designed in close collaboration with maritime historical researchers. The individual models are mapped to a common interoperability layer, allowing for analysis of the data on the general level. We present the datasets, modeling decisions, internal links and links to external data sources. We show ways of accessing the data and present a number of examples of how the dataset can be used for historical research. The Dutch Ships and Sailors Linked Data Cloud is a potential hub dataset for digital history research and a prime example of the benefits of Linked Data for this field.</p></td></tr><tr><td>295</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_14">LOD Laundromat: A Uniform Way of Publishing Other People’s Dirty Data</a></td></tr><tr><td colspan=3><p>It is widely accepted that </p></td></tr><tr><td>296</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_13">Diversified Stress Testing of RDF Data Management Systems</a></td></tr><tr><td colspan=3><p>The Resource Description Framework (RDF) is a standard for conceptually describing data on the Web, and SPARQL is the query language for RDF. As RDF data continue to be published across heterogeneous domains and integrated at Web-scale such as in the Linked Open Data (LOD) cloud, RDF data management systems are being exposed to queries that are far more diverse and workloads that are far more varied. The first contribution of our work is an in-depth experimental analysis that shows existing SPARQL benchmarks are not suitable for testing systems for diverse queries and varied workloads. To address these shortcomings, our second contribution is the Waterloo SPARQL Diversity Test Suite (WatDiv) that provides stress testing tools for RDF data management systems. Using WatDiv, we have been able to reveal issues with existing systems that went unnoticed in evaluations using earlier benchmarks. Specifically, our experiments with five popular RDF data management systems show that they cannot deliver good performance uniformly across workloads. For some queries, there can be as much as five orders of magnitude difference between the query execution time of the fastest and the slowest system while the fastest system on one query may unexpectedly time out on another query. By performing a detailed analysis, we pinpoint these problems to specific types of queries and workloads.</p></td></tr><tr><td>297</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_12">Querying Datasets on the Web with High Availability</a></td></tr><tr><td colspan=3><p>As the Web of Data is growing at an ever increasing speed, the lack of reliable query solutions for live public data becomes apparent. </p></td></tr><tr><td>298</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_10">SYRql: A Dataflow Language for Large Scale Processing of RDF Data</a></td></tr><tr><td colspan=3><p>The recent big data movement resulted in a surge of activity on layering declarative languages on top of distributed computation platforms. In the Semantic Web realm, this surge of analytics languages was not reflected despite the significant growth in the available RDF data. Consequently, when analysing large RDF datasets, users are left with two main options: using SPARQL or using an existing non-RDF-specific big data language, both with its own limitations. The pure declarative nature of SPARQL and the high cost of evaluation can be limiting in some scenarios. On the other hand, existing big data languages are designed mainly for tabular data and, therefore, applying them to RDF data results in verbose, unreadable, and sometimes inefficient scripts. In this paper, we introduce </p></td></tr><tr><td>299</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_11">Sempala: Interactive SPARQL Query Processing on Hadoop</a></td></tr><tr><td colspan=3><p>Driven by initiatives like Schema.org, the amount of semantically annotated data is expected to grow steadily towards massive scale, requiring cluster-based solutions to query it. At the same time, Hadoop has become dominant in the area of Big Data processing with large infrastructures being already deployed and used in manifold application fields. For Hadoop-based applications, a common data pool (HDFS) provides many synergy benefits, making it very attractive to use these infrastructures for semantic data processing as well. Indeed, existing SPARQL-on- Hadoop (MapReduce) approaches have already demonstrated very good scalability, however, query runtimes are rather slow due to the underlying batch processing framework. While this is acceptable for data-intensive queries, it is not satisfactory for the majority of SPARQL queries that are typically much more selective requiring only small subsets of the data. In this paper, we present </p></td></tr><tr><td>300</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_8">Linked Biomedical Dataspace: Lessons Learned Integrating Data for Drug Discovery</a></td></tr><tr><td colspan=3><p>The increase in the volume and heterogeneity of biomedical data sources has motivated researchers to embrace Linked Data (LD) technologies to solve the ensuing integration challenges and enhance information discovery. As an integral part of the EU GRANATUM project, a Linked Biomedical Dataspace (LBDS) was developed to semantically interlink data from multiple sources and augment the design of </p></td></tr><tr><td>301</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_7">Scientific Lenses to Support Multiple Views over Linked Chemistry Data</a></td></tr><tr><td colspan=3><p>When are two entries about a small molecule in different datasets the same? If they have the same drug name, chemical structure, or some other criteria? The choice depends upon the application to which the data will be put. However, existing Linked Data approaches provide a single global view over the data with no way of varying the notion of equivalence to be applied.</p></td></tr><tr><td>302</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_6">EPCIS Event-Based Traceability in Pharmaceutical Supply Chains via Automated Generation of Linked Pedigrees</a></td></tr><tr><td colspan=3><p>In this paper we show how event processing over semantically annotated streams of events can be exploited, for implementing tracing and tracking of products in supply chains through the automated generation of linked pedigrees. In our abstraction, events are encoded as spatially and temporally oriented named graphs, while linked pedigrees as RDF datasets are their specific compositions. We propose an algorithm that operates over streams of RDF annotated EPCIS events to generate linked pedigrees. We exemplify our approach using the pharmaceuticals supply chain and show how counterfeit detection is an implicit part of our pedigree generation. Our evaluation results show that for fast moving supply chains, smaller window sizes on event streams provide significantly higher efficiency in the generation of pedigrees as well as enable early counterfeit detection.</p></td></tr><tr><td>303</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_9">Drug-Target Interaction Prediction Using Semantic Similarity and Edge Partitioning</a></td></tr><tr><td colspan=3><p>The ability to integrate a wealth of human-curated knowledge from scientific datasets and ontologies can benefit drug-target interaction prediction. The hypothesis is that similar drugs interact with the same targets, and similar targets interact with the same drugs. The similarities between drugs reflect a chemical semantic space, while similarities between targets reflect a genomic semantic space. In this paper, we present a novel method that combines a data mining framework for link prediction, semantic knowledge (similarities) from ontologies or semantic spaces, and an algorithmic approach to partition the edges of a heterogeneous graph that includes drug-target interaction edges, and drug-drug and target-target similarity edges. Our semantics based edge partitioning approach, semEP, has the advantages of edge based community detection which allows a node to participate in more than one cluster or community. The semEP problem is to create a minimal partitioning of the edges such that the cluster density of each subset of edges is maximal. We use semantic knowledge (similarities) to specify edge constraints, i.e., specific drug-target interaction edges that should not participate in the same cluster. Using a well-known dataset of drug-target interactions, we demonstrate the benefits of using semEP predictions to improve the performance of a range of state-of-the-art machine learning based prediction methods. Validation of the novel best predicted interactions of semEP against the STITCH interaction resource reflect both accurate and diverse predictions.</p></td></tr><tr><td>304</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_4">Introducing Wikidata to the Linked Data Web</a></td></tr><tr><td colspan=3><p>Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.</p></td></tr><tr><td>305</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_5">Web-Scale Extension of RDF Knowledge Bases from Templated Websites</a></td></tr><tr><td colspan=3><p>Only a small fraction of the information on the Web is represented as Linked Data. This lack of coverage is partly due to the paradigms followed so far to extract Linked Data. While converting structured data to RDF is well supported by tools, most approaches to extract RDF from semi-structured data rely on extraction methods based on ad-hoc solutions. In this paper, we present a holistic and open-source framework for the extraction of RDF from templated websites. We discuss the architecture of the framework and the initial implementation of each of its components. In particular, we present a novel wrapper induction technique that does not require any human supervision to detect wrappers for web sites. Our framework also includes a consistency layer with which the data extracted by the wrappers can be checked for logical consistency. We evaluate the initial version of REX on three different datasets. Our results clearly show the potential of using templated Web pages to extend the Linked Data Cloud. Moreover, our results indicate the weaknesses of our current implementations and how they can be extended.</p></td></tr><tr><td>306</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_2">HELIOS – Execution Optimization for Link Discovery</a></td></tr><tr><td colspan=3><p>Links between knowledge bases build the backbone of the Linked Data Web. In previous works, the combination of the results of time-efficient algorithms through set-theoretical operators has been shown to be very time-efficient for Link Discovery. However, the further optimization of such link specifications has not been paid much attention to. We address the issue of further optimizing the runtime of link specifications by presenting </p></td></tr><tr><td>307</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_48">Semantic Cooperation and Knowledge Reuse by Using Autonomous Ontologies</a></td></tr><tr><td colspan=3><p>Several proposals have been put forward to support distributed agent cooperation in the Semantic Web, by allowing concepts and roles in one ontology be reused in another ontology. In general, these proposals reduce the autonomy of each ontology by defining the semantics of the ontology to depend on the semantics of the other ontologies.</p></td></tr><tr><td>308</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_46">On the Foundations of Computing Deltas Between RDF Models</a></td></tr><tr><td colspan=3><p>The ability to compute the differences that exist between two RDF models is an important step to cope with the evolving nature of the Semantic Web (SW). In particular, RDF Deltas can be employed to reduce the amount of data that need to be exchanged and managed over the network and hence build advanced SW synchronization and versioning services. By considering Deltas as sets of change operations, in this paper we study various RDF comparison functions in conjunction with the semantics of the underlying change operations and formally analyze their possible combinations in terms of correctness, minimality, semantic identity and redundancy properties.</p></td></tr><tr><td>309</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_47">Semplore: An IR Approach to Scalable Hybrid Query of Semantic Web Data</a></td></tr><tr><td colspan=3><p>As an extension to the current Web, Semantic Web will not only contain structured data with machine understandable semantics but also textual information. While structured queries can be used to find information more precisely on the Semantic Web, keyword searches are still needed to help exploit textual information. It thus becomes very important that we can combine precise structured queries with imprecise keyword searches to have a hybrid query capability. In addition, due to the huge volume of information on the Semantic Web, the hybrid query must be processed in a very scalable way. In this paper, we define such a hybrid query capability that combines unary tree-shaped structured queries with keyword searches. We show how existing information retrieval (IR) index structures and functions can be reused to index semantic web data and its textual information, and how the hybrid query is evaluated on the index structure using IR engines in an efficient and scalable manner. We implemented this IR approach in an engine called Semplore. Comprehensive experiments on its performance show that it is a promising approach. It leads us to believe that it may be possible to evolve current web search engines to query and search the Semantic Web. Finally, we breifly describe how Semplore is used for searching Wikipedia and an IBM customer’s product information.</p></td></tr><tr><td>310</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_1">CAMO: Integration of Linked Open Data for Multimedia Metadata Enrichment</a></td></tr><tr><td colspan=3><p>Metadata is a vital factor for effective management, organization and retrieval of multimedia content. In this paper, we introduce CAMO, a new system developed jointly with Samsung to enrich multimedia metadata by integrating Linked Open Data (LOD). Large-scale, heterogeneous LOD sources, e.g., DBpedia, LinkMDB and MusicBrainz, are integrated using ontology matching and instance linkage techniques. A mobile app for Android devices is built on top of the LOD to improve multimedia content browsing. An empirical evaluation is conducted to demonstrate the effectiveness and accuracy of the system in the multimedia domain.</p></td></tr><tr><td>311</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_3">SAKey: Scalable Almost Key Discovery in RDF Data</a></td></tr><tr><td colspan=3><p>Exploiting identity links among RDF resources allows applications to efficiently integrate data. Keys can be very useful to discover these identity links. A set of properties is considered as a key when its values uniquely identify resources. However, these keys are usually not available. The approaches that attempt to automatically discover keys can easily be overwhelmed by the size of the data and require clean data. We present SAKey, an approach that discovers keys in RDF data in an efficient way. To prune the search space, SAKey exploits characteristics of the data that are dynamically detected during the process. Furthermore, our approach can discover keys in datasets where erroneous data or duplicates exist (i.e., almost keys). The approach has been evaluated on different synthetic and real datasets. The results show both the relevance of almost keys and the efficiency of discovering them.</p></td></tr><tr><td>312</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_44">The Semantic Web and Human Inference: A Lesson from Cognitive Science</a></td></tr><tr><td colspan=3><p>For the development of Semantic Web technology, researchers and developers in the Semantic Web community need to focus on the areas in which human reasoning is particularly difficult. Two studies in this paper demonstrate that people are predisposed to use class-inclusion labels for inductive judgments. This tendency appears to stem from a general characteristic of human reasoning – using heuristics to solve problems. The inference engines and interface designs that incorporate human reasoning need to integrate this general characteristic underlying human induction.</p></td></tr><tr><td>313</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_41">nan</a></td></tr><tr><td colspan=3><p>Automatic knowledge reuse for Semantic Web applications imposes several challenges on ontology search. Existing ontology retrieval systems merely return a lengthy list of relevant single ontologies, which may not completely cover the specified user requirements. Therefore, there arises an increasing demand for a tool or algorithm with a mechanism to check concept adequacy of existing ontologies with respect to a user query, and then recommend a single or combination of ontologies which can entirely fulfill the requirements. Thus, this paper develops an algorithm, namely </p></td></tr><tr><td>314</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_45">From Web Directories to Ontologies: Natural Language Processing Challenges</a></td></tr><tr><td colspan=3><p>Hierarchical classifications are used pervasively by humans as a means to organize their data and knowledge about the world. One of their main advantages is that natural language labels, used to describe their contents, are easily understood by human users. However, at the same time, this is also one of their main disadvantages as these same labels are ambiguous and very hard to be reasoned about by software agents. This fact creates an insuperable hindrance for classifications to being embedded in the Semantic Web infrastructure. This paper presents an approach to converting classifications into lightweight ontologies, and it makes the following contributions: (i) it identifies the main NLP problems related to the conversion process and shows how they are different from the classical problems of NLP; (ii) it proposes heuristic solutions to these problems, which are especially effective in this domain; and (iii) it evaluates the proposed solutions by testing them on DMoz data.</p></td></tr><tr><td>315</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_43">Ontology Performance Profiling and Model Examination: First Steps</a></td></tr><tr><td colspan=3><p>“[Reasoner] performance can be scary, so much so, that we cannot deploy the technology in our products.” – Michael Shepard. What are typical OWL users to do when their favorite reasoner never seems to return? In this paper, we present our first steps considering this problem. We describe the challenges and our approach, and present a prototype tool to help users identify reasoner performance bottlenecks with respect to their ontologies. We then describe 4 case studies on synthetic and real-world ontologies. While the anecdotal evidence suggests that the service can be useful for both ontology developers and reasoner implementors, much more is desired.</p></td></tr><tr><td>316</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_38">Ontology-Based Interpretation of Keywords for Semantic Search</a></td></tr><tr><td colspan=3><p>Current information retrieval (IR) approaches do not formally capture the explicit meaning of a keyword query but provide a comfortable way for the user to specify information needs on the basis of keywords. Ontology-based approaches allow for sophisticated semantic search but impose a query syntax more difficult to handle. In this paper, we present an approach for translating keyword queries to DL conjunctive queries using background knowledge available in ontologies. We present an implementation which shows that this interpretation of keywords can then be used for both exploration of asserted knowledge and for a semantics-based declarative query answering process. We also present an evaluation of our system and a discussion of the limitations of the approach with respect to our underlying assumptions which directly points to issues for future work.</p></td></tr><tr><td>317</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_40">Sindice.com: Weaving the Open Linked Data</a></td></tr><tr><td colspan=3><p>Developers of Semantic Web applications face a challenge with respect to the decentralised publication model: where to find statements about encountered resources. The “linked data” approach, which mandates that resource URIs should be de-referenced and yield metadata about the resource, helps but is only a partial solution. We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index allows applications to automatically retrieve sources with information about a given resource. In addition we allow resource retrieval through inverse-functional properties, offer full-text search and index SPARQL endpoints.</p></td></tr><tr><td>318</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_42">PORE: Positive-Only Relation Extraction from Wikipedia Text</a></td></tr><tr><td colspan=3><p>Extracting semantic relations is of great importance for the creation of the Semantic Web content. It is of great benefit to semi-automatically extract relations from the free text of Wikipedia using the structured content readily available in it. Pattern matching methods that employ information redundancy cannot work well since there is not much redundancy information in Wikipedia, compared to the Web. Multi-class classification methods are not reasonable since no classification of relation types is available in Wikipedia. In this paper, we propose </p></td></tr><tr><td>319</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_39">RDFSync: Efficient Remote Synchronization of RDF Models</a></td></tr><tr><td colspan=3><p>In this paper we describe RDFSync, a methodology for efficient synchronization and merging of RDF models. RDFSync is based on decomposing a model into Minimum Self-Contained graphs (MSGs). After illustrating theory and deriving properties of MSGs, we show how a RDF model can be represented by a list of hashes of such information fragments. The synchronization procedure here described is based on the evaluation and remote comparison of these ordered lists. Experimental results show that the algorithm provides very significant savings on network traffic compared to the fileoriented synchronization of serialized RDF graphs. Finally, we provide the design and report the implementation of a protocol for executing the RDFSync algorithm over HTTP.</p></td></tr><tr><td>320</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_37">Lifecycle-Support in Architectures for Ontology-Based Information Systems</a></td></tr><tr><td colspan=3><p>Ontology-based applications play an increasingly important role in the public and corporate Semantic Web. While today there exist a range of tools and technologies to support specific ontology engineering and management activities, architectural design guidelines for building ontology-based applications are missing. In this paper, we present an architecture for ontology-based applications—covering the complete ontology-lifecycle—that is intended to support software engineers in designing and developing ontology-based applications. We illustrate the use of the architecture in a concrete case study using the NeOn toolkit as one implementation of the architecture.</p></td></tr><tr><td>321</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_33">Instance Migration in Heterogeneous Ontology Environments</a></td></tr><tr><td colspan=3><p>In this paper we address the problem of migrating instances between heterogeneous overlapping ontologies. The instance migration problem arises when one wants to reclassify a set of instances of a source ontology into a semantically related target ontology. Our approach exploits mappings between ontologies, which are used to reconcile both conceptual and individual level heterogeneity, and further used to draw the migration process. We ground the approach on a distributed description logic (DDL), in which ontologies are formally encoded as DL knowledge bases and mappings as bridge rules and individual correspondences. From the theoretical side, we study the task of reasoning with instance data in DDL composed of </p></td></tr><tr><td>322</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_35">A Caching Mechanism for Semantic Web Service Discovery</a></td></tr><tr><td colspan=3><p>The discovery of suitable Web services for a given task is one of the central operations in Service-oriented Architectures (SOA), and research on Semantic Web services (SWS) aims at automating this step. For the large amount of available Web services that can be expected in real-world settings, the computational costs of automated discovery based on semantic matchmaking become important. To make a discovery engine a reliable software component, we must thus aim at minimizing both the mean and the variance of the duration of the discovery task. For this, we present an extension for discovery engines in SWS environments that exploits structural knowledge and previous discovery results for reducing the search space of consequent discovery operations. Our prototype implementation shows significant improvements when applied to the Stanford SWS Challenge scenario and dataset.</p></td></tr><tr><td>323</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_36">A Method for Recommending Ontology Alignment Strategies</a></td></tr><tr><td colspan=3><p>In different areas ontologies have been developed and many of these ontologies contain overlapping information. Often we would therefore want to be able to use multiple ontologies. To obtain good results, we need to find the relationships between terms in the different ontologies, i.e. we need to align them. Currently, there already exist a number of different alignment strategies. However, it is usually difficult for a user that needs to align two ontologies to decide which of the different available strategies are the most suitable. In this paper we propose a method that provides recommendations on alignment strategies for a given alignment problem. The method is based on the evaluation of the different available alignment strategies on several small selected pieces from the ontologies, and uses the evaluation results to provide recommendations. In the paper we give the basic steps of the method, and then illustrate and discuss the method in the setting of an alignment problem with two well-known biomedical ontologies. We also experiment with different implementations of the steps in the method.</p></td></tr><tr><td>324</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_34">: Automatic Ontology Instantiation from Tabular Web Documents</a></td></tr><tr><td colspan=3><p>The process of instantiating an ontology with high-quality and up-to-date instance information manually is both time consuming and prone to error. </p></td></tr><tr><td>325</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_32">Using Tableau to Decide Expressive Description Logics with Role Negation</a></td></tr><tr><td colspan=3><p>This paper presents a tableau approach for deciding description logics outside the scope of OWL DL/1.1 and current state-of-the-art tableau-based description logic systems. In particular, we define a sound and complete tableau calculus for the description logic </p></td></tr><tr><td>326</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_29">Alternating-Offers Protocol for Multi-issue Bilateral Negotiation in Semantic-Enabled Marketplaces</a></td></tr><tr><td colspan=3><p>We present a semantic-based approach to multi-issue bilateral negotiation for e-commerce. We use Description Logics to model advertisements, and relations among issues as axioms in a TBox. We then introduce a logic-based alternating-offers protocol, able to handle conflicting information, that merges non-standard reasoning services in Description Logics with utility thoery to find the most suitable agreements. We illustrate and motivate the theoretical framework, the logical language, and the negotiation protocol.</p></td></tr><tr><td>327</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_30">An Event-Based Approach for Semantic Metadata Interoperability</a></td></tr><tr><td colspan=3><p>This paper presents a method for making metadata conforming to heterogeneous schemas semantically interoperable. The idea is to make the knowledge embedded in the schema structures interoperable and explicit by transforming the schemas into a shared, event-based representation of knowledge about the real world. This enables and simplifies accurate reasoning services such as cross-domain semantic search, browsing, and recommending. A case study of transforming three different schemas and datasets is presented. An implemented knowledge-based recommender system utilizing the results in the semantic portal </p></td></tr><tr><td>328</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_31">Evaluating the Semantic Web: A Task-Based Approach</a></td></tr><tr><td colspan=3><p>The increased availability of online knowledge has led to the design of several algorithms that solve a variety of tasks by harvesting the Semantic Web, i.e., by dynamically selecting and exploring a multitude of online ontologies. Our hypothesis is that the performance of such novel algorithms implicitly provides an insight into the quality of the used ontologies and thus opens the way to a task-based evaluation of the Semantic Web. We have investigated this hypothesis by studying the lessons learnt about online ontologies when used to solve three tasks: ontology matching, folksonomy enrichment, and word sense disambiguation. Our analysis leads to a suit of conclusions about the status of the Semantic Web, which highlight a number of strengths and weaknesses of the semantic information available online and complement the findings of other analysis of the Semantic Web landscape.</p></td></tr><tr><td>329</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_26">Ontology-Based Controlled Natural Language Editor Using CFG with Lexical Dependency</a></td></tr><tr><td colspan=3><p>In recent years, CNL (Controlled Natural Language) has received much attention with regard to ontology-based knowledge acquisition systems. CNLs, as subsets of natural languages, can be useful for both humans and computers by eliminating ambiguity of natural languages. Our previous work, OntoPath [10], proposed to edit natural language-like narratives that are structured in RDF (Resource Description Framework) triples, using a domain-specific ontology as their language constituents. However, our previous work and other systems employing CFG for grammar definition have difficulties in enlarging the expression capacity. A newly developed editor, which we propose in this paper, permits grammar definitions through CFG-LD (Context-Free Grammar with Lexical Dependency) that includes sequential and semantic structures of the grammars. With CFG describing the sequential structure of grammar, lexical dependencies between sentence elements can be designated in the definition system. Through the defined grammars, the implemented editor guides users’ narratives in more familiar expressions with a domain-specific ontology and translates the content into RDF triples.</p></td></tr><tr><td>330</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_27">Web Search Personalization Via Social Bookmarking and Tagging</a></td></tr><tr><td colspan=3><p>In this paper, we present a new approach to web search personalization based on user collaboration and sharing of information about web documents. The proposed personalization technique separates data collection and user profiling from the information system whose contents and indexed documents are being searched for, i.e. the search engines, and uses social bookmarking and tagging to re-rank web search results. It is independent of the search engine being used, so users are free to choose the one they prefer, even if their favorite search engine does not natively support personalization. We show how to design and implement such a system in practice and investigate its feasibility and usefulness with large sets of real-word data and a user study.</p></td></tr><tr><td>331</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_25">Bringing Semantic Annotations to Web Services: OWL-S from the SAWSDL Perspective</a></td></tr><tr><td colspan=3><p>Recently the World Wide Web Consortium (W3C) produced a standard set of “Semantic Annotations for WSDL and XML Schema” (SAWSDL). SAWSDL provides a standard means by which WSDL documents can be related to semantic descriptions, such as those provided by OWL-S (OWL for Services) and other Semantic Web services frameworks. We argue that the value of SAWSDL cannot be realized until its use is specified, and its benefits explained, in connection with a particular framework. This paper is an important first step toward meeting that need, with respect to OWL-S. We explain what OWL-S constructs are appropriate for use with the various SAWSDL annotations, and provide a rationale and guidelines for their use. In addition, we discuss some weaknesses of SAWSDL, and identify some ways in which OWL-S could evolve so as to integrate more smoothly with SAWSDL.</p></td></tr><tr><td>332</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_28">Measuring Incoherence in Description Logic-Based Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies play a core role in the success of the Semantic Web as they provide a shared vocabulary for different resources and applications. Developing an error-free ontology is a difficult task. A common kind of error for an ontology is logical contradiction or </p></td></tr><tr><td>333</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_24">Continuous RDF Query Processing over DHTs</a></td></tr><tr><td colspan=3><p>We study the continuous evaluation of conjunctive triple pattern queries over RDF data stored in distributed hash tables. In a continuous query scenario network nodes subscribe with long-standing queries and receive answers whenever RDF triples satisfying their queries are published. We present two novel query processing algorithms for this scenario and analyze their properties formally. Our performance goal is to have algorithms that scale to large amounts of RDF data, distribute the storage and query processing load evenly and incur as little network traffic as possible. We discuss the various performance tradeoffs that occur through a detailed experimental evaluation of the proposed algorithms.</p></td></tr><tr><td>334</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_23">Conjunctive Queries for a Tractable Fragment of OWL 1.1</a></td></tr><tr><td colspan=3><p>Despite the success of the Web Ontology Language OWL, the development of expressive means for querying OWL knowledge bases is still an open issue. In this paper, we investigate how a very natural and desirable form of queries—namely conjunctive ones—can be used in conjunction with OWL such that one of the major design criteria of the latter—namely decidability—can be retained. More precisely, we show that querying the tractable fragment </p></td></tr><tr><td>335</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_21">How Useful Are Natural Language Interfaces to the Semantic Web for Casual End-Users?</a></td></tr><tr><td colspan=3><p>Natural language interfaces offer end-users a familiar and convenient option for querying ontology-based knowledge bases. Several studies have shown that they can achieve high retrieval performance as well as domain independence. This paper focuses on usability and investigates if NLIs are useful from an end-user’s point of view. To that end, we introduce four interfaces each allowing a different query language and present a usability study benchmarking these interfaces. The results of the study reveal a clear preference for full sentences as query language and confirm that NLIs are useful for querying Semantic Web data.</p></td></tr><tr><td>336</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_22">The Fundamentals of iSPARQL: A Virtual Triple Approach for Similarity-Based Semantic Web Tasks</a></td></tr><tr><td colspan=3><p>This research explores three SPARQL-based techniques to solve Semantic Web tasks that often require similarity measures, such as semantic data integration, ontology mapping, and Semantic Web service matchmaking. Our aim is to see how far it is possible to integrate customized similarity functions (CSF) into SPARQL to achieve good results for these tasks. Our first approach exploits virtual triples calling property functions to establish virtual relations among resources under comparison; the second approach uses extension functions to filter out resources that do not meet the requested similarity criteria; finally, our third technique applies new solution modifiers to post-process a SPARQL solution sequence. The semantics of the three approaches are formally elaborated and discussed. We close the paper with a demonstration of the usefulness of our iSPARQL framework in the context of a data integration and an ontology mapping experiment.</p></td></tr><tr><td>337</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_17">Discovering Simple Mappings Between Relational Database Schemas and Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies proliferate with the growth of the Semantic Web. However, most of data on the Web are still stored in relational databases. Therefore, it is important to establish interoperability between relational databases and ontologies for creating a Web of data. An effective way to achieve interoperability is finding mappings between relational database schemas and ontologies. In this paper, we propose a new approach to discovering simple mappings between a relational database schema and an ontology. It exploits simple mappings based on virtual documents, and eliminates incorrect mappings via validating mapping consistency. Additionally, it also constructs a special type of semantic mappings, called contextual mappings, which is useful for practical applications. Experimental results demonstrate that our approach performs well on several data sets from real world domains.</p></td></tr><tr><td>338</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_19">An Empirical Study of Instance-Based Ontology Matching</a></td></tr><tr><td colspan=3><p>Instance-based ontology mapping is a promising family of solutions to a class of ontology alignment problems. It crucially depends on measuring the similarity between sets of annotated instances. In this paper we study how the choice of co-occurrence measures affects the performance of instance-based mapping.</p></td></tr><tr><td>339</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_18">Potluck: Data Mash-Up Tool for Casual Users</a></td></tr><tr><td colspan=3><p>As more and more reusable structured data appears on the Web, casual users will want to take into their own hands the task of mashing up data rather than wait for mash-up sites to be built that address exactly their individually unique needs. In this paper, we present Potluck, a Web user interface that lets casual users –those without programming skills and data modeling expertise–mash up data themselves.</p></td></tr><tr><td>340</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_15">SALT: Weaving the Claim Web</a></td></tr><tr><td colspan=3><p>In this paper we present a solution for “weaving the claim web”, i.e. the creation of knowledge networks via so-called claims stated in scientific publications created with the SALT (Semantically Annotated </p></td></tr><tr><td>341</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_16">YARS2: A Federated Repository for Querying Graph Structured Data from the Web</a></td></tr><tr><td colspan=3><p>We present the architecture of an end-to-end semantic search engine that uses a graph data model to enable interactive query answering over structured and interlinked data collected from many disparate sources on the Web. In particular, we study distributed indexing methods for graph-structured data and parallel query evaluation methods on a cluster of computers. We evaluate the system on a dataset with 430 million statements collected from the Web, and provide scale-up experiments on 7 billion synthetically generated statements.</p></td></tr><tr><td>342</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_20">Finding All Justifications of OWL DL Entailments</a></td></tr><tr><td colspan=3><p>Finding the justifications of an entailment (that is, all the minimal set of axioms sufficient to produce an entailment) has emerged as a key inference service for the Web Ontology Language (OWL). Justifications are essential for debugging unsatisfiable classes and contradictions. The availability of justifications as explanations of entailments improves the understandability of large and complex ontologies. In this paper, we present several algorithms for computing all the justifications of an entailment in an OWL-DL Ontology and show, by an empirical evaluation, that even a reasoner independent approach works well on real ontologies.</p></td></tr><tr><td>343</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_11">CLOnE: Controlled Language for Ontology Editing</a></td></tr><tr><td colspan=3><p>This paper presents a controlled language for ontology editing and a software implementation, based partly on standard NLP tools, for processing that language and manipulating an ontology. The input sentences are analysed deterministically and compositionally with respect to a given ontology, which the software consults in order to interpret the input’s semantics; this allows the user to learn fewer syntactic structures since some of them can be used to refer to either classes or instances, for example. A repeated-measures, task-based evaluation has been carried out in comparison with a well-known ontology editor; our software received favourable results for basic tasks. The paper also discusses work in progress and future plans for developing this language and tool.</p></td></tr><tr><td>344</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_12">Creating a Dead Poets Society: Extracting a Social Network of Historical Persons from the Web</a></td></tr><tr><td colspan=3><p>We present a simple method to extract information from search engine snippets. Although the techniques presented are domain independent, this work focuses on extracting biographical information of historical persons from multiple unstructured sources on the Web. We first similarly find a list of persons and their periods of life by querying the periods and scanning the retrieved snippets for person names. Subsequently, we find biographical information for the persons extracted. In order to get insight in the mutual relations among the persons identified, we create a social network using co-occurrences on the Web. Although we use uncontrolled and unstructured Web sources, the information extracted is reliable. Moreover we show that Web Information Extraction can be used to create both informative and enjoyable applications.</p></td></tr><tr><td>345</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_14">History Matters: Incremental Ontology Reasoning Using Modules</a></td></tr><tr><td colspan=3><p>The development of ontologies involves continuous but relatively small modifications. Existing ontology reasoners, however, do not take advantage of the similarities between different versions of an ontology. In this paper, we propose a technique for incremental reasoning—that is, reasoning that reuses information obtained from previous versions of an ontology—based on the notion of a module. Our technique does not depend on a particular reasoning calculus and thus can be used in combination with any reasoner. We have applied our results to incremental classification of OWL DL ontologies and found significant improvement over regular classification time on a set of real-world ontologies.</p></td></tr><tr><td>346</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_13">OBO and OWL: Leveraging Semantic Web Technologies for the Life Sciences</a></td></tr><tr><td colspan=3><p>OBO is an ontology language that has often been used for modeling ontologies in the life sciences. Its definition is relatively informal, so, in this paper, we provide a clear specification for OBO syntax and semantics via a mapping to OWL. This mapping also allows us to apply existing Semantic Web tools and techniques to OBO. We show that Semantic Web reasoners can be used to efficiently reason with OBO ontologies. Furthermore, we show that grounding the OBO language in formal semantics is useful for the ontology development process: using an OWL reasoner, we detected a likely modeling error in one OBO ontology.</p></td></tr><tr><td>347</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_6">An Ontology Design Pattern for Representing Relevance in OWL</a></td></tr><tr><td colspan=3><p>Design patterns are widely-used software engineering abstractions which define guidelines for modeling common application scenarios. Ontology design patterns are the extension of software patterns for knowledge acquisition in the Semantic Web. In this work we present a design pattern for representing relevance depending on context in OWL ontologies, i.e. to assert which knowledge from the domain ought to be considered in a given scenario. Besides the formal semantics and the features of the pattern, we describe a reasoning procedure to extract relevant knowledge in the resulting ontology and a plug-in for Protégé which assists pattern use.</p></td></tr><tr><td>348</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_10">Making More Wikipedians: Facilitating Semantics Reuse for Wikipedia Authoring</a></td></tr><tr><td colspan=3><p>Wikipedia, a killer application in Web 2.0, has embraced the power of collaborative editing to harness collective intelligence. It can also serve as an ideal Semantic Web data source due to its abundance, influence, high quality and well-structuring. However, the heavy burden of up-building and maintaining such an enormous and ever-growing online encyclopedic knowledge base still rests on a very small group of people. Many casual users may still feel difficulties in writing high quality Wikipedia articles. In this paper, we use RDF graphs to model the key elements in Wikipedia authoring, and propose an integrated solution to make Wikipedia authoring easier based on RDF graph matching, expecting making more Wikipedians. Our solution facilitates semantics reuse and provides users with: 1) a link suggestion module that suggests and auto-completes internal links between Wikipedia articles for the user; 2) a category suggestion module that helps the user place her articles in correct categories. A prototype system is implemented and experimental results show significant improvements over existing solutions to link and category suggestion tasks. The proposed enhancements can be applied to attract more contributors and relieve the burden of professional editors, thus enhancing the current Wikipedia to make it an even better Semantic Web data source.</p></td></tr><tr><td>349</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_9">A Cognitive Support Framework for Ontology Mapping</a></td></tr><tr><td colspan=3><p>Ontology mapping is the key to data interoperability in the semantic web. This problem has received a lot of research attention, however, the research emphasis has been mostly devoted to automating the mapping process, even though the creation of mappings often involve the user. As industry interest in semantic web technologies grows and the number of widely adopted semantic web applications increases, we must begin to support the user. In this paper, we combine data gathered from background literature, theories of cognitive support and decision making, and an observational case study to propose a theoretical framework for cognitive support in ontology mapping tools. We also describe a tool called </p></td></tr><tr><td>350</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_8">Scalable Cleanup of Information Extraction Data Using Ontologies</a></td></tr><tr><td colspan=3><p>The approach of using ontology reasoning to cleanse the output of information extraction tools was first articulated in SemantiClean. A limiting factor in applying this approach has been that ontology reasoning to find inconsistencies does not scale to the size of data produced by information extraction tools. In this paper, we describe techniques to scale inconsistency detection, and illustrate the use of our techniques to produce a consistent subset of a knowledge base with several thousand inconsistencies.</p></td></tr><tr><td>351</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_7">Logical Foundations of (e)RDF(S): Complexity and Reasoning</a></td></tr><tr><td colspan=3><p>An important open question in the semantic Web is the precise relationship between the RDF(S) semantics and the semantics of standard knowledge representation formalisms such as logic programming and description logics. In this paper we address this issue by considering embeddings of RDF and RDFS in logic. Using these embeddings, combined with existing results about various fragments of logic, we establish several novel complexity results. The embeddings we consider show how techniques from deductive databases and description logics can be used for reasoning with RDF(S). Finally, we consider querying RDF graphs and establish the data complexity of conjunctive querying for the various RDF entailment regimes.</p></td></tr><tr><td>352</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_5">Kernel Methods for Mining Instance Data in Ontologies</a></td></tr><tr><td colspan=3><p>The amount of ontologies and meta data available on the Web is constantly growing. The successful application of machine learning techniques for learning of ontologies from textual data, i.e. mining </p></td></tr><tr><td>353</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_4">How Service Choreography Statistics Reduce the Ontology Mapping Problem</a></td></tr><tr><td colspan=3><p>In open and distributed environments ontology mapping provides interoperability between interacting actors. However, conventional mapping systems focus on acquiring static information, and on mapping whole ontologies, which is infeasible in open systems. This paper shows that the interactions themselves between the actors can be used to predict mappings, simplifying dynamic ontology mapping. The intuitive idea is that similar interactions follow similar conventions and patterns, which can be analysed. The computed model can be used to suggest the possible mappings for the exchanged messages in new interactions. The suggestions can be evaluate by any standard ontology matcher: if they are accurate, the matchers avoid evaluating mappings unrelated to the interaction.</p></td></tr><tr><td>354</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_3">COMM: Designing a Well-Founded Multimedia Ontology for the Web</a></td></tr><tr><td colspan=3><p>Semantic descriptions of non-textual media available on the web can be used to facilitate retrieval and presentation of media assets and documents containing them. While technologies for multimedia semantic descriptions already exist, there is as yet no formal description of a high quality multimedia ontology that is compatible with existing (semantic) web technologies. We explain the complexity of the problem using an annotation scenario. We then derive a number of requirements for specifying a formal multimedia ontology before we present the developed ontology, COMM, and evaluate it with respect to our requirements. We provide an API for generating multimedia annotations that conform to COMM.</p></td></tr><tr><td>355</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_1">Enabling Advanced and Context-Dependent Access Control in RDF Stores</a></td></tr><tr><td colspan=3><p>Semantic Web databases allow efficient storage and access to RDF statements. Applications are able to use expressive query languages in order to retrieve relevant metadata to perform different tasks. However, access to metadata may not be public to just any application or service. Instead, powerful and flexible mechanisms for protecting sets of RDF statements are required for many Semantic Web applications. Unfortunately, current RDF stores do not provide fine-grained protection. This paper fills this gap and presents a mechanism by which complex and expressive policies can be specified in order to protect access to metadata in multi-service environments.</p></td></tr><tr><td>356</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_49">A Survey of the Web Ontology Landscape</a></td></tr><tr><td colspan=3><p>We survey nearly 1300 OWL ontologies and RDFS schemas. The collection of statistical data allows us to perform analysis and report some trends. Though most of the documents are syntactically OWL Full, very few stay in OWL Full when they are syntactically patched by adding type triples. We also report the frequency of occurrences of OWL language constructs and the shape of class hierarchies in the ontologies. Finally, we note that of the largest ontologies surveyed here, most do not exceed the description logic expressivity of </p></td></tr><tr><td>357</td><td>2007</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-76298-0_2">Automatically Composing Data Workflows with Relational Descriptions and Shim Services</a></td></tr><tr><td colspan=3><p>Many scientific problems can be represented as computational workflows of operations that access remote data, integrate heterogeneous data, and analyze and derive new data. Even when the data access and processing operations are implemented as web or grid services, workflows are often constructed </p></td></tr><tr><td>358</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_47">Framework for an Automated Comparison of Description Logic Reasoners</a></td></tr><tr><td colspan=3><p>OWL is an ontology language developed by the W3C, and although initially developed for the Semantic Web, OWL has rapidly become a de facto standard for ontology development in general. The design of OWL was heavily influenced by research in description logics, and the specification includes a formal semantics. One of the goals of this formal approach was to provide interoperability: different OWL reasoners should provide the same results when processing the same ontologies. In this paper we present a system that allows users: (a) to test and compare OWL reasoners using an extensible library of real-life ontologies; (b) to check the “correctness” of the reasoners by comparing the computed class hierarchy; (c) to compare the performance of the reasoners when performing this task; and (d) to use SQL queries to analyse and present the results in any way they see fit.</p></td></tr><tr><td>359</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_46">Tree-Structured Conditional Random Fields for Semantic Annotation</a></td></tr><tr><td colspan=3><p>The large volume of web content needs to be annotated by ontologies (called Semantic Annotation), and our empirical study shows that strong dependencies exist across different types of information (it means that identification of one kind of information can be used for identifying the other kind of information). Conditional Random Fields (CRFs) are the state-of-the-art approaches for modeling the dependencies to do better annotation. However, as information on a Web page is not necessarily linearly laid-out, the previous linear-chain CRFs have their limitations in semantic annotation. This paper is concerned with semantic annotation on hierarchically dependent data (</p></td></tr><tr><td>360</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_45">: A Cost Estimation Model for Ontology Engineering</a></td></tr><tr><td colspan=3><p>The technical challenges associated with the development and deployment of ontologies have been subject to a considerable number of research initiatives since the beginning of the nineties. The economical aspects of these processes are, however, still poorly exploited, impeding the dissemination of ontology-driven technologies beyond the boundaries of the academic community. This paper aims at contributing to the alleviation of this situation by proposing </p></td></tr><tr><td>361</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_44">Querying the Semantic Web with Preferences</a></td></tr><tr><td colspan=3><p>Ranking is an important concept to avoid empty or overfull and unordered result sets. However, such scoring can only express total orders, which restricts its usefulness when several factors influence result relevance. A more flexible way to express relevance is the notion of preferences. Users state which kind of answers they ‘prefer’ by adding soft constraints to their queries.</p></td></tr><tr><td>362</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_48">Integrating and Querying Parallel Leaf Shape Descriptions</a></td></tr><tr><td colspan=3><p>Information integration and retrieval have been important problems for many information systems — it is hard to combine new information with any other piece of related information we already possess, and to make them both available for application queries. Many ontology-based applications are still cautious about integrating and retrieving information from natural language (NL) documents, preferring structured or semi-structured sources. In this paper, we investigate how to use ontologies to facilitate integrating and querying information on parallel leaf shape descriptions from NL documents. Our approach takes advantage of ontologies to precisely represent the semantics in shape description, to integrates parallel descriptions according to their semantic distances, and to answer shape-related species identification queries. From this highly specialised domain, we learn a set of more general methodological rules, which could be useful in other domains.</p></td></tr><tr><td>363</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_43">Web Service Composition Via Generic Procedures and Customizing User Preferences</a></td></tr><tr><td colspan=3><p>We claim that user preferences are a key component of Web service composition – a component that has largely been ignored. In this paper we propose a means of specifying and intergrating user preferences into Web service composition. To this end, we propose a means of performing automated Web service composition by exploiting generic procedures together with rich qualitative user preferences. We exploit the agent programming language Golog to represent our generic procedures and a first-order preference language to represent rich qualitative temporal user preferences. From these we generate Web service compositions that realize the generic procedure, satisfying the user’s hard constraints and optimizing for the user’s preferences. We prove our approach sound and optimal. Our system, GologPref, is implemented and interacting with services on the Web. The language and techniques proposed in this paper can be integrated into a variety of approaches to Web or Grid service composition.</p></td></tr><tr><td>364</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_39">A Framework for Ontology Evolution in Collaborative Environments</a></td></tr><tr><td colspan=3><p>With the wider use of ontologies in the Semantic Web and as part of production systems, multiple scenarios for ontology maintenance and evolution are emerging. For example, successive ontology versions can be posted on the (Semantic) Web, with users discovering the new versions serendipitously; ontology-development in a collaborative environment can be synchronous or asynchronous; managers of projects may exercise quality control, examining changes from previous baseline versions and accepting or rejecting them before a new baseline is published, and so on. In this paper, we present different scenarios for ontology maintenance and evolution that we have encountered in our own projects and in those of our collaborators. We define several features that categorize these scenarios. For each scenario, we discuss the high-level tasks that an editing environment must support. We then present a unified comprehensive set of tools to support different scenarios in a single framework, allowing users to switch between different modes easily.</p></td></tr><tr><td>365</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_40">Extending Faceted Navigation for RDF Data</a></td></tr><tr><td colspan=3><p>Data on the Semantic Web is semi-structured and does not follow one fixed schema. Faceted browsing [23] is a natural technique for navigating such data, partitioning the information space into orthogonal conceptual dimensions. Current faceted interfaces are manually constructed and have limited query expressiveness. We develop an expressive faceted interface for semi-structured data and formally show the improvement over existing interfaces. Secondly, we develop metrics for automatic ranking of facet quality, bypassing the need for manual construction of the interface. We develop a prototype for faceted navigation of arbitrary RDF data. Experimental evaluation shows improved usability over current interfaces.</p></td></tr><tr><td>366</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_42">A Framework for Schema-Driven Relationship Discovery from Unstructured Text</a></td></tr><tr><td colspan=3><p>We address the issue of extracting implicit and explicit relationships between entities in biomedical text. We argue that entities seldom occur in text in their simple form and that relationships in text relate the modified, complex forms of entities with each other. We present a rule-based method for (1) extraction of such complex entities and (2) relationships between them and (3) the conversion of such relationships into RDF. Furthermore, we present results that clearly demonstrate the utility of the generated RDF in discovering knowledge from text corpora by means of locating paths composed of the extracted relationships.</p></td></tr><tr><td>367</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_41">Reducing the Inferred Type Statements with Individual Grouping Constructs</a></td></tr><tr><td colspan=3><p>A common approach for reasoning is to compute the deductive closure of an ontology using the rules specified and to work on the closure at query time. This approach reduces the run time complexity but increases the space requirements. The main reason of this increase is the type and subclass statements in the ontology. Type statements show a significant percentage in most ontologies. Since subclass is a transitive property, derivation of other statements, in particular type statements relying on it, gives rise to cyclic repetition and an excess of inferred type statements. In brief, a major part of closure computation is deriving the type statements relying on subclass statements. In this paper, we propose a syntactic transformation that is based on novel individual grouping constructs. This transformation reduces the number of inferred type statements relying on subclass relations. Thus, the space requirement of reasoning is reduced without affecting the soundness and the completeness.</p></td></tr><tr><td>368</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_38">Modeling Social Attitudes on the Web</a></td></tr><tr><td colspan=3><p>This paper argues that in order to allow for the representation, comparison and assessment of possibly controversial or uncertain information on the web, the semantic web effort requires capabilities for the social reasoning about web ontologies and other information acquired from multiple heterogeneous sources. As an approach to this, we propose formal means for the representation of possibly controversial opinions of groups and individuals, and of several other social attitudes regarding information on the web. Doing so, we integrate concepts from distributed artificial intelligence with approaches to web semantics, aiming for a </p></td></tr><tr><td>369</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_35">Extracting Relations in Social Networks from the Web Using Similarity Between Collective Contexts</a></td></tr><tr><td colspan=3><p>Social networks have recently garnered considerable interest. With the intention of utilizing social networks for the Semantic Web, several studies have examined automatic extraction of social networks. However, most methods have addressed extraction of the strength of relations. Our goal is extracting the underlying relations between entities that are embedded in social networks. To this end, we propose a method that automatically extracts labels that describe relations among entities. Fundamentally, the method clusters similar entity pairs according to their collective contexts in Web documents. The descriptive labels for relations are obtained from results of clustering. The proposed method is entirely unsupervised and is easily incorporated into existing social network extraction methods. Our method also contributes to ontology population by elucidating relations between instances in social networks. Our experiments conducted on entities in political social networks achieved clustering with high precision and recall. We extracted appropriate relation labels to represent the entities.</p></td></tr><tr><td>370</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_37">Innovation Detection Based on User-Interest Ontology of Blog Community</a></td></tr><tr><td colspan=3><p>Recently, the use of blogs has been a remarkable means to publish user interests. In order to find suitable information resources from a large amount of blog entries which are published every day, we need an information filtering technique to automatically transcribe user interests to a user profile in detail. In this paper, we first classify user blog entries into service domain ontologies and extract interest ontologies that express a user’s interests semantically as a hierarchy of classes according to interest weight by a top-down approach. Next, with a bottom-up approach, users modify their interest ontologies to update their interests in more detail. Furthermore, we propose a similarity measurement between ontologies considering the interest weight assigned to each class and instance. Then, we detect innovative blog entries that include concepts that the user has not thought about in the past based on the analysis of approximated ontologies of a user’s interests. We present experimental results that demonstrate the performance of our proposed methods using a large-scale blog entries and music domain ontologies.</p></td></tr><tr><td>371</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_36">Can OWL and Logic Programming Live Together Happily Ever After?</a></td></tr><tr><td colspan=3><p>Logic programming (LP) is often seen as a way to overcome several shortcomings of the Web Ontology Language (OWL), such as the inability to model integrity constraints or perform closed-world querying. However, the open-world semantics of OWL seems to be fundamentally incompatible with the closed-world semantics of LP. This has sparked a heated debate in the Semantic Web community, resulting in proposals for alternative ontology languages based entirely on logic programming. To help resolving this debate, we investigate the practical use cases which seem to be addressed by logic programming. In fact, many of these requirements have already been addressed outside the Semantic Web. By drawing inspiration from these existing formalisms, we present a novel logic of </p></td></tr><tr><td>372</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_33">Formal Model for Ontology Mapping Creation</a></td></tr><tr><td colspan=3><p>In a semantic environment data is described by ontologies and heterogeneity problems have to be solved at the ontological level. This means that alignments between ontologies have to be created, most probably during design-time, and used in various run-time processes. Such alignments describe a set of mappings between the source and target ontologies, where the mappings show how instance data from one ontology can be expressed in terms of another ontology. We propose a formal model for mapping creation. Starting from this model we explore how such a model maps onto a design-time graphical tool that can be used in creating alignments between ontologies. We also investigate how such a model helps in expressing the mappings in a logical language, based on the semantic relationships identified using the graphical tool.</p></td></tr><tr><td>373</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_34">A Semantic Context-Aware Access Control Framework for Secure Collaborations in Pervasive Computing Environments</a></td></tr><tr><td colspan=3><p>Wireless connectivity and widespread diffusion of portable devices offer novel opportunities for users to share resources anywhere and anytime, and to form ad-hoc coalitions. Resource access control is crucial to leverage these ad-hoc collaborations. In pervasive scenarios, however, collaborating entities cannot be predetermined and resource availability frequently varies, even unpredictably, due to user/device mobility, thus complicating resource access control. Access control policies cannot be defined based on entity’s identities/roles, as in traditional access control solutions, or be specified a priori to face any operative run time condition, but require continuous adjustments to adapt to the current situation. To address these issues, this paper advocates the adoption of novel access control policy models that follow two main design guidelines: </p></td></tr><tr><td>374</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_32">Ontology Query Answering on Databases</a></td></tr><tr><td colspan=3><p>With the fast development of Semantic Web, more and more RDF and OWL ontologies are created and shared. The effective management, such as storage, inference and query, of these ontologies on databases gains increasing attention. This paper addresses ontology query answering on databases by means of Datalog programs. Via epistemic operators, integrity constraints are introduced, and used for conveying semantic aspects of OWL that are not covered by Datalog-style rule languages. We believe such a processing suitable to capture ontologies in the database flavor, while keeping reasoning tractable. Here, we present a logically equivalent knowledge base whose (sound and complete) inference system appears as a Datalog program. As such, SPARQL query answering on OWL ontologies could be solved in databases. Bi-directional strategies, taking advantage of both forward and backward chaining, are then studied to support this kind of customized Datalog programs, returning exactly answers to the query within our logical framework.</p></td></tr><tr><td>375</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_30">PowerMap: Mapping the Real Semantic Web on the Fly</a></td></tr><tr><td colspan=3><p>Ontology mapping plays an important role in bridging the semantic gap between distributed and heterogeneous data sources. As the Semantic Web slowly becomes real and the amount of online semantic data increases, a new generation of tools is developed that automatically find and integrate this data. Unlike in the case of earlier tools where mapping has been performed at the design time of the tool, these new tools require mapping techniques that can be performed at run time. The contribution of this paper is twofold. First, we investigate the general requirements for run time mapping techniques. Second, we describe our PowerMap mapping algorithm that was designed to be used at run-time by an ontology based question answering tool.</p></td></tr><tr><td>376</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_31">Ontology-Driven Information Extraction with OntoSyphon</a></td></tr><tr><td colspan=3><p>The Semantic Web’s need for machine understandable content has led researchers to attempt to automatically acquire such content from a number of sources, including the web. To date, such research has focused on “document-driven” systems that individually process a small set of documents, annotating each with respect to a given ontology. This paper introduces OntoSyphon, an alternative that strives to more fully leverage existing ontological content while scaling to extract comparatively shallow content from millions of documents. OntoSyphon operates in an “ontology-driven” manner: taking any ontology as input, OntoSyphon uses the ontology to specify web searches that identify possible semantic instances, relations, and taxonomic information. Redundancy in the web, together with information from the ontology, is then used to automatically verify these candidate instances and relations, enabling OntoSyphon to operate in a fully automated, unsupervised manner. A prototype of OntoSyphon is fully implemented and we present experimental results that demonstrate substantial instance learning in a variety of domains based on independently constructed ontologies. We also introduce new methods for improving instance verification, and demonstrate that they improve upon previously known techniques.</p></td></tr><tr><td>377</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_26">Semantic Metadata Generation for Large Scientific Workflows</a></td></tr><tr><td colspan=3><p>In recent years, workflows have been increasingly used in scientific applications. This paper presents novel metadata reasoning capabilities that we have developed to support the creation of large workflows. They include 1) use of semantic web technologies in handling metadata constraints on file collections and nested file collections, 2) propagation and validation of metadata constraints from inputs to outputs in a workflow component, and through the links among components in a workflow, and 3) sub-workflows that generate metadata needed for workflow creation. We show how we used these capabilities to support the creation of large executable workflows in an earthquake science application with more than 7,000 jobs, generating metadata for more than 100,000 new files.</p></td></tr><tr><td>378</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_29">Evaluating Conjunctive Triple Pattern Queries over Large Structured Overlay Networks</a></td></tr><tr><td colspan=3><p>We study the problem of evaluating conjunctive queries composed of triple patterns over RDF data stored in distributed hash tables. Our goal is to develop algorithms that scale to large amounts of RDF data, distribute the query processing load evenly and incur little network traffic. We present and evaluate two novel query processing algorithms with these possibly conflicting goals in mind. We discuss the various tradeoffs that occur in our setting through a detailed experimental evaluation of the proposed algorithms.</p></td></tr><tr><td>379</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_27">Reaching Agreement over Ontology Alignments</a></td></tr><tr><td colspan=3><p>When agents communicate, they do not necessarily use the same vocabulary or ontology. For them to interact successfully, they must find correspondences (mappings) between the terms used in their respective ontologies. While many proposals for matching two agent ontologies have been presented in the literature, the resulting alignment may not be satisfactory to both agents, and thus may necessitate additional negotiation to identify a mutually agreeable set of correspondences.</p></td></tr><tr><td>380</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_25">The Summary Abox: Cutting Ontologies Down to Size</a></td></tr><tr><td colspan=3><p>Reasoning on OWL ontologies is known to be intractable in the worst-case, which is a serious problem because in practice, most OWL ontologies have large Aboxes, i.e., numerous assertions about individuals and their relations. We propose a technique that uses a summary of the ontology (</p></td></tr><tr><td>381</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_28">A Formal Model for Semantic Web Service Composition</a></td></tr><tr><td colspan=3><p>Automated composition of Web services or the process of forming new value added Web services is one of the most promising challenges in the semantic Web service research area. Semantics is one of the key elements for the automated composition of Web services because such a process requires rich machine-understandable descriptions of services that can be shared. Semantics enables Web service to describe their capabilities and processes, nevertheless there is still some work to be done. Indeed Web services described at functional level need a formal context to perform the automated composition of Web services. The suggested model (i.e., Causal link matrix) is a necessary starting point to apply problem-solving techniques such as regression-based search for Web service composition. The model supports a semantic context in order to find a correct, complete, consistent and optimal plan as a solution. In this paper an innovative and formal model for an AI planning-oriented composition is presented.</p></td></tr><tr><td>382</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_24">Mining Information for Instance Unification</a></td></tr><tr><td colspan=3><p>Instance unification determines whether two instances in an ontology refer to the same object in the real world. More specifically, this paper addresses the instance unification problem for person names. The approach combines the use of citation information (i.e., abstract, initials, titles and co-authorship information) with web mining, in order to gather additional evidence for the instance unification algorithm. The method is evaluated on two datasets – one from the BT digital library and one used in previous work on name disambiguation. The results show that the information mined from the web contributes substantially towards the successful handling of highly ambiguous cases which lowered the performance of previous methods.</p></td></tr><tr><td>383</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_21">Using Ontologies for Extracting Product Features from Web Pages</a></td></tr><tr><td colspan=3><p>In this paper, we show how to use ontologies to bootstrap a knowledge acquisition process that extracts product information from tabular data on Web pages. Furthermore, we use logical rules to reason about product specific properties and to derive higher-order knowledge about product features. We will also explain the knowledge acquisition process, covering both ontological and procedural aspects. Finally, we will give an qualitative and quantitative evaluation of our results.</p></td></tr><tr><td>384</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_23">A Relaxed Approach to RDF Querying</a></td></tr><tr><td colspan=3><p>We explore flexible querying of RDF data, with the aim of making it possible to return data satisfying query conditions with varying degrees of exactness, and also to rank the results of a query depending on how “closely” they satisfy the query conditions. We make queries more flexible by logical relaxation of their conditions based on RDFS entailment and RDFS ontologies. We develop a notion of ranking of query answers, and present a query processing algorithm for incrementally computing the relaxed answer of a query. Our approach has application in scenarios where there is a lack of understanding of the ontology underlying the data, or where the data objects have heterogeneous sets of properties or irregular structures.</p></td></tr><tr><td>385</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_20">/facet: A Browser for Heterogeneous Semantic Web Repositories</a></td></tr><tr><td colspan=3><p>Facet browsing has become popular as a user friendly interface to data repositories. The Semantic Web raises new challenges due to the heterogeneous character of the data. First, users should be able to select and navigate through facets of resources of any type and to make selections based on properties of other, semantically related, types. Second, where traditional facet browsers require manual configuration of the software, a semantic web browser should be able to handle any RDFS dataset without any additional configuration. Third, hierarchical data on the semantic web is not designed for browsing: complementary techniques, such as search, should be available to overcome this problem. We address these requirements in our browser, /facet. Additionally, the interface allows the inclusion of facet-specific display options that go beyond the hierarchical navigation that characterizes current facet browsing. /facet is a tool for Semantic Web developers as an instant interface to their complete dataset. The automatic facet configuration generated by the system can then be further refined to configure it as a tool for end users. The implementation is based on current Web standards and open source software. The new functionality is motivated using a scenario from the cultural heritage domain.</p></td></tr><tr><td>386</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_22">Block Matching for Ontologies</a></td></tr><tr><td colspan=3><p>Ontology matching is a crucial task to enable interoperation between Web applications using different but related ontologies. Today, most of the ontology matching techniques are targeted to find 1:1 mappings. However, block mappings are in fact more pervasive. In this paper, we discuss the block matching problem and suggest that both the mapping quality and the partitioning quality should be considered in block matching. We propose a novel partitioning-based approach to address the block matching issue. It considers both linguistic and structural characteristics of domain entities based on virtual documents, and uses a hierarchical bisection algorithm for partitioning. We set up two kinds of metrics to evaluate of the quality of block matching. The experimental results demonstrate that our approach is feasible.</p></td></tr><tr><td>387</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_19">MultiCrawler: A Pipelined Architecture for Crawling and Indexing Semantic Web Data</a></td></tr><tr><td colspan=3><p>The goal of the work presented in this paper is to obtain large amounts of semistructured data from the web. Harvesting semistructured data is a prerequisite to enabling large-scale query answering over web sources. We contrast our approach to conventional web crawlers, and describe and evaluate a five-step pipelined architecture to crawl and index data from both the traditional and the Semantic Web.</p></td></tr><tr><td>388</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_15">IRS-III: A Broker for Semantic Web Services Based Applications</a></td></tr><tr><td colspan=3><p>In this paper we describe IRS-III which takes a semantic broker based approach to creating applications from Semantic Web Services by mediating between a service requester and one or more service providers. Business organisations can view Semantic Web Services as the basic mechanism for integrating data and processes across applications on the Web. This paper extends previous publications on IRS by providing an overall description of our framework from the point of view of application development. More specifically, we describe the IRS-III methodology for building applications using Semantic Web Services and illustrate our approach through a use case on e-government.</p></td></tr><tr><td>389</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_18">Characterizing the Semantic Web on the Web</a></td></tr><tr><td colspan=3><p>Semantic Web languages are being used to represent, encode and exchange </p></td></tr><tr><td>390</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_17">On How to Perform a Gold Standard Based Evaluation of Ontology Learning</a></td></tr><tr><td colspan=3><p>In recent years several measures for the gold standard based evaluation of ontology learning were proposed. They can be distinguished by the layers of an ontology (e.g. lexical term layer and concept hierarchy) they evaluate. Judging those measures with a list of criteria we show that there exist some measures sufficient for evaluating the lexical term layer. However, existing measures for the evaluation of concept hierarchies fail to meet basic criteria. This paper presents a new taxonomic measure which overcomes the problems of current approaches.</p></td></tr><tr><td>391</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_16">Provenance Explorer – Customized Provenance Views Using Semantic Inferencing</a></td></tr><tr><td colspan=3><p>This paper presents </p></td></tr><tr><td>392</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_13">A Software Engineering Approach to Design and Development of Semantic Web Service Applications</a></td></tr><tr><td colspan=3><p>We present a framework for designing and developing Semantic Web Service applications that span over several enterprises by applying techniques, methodologies, and notations offered by Software engineering, Web engineering, and Business Process modeling. In particular, we propose to exploit existing standards for the specification of business processes (e.g., BPMN), for modeling the cross enterprise process, combined with powerful methodologies, tools and notations (e.g., WebML) borrowed from the Web engineering field for designing and developing semantically rich Web applications, with semi-automatic elicitation of semantic descriptions (i.e., WSMO Ontologies, Goals, Web Services and Mediators) from the design of the applications, with huge advantages in terms of efficiency of the design and reduction of the extra work necessary for semantically annotating the information crossing the organization boundaries.</p></td></tr><tr><td>393</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_14">A Model Driven Approach for Building OWL DL and OWL Full Ontologies</a></td></tr><tr><td colspan=3><p>This paper presents an approach for visually modeling OWL DL and OWL Full ontologies based on the well-established visual modeling language UML. We discuss a metamodel for OWL based on the Meta-Object Facility, an associated UML profile as visual syntax, and transformations between both. The work we present supports model-driven development of OWL ontologies and is currently undergoing the standardization process of the Object Management Group. After describing our approach, we present the implementation of our approach and an example, showing how the metamodel and UML profile can be used to improve developing Semantic Web applications.</p></td></tr><tr><td>394</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_12">Fresnel: A Browser-Independent Presentation Vocabulary for RDF</a></td></tr><tr><td colspan=3><p>Semantic Web browsers and other tools aimed at displaying RDF data to end users are all concerned with the same problem: presenting content primarily intended for machine consumption in a human-readable way. Their solutions differ but in the end address the same two high-level issues, no matter the underlying representation paradigm: specifying (i) </p></td></tr><tr><td>395</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_11">GINO – A Guided Input Natural Language Ontology Editor</a></td></tr><tr><td colspan=3><p>The casual user is typically overwhelmed by the formal logic of the Semantic Web. The gap between the end user and the logic-based scaffolding has to be bridged if the Semantic Web’s capabilities are to be utilized by the general public. This paper proposes that controlled natural languages offer one way to bridge the gap. We introduce GINO, a </p></td></tr><tr><td>396</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_7">RS2D: Fast Adaptive Search for Semantic Web Services in Unstructured P2P Networks</a></td></tr><tr><td colspan=3><p>In this paper, we present an approach, called RS2D v1, to adaptive probabilistic search for semantic web services in unstructured P2P networks. Each service agent dynamically learns the averaged query-answer behavior of its neighbor peers, and forwards service requests to those with minimal mixed Bayesian risk of doing so in terms of estimated semantic gain and commmunication cost. Experimental evaluation shows that the RS2D search mechanism is robust against changes in the network, and fast with reasonably high precision compared to other existing relevant approaches.</p></td></tr><tr><td>397</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_9">Automatic Annotation of Web Services Based on Workflow Definitions</a></td></tr><tr><td colspan=3><p>Semantic annotations of web services can facilitate the discovery of services, as well as their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service annotations available for general use. Resources for manual annotation are scarce, and therefore some means is required by which services can be automatically (or semi-automatically) annotated. In this paper, we show how information can be inferred about the semantics of operation parameters based on their connections to other (annotated) operation parameters within tried-and-tested workflows. In an open-world context, we can infer only constraints on the semantics of parameters, but these so-called </p></td></tr><tr><td>398</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_10">A Constraint-Based Approach to Horizontal Web Service Composition</a></td></tr><tr><td colspan=3><p>The task of automatically composing Web services involves two main composition processes, vertical and horizontal composition. Vertical composition consists of defining an appropriate combination of simple processes to perform a composition task. Horizontal composition process consists of determining the most appropriate Web service, from among a set of functionally equivalent ones for each component process. Several recent research efforts have dealt with the Web service composition problem. Nevertheless, most of them tackled only the vertical composition of Web services despite the growing trend towards functionally equivalent Web services. In an attempt to facilitate and streamline the process of horizontal composition of Web services while taking the above limitation into consideration, this work includes two main contributions. The first is a generic formalization of any Web service composition problem based on a constraint optimization problem (COP); this formalization is compatible to any Web service description language. The second contribution is an incremental user-intervention-based protocol to find the optimal composite Web service according to some predefined criteria at run-time. Our goal is </p></td></tr><tr><td>399</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_8">: Semantic Annotation for Accessibility</a></td></tr><tr><td colspan=3><p>Visually impaired users are hindered in their efforts to access the largest repository of electronic information in the world – the World Wide Web (Web). The web is visually-centric with regard to presentation and information order / layout, this can (and does) hinder users who need presentation-agnostic access to information. Transcoding can help to make information more accessible via a restructuring of pages. We describe an approach based on annotation of web pages, encoding semantic information that can then be used by tools in order to manipulate and present web pages in a form that provides easier access to content. Annotations are made directly to style sheet information, allowing the annotation of large numbers of similar pages with little effort.</p></td></tr><tr><td>400</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_6">On the Semantics of Linking and Importing in Modular Ontologies</a></td></tr><tr><td colspan=3><p>Modular ontology languages, such as Distributed Description Logics (DDL), </p></td></tr><tr><td>401</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_5">Augmenting Navigation for Collaborative Tagging with Emergent Semantics</a></td></tr><tr><td colspan=3><p>We propose an approach that unifies browsing by tags and visual features for intuitive exploration of image databases. In contrast to traditional image retrieval approaches, we utilise tags provided by users on collaborative tagging sites, complemented by simple image analysis and classification. This allows us to find new relations between data elements. We introduce the concept of a navigation map, that describes links between users, tags, and data elements for the example of the collaborative tagging site Flickr. We show that introducing similarity search based on image features yields additional links on this map. These theoretical considerations are supported by examples provided by our system, using data and tags from real Flickr users.</p></td></tr><tr><td>402</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_4">Ontology-Driven Automatic Entity Disambiguation in Unstructured Text</a></td></tr><tr><td colspan=3><p>Precisely identifying entities in web documents is essential for document indexing, web search and data integration. Entity disambiguation is the challenge of determining the correct entity out of various candidate entities. Our novel method utilizes background knowledge in the form of a populated ontology. Additionally, it does not rely on the existence of any structure in a document or the appearance of data items that can provide strong evidence, such as email addresses, for disambiguating person names. Originality of our method is demonstrated in the way it uses different relationships in a document as well as from the ontology to provide clues in determining the correct entity. We demonstrate the applicability of our method by disambiguating names of researchers appearing in a collection of DBWorld posts using a large scale, real-world ontology extracted from the DBLP bibliography website. The precision and recall measurements provide encouraging results.</p></td></tr><tr><td>403</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_3">Semantics and Complexity of SPARQL</a></td></tr><tr><td colspan=3><p>SPARQL is the W3C candidate recommendation query language for RDF. In this paper we address systematically the formal study of SPARQL, concentrating in its graph pattern facility. We consider for this study simple RDF graphs without special semantics for literals and a simplified version of filters which encompasses all the main issues. We provide a compositional semantics, prove there are normal forms, prove complexity bounds, among others that the evaluation of SPARQL patterns is PSPACE-complete, compare our semantics to an alternative operational semantics, give simple and natural conditions when both semantics coincide and discuss optimization procedures.</p></td></tr><tr><td>404</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_2">Three Semantics for Distributed Systems and Their Relations with Alignment Composition</a></td></tr><tr><td colspan=3><p>An ontology alignment explicitly describes the relations holding between two ontologies. A system composed of ontologies and alignments interconnecting them is herein called a distributed system. We give three different semantics of a distributed system, that do not interfere with the semantics of ontologies. Their advantages are compared with respect to allowing consistent merge of ontologies, managing heterogeneity and complying with an alignment composition operation. We show that only the first two variants, which differ from other proposed semantics, can offer a sound composition operation.</p></td></tr><tr><td>405</td><td>2006</td><td><a href="https://link.springer.com/chapter/10.1007/11926078_1">Ranking Ontologies with AKTiveRank</a></td></tr><tr><td colspan=3><p>Ontology search and reuse is becoming increasingly important as the quest for methods to reduce the cost of constructing such knowledge structures continues. A number of ontology libraries and search engines are coming to existence to facilitate locating and retrieving potentially relevant ontologies. The number of ontologies available for reuse is steadily growing, and so is the need for methods to evaluate and rank existing ontologies in terms of their relevance to the needs of the knowledge engineer. This paper presents AKTiveRank, a prototype system for ranking ontologies based on a number of structural metrics.</p></td></tr><tr><td>406</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_49">Benchmarking Database Representations of RDF/S Stores</a></td></tr><tr><td colspan=3><p>In this paper we benchmark three popular database representations of RDF/S schemata and data: (a) a schema-aware (i.e., one table per RDF/S class or property) with explicit (</p></td></tr><tr><td>407</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_48">Combining RDF and Part of OWL with Rules: Semantics, Decidability, Complexity</a></td></tr><tr><td colspan=3><p>This paper extends the model theory of RDF with rules, placing an emphasis on integration with OWL and decidability of entailment. We start from an abstract syntax that views a rule as a pair of rule graphs which generalize RDF graphs by also allowing rule variables in subject, predicate and object positions. We include RDFS as well as a decidable part of OWL that weakens </p></td></tr><tr><td>408</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_47">Introducing Autonomic Behaviour in Semantic Web Agents</a></td></tr><tr><td colspan=3><p>This paper presents </p></td></tr><tr><td>409</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_46">An Ontological Framework for Dynamic Coordination</a></td></tr><tr><td colspan=3><p>Coordination is the process of managing the possible interactions between activities and processes; a mechanism to handle such interactions is known as a coordination regime. A successful coordination regime will prevent negative interactions occurring (e.g., by preventing two processes from simultaneously accessing a non-shareable resource), and wherever possible will facilitate positive interactions (e.g., by ensuring that activities are not needlessly duplicated). We start from the premise that effective coordination mechanisms require the sharing of knowledge about activities, resources and their properties, and hence, that in a heterogeneous environment, an ontological approach to coordination is appropriate. After surveying recent work on dynamic coordination, we describe an ontology for coordination that we have developed with the goal of coordinating semantic web processes. We then present a implementation of our ideas, which serves as a proof of concept for how this ontology can be used for dynamic coordination. We conclude with a summary of the presented work, illustrate its relation to the Semantic Web, and provide insights into future extensions.</p></td></tr><tr><td>410</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_45">A String Metric for Ontology Alignment</a></td></tr><tr><td colspan=3><p>Ontologies are today a key part of every knowledge based system. They provide a source of shared and precisely defined terms, resulting in system interoperability by knowledge sharing and reuse. Unfortunately, the variety of ways that a domain can be conceptualized results in the creation of different ontologies with contradicting or overlapping parts. For this reason ontologies need to be brought into mutual agreement (aligned). One important method for ontology alignment is the comparison of class and property names of ontologies using string-distance metrics. Today quite a lot of such metrics exist in literature. But all of them have been initially developed for different applications and fields, resulting in poor performance when applied in this new domain. In the current paper we present a new string metric for the comparison of names which performs better on the process of ontology alignment as well as to many other field matching problems.</p></td></tr><tr><td>411</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_44">Containment and Minimization of RDF/S Query Patterns</a></td></tr><tr><td colspan=3><p>Semantic query optimization (SQO) has been proved to be quite useful in various applications (e.g., data integration, graphical query generators, caching, etc.) and has been extensively studied for relational, deductive, object, and XML databases. However, less attention to SQO has been devoted in the context of the Semantic Web. In this paper, we present sound and complete algorithms for the containment and minimization of RDF/S query patterns. More precisely, we consider two widely used RDF/S query fragments supporting pattern matching at the data, but also, at the schema level. To this end, we advocate a logic framework for capturing the RDF/S data model and semantics and we employ well-established techniques proposed in the relational context, in particular, the Chase and Backchase algorithms.</p></td></tr><tr><td>412</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_43">: A Tool for Relation Extraction from Text in Ontology Extension</a></td></tr><tr><td colspan=3><p>Domain ontologies very rarely model verbs as relations holding between concepts. However, the role of the verb as a central connecting element between concepts is undeniable. Verbs specify the interaction between the participants of some action or event by expressing relations between them. In parallel, it can be argued from an ontology engineering point of view that verbs express a relation between two classes that specify domain and range. The work described here is concerned with relation extraction for ontology extension along these lines. We describe a system (RelExt) that is capable of automatically identifying highly relevant triples (pairs of concepts connected by a relation) over concepts from an existing ontology. RelExt works by extracting relevant verbs and their grammatical arguments (i.e. terms) from a domain-specific text collection and computing corresponding relations through a combination of linguistic and statistical processing. The paper includes a detailed description of the system architecture and evaluation results on a constructed benchmark. RelExt has been developed in the context of the SmartWeb project, which aims at providing intelligent information services via mobile broadband devices on the FIFA World Cup that will be hosted in Germany in 2006. Such services include location based navigational information as well as question answering in the football domain.</p></td></tr><tr><td>413</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_39">OMEN: A Probabilistic Ontology Mapping Tool</a></td></tr><tr><td colspan=3><p>Most existing ontology mapping tools are inexact. Inexact ontology mapping rules, if not rectified, result in imprecision in the applications that use them. We describe a framework to probabilistically improve existing ontology mappings using a Bayesian Network. </p></td></tr><tr><td>414</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_41">A Bayesian Network Approach to Ontology Mapping</a></td></tr><tr><td colspan=3><p>This paper presents our ongoing effort on developing a principled methodology for automatic ontology mapping based on </p></td></tr><tr><td>415</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_38">Ontologies Are Us: A Unified Model of Social Networks and Semantics</a></td></tr><tr><td colspan=3><p>In our work we extend the traditional bipartite model of ontologies with the social dimension, leading to a tripartite model of actors, concepts and instances. We demonstrate the application of this representation by showing how community-based semantics emerges from this model through a process of graph transformation. We illustrate ontology emergence by two case studies, an analysis of a large scale folksonomy system and a novel method for the extraction of community-based ontologies from Web pages.</p></td></tr><tr><td>416</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_40">On the Properties of Metamodeling in OWL</a></td></tr><tr><td colspan=3><p>A common practice in conceptual modeling is to separate the intensional from the extensional model. Although very intuitive, this approach is inadequate for many complex domains, where the borderline between the two models is not clear-cut. Therefore, OWL-Full, the most expressive of the Semantic Web ontology languages, allows combining the intensional and the extensional model by a feature we refer to as </p></td></tr><tr><td>417</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_42">Ontology Change Detection Using a Version Log</a></td></tr><tr><td colspan=3><p>In this article, we propose a new ontology evolution approach that combines a top-down and a bottom-up approach. This means that the manual request for changes (top-down) by the ontology engineer is complemented with an automatic change detection mechanism (bottom-up). The approach is based on keeping track of the different versions of ontology concepts throughout their lifetime (called virtual versions). In this way, changes can be defined in terms of these virtual versions.</p></td></tr><tr><td>418</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_37">RUL: A Declarative Update Language for RDF</a></td></tr><tr><td colspan=3><p>We propose a declarative update language for RDF graphs which is based on the paradigms of query and view languages RQL and RVL. Our language, called RUL, ensures that the execution of the update primitives on nodes and arcs neither violates the semantics of the RDF model nor the semantics of the given RDFS schema. In addition, RUL supports fine-grained updates at the class and property instance level, set-oriented updates with a deterministic semantics and takes benefit of the full expressive power of RQL for restricting the range of variables to nodes and arcs of RDF graphs.</p></td></tr><tr><td>419</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_35">Information Modeling for End to End Composition of Semantic Web Services</a></td></tr><tr><td colspan=3><p>One of the main goals of the semantic web services effort is to enable automated composition of web services. An end-to-end view of the service composition process involves automation of composite service creation, development of executable workflows and deployment on an execution environment. However, the main focus in literature has been on the initial part of formally representing web service capabilities and reasoning about their composition using AI techniques. Based upon our experience in building an end-to-end composition tool for application integration, we bring out issues that have an impact on information modeling aspects of the composition process. In this paper, we present approaches for solving problems relating to scalability and manageability of service descriptions and data flow construction for operationalizing the composed services.</p></td></tr><tr><td>420</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_36">Searching Dynamic Communities with Personal Indexes</a></td></tr><tr><td colspan=3><p>Often the challenge of finding relevant information is reduced to find the ‘right’ people who will answer our question. In this paper we present innovative algorithms called INGA (Interest-based Node Grouping Algorithms) which integrate personal routing indices into semantic query processing to boost performance. Similar to social networks peers in INGA cooperate to efficiently route queries for documents along adaptive shortcut-based overlays using only local, but semantically well chosen information. We propose active and passive shortcut creation strategies for index building and a novel algorithm to select the most promising content providers depending on each peer index with respect to the individual query. We quantify the benefit of our indexing strategy by extensive performance experiments in the SWAP simulation infrastructure. While obtaining high recall values compared to other state-of-the-art algorithms, we show that INGA improves recall and reduces the number of messages significantly.</p></td></tr><tr><td>421</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_34">Representing Web Service Policies in OWL-DL</a></td></tr><tr><td colspan=3><p>Recently, there have been a number of proposals for languages for expressing web service constraints and capabilities, with WS-Policy and WSPL leading the way. The proposed languages, although relatively inexpressive, suffer from a lack of formal semantics. In this paper, we provide a mapping of WS-Policy to the description logic fragment species of the Web Ontology Language (OWL-DL), and describe how standard OWL-DL reasoners can be used to check policy conformance and perform an array of policy analysis tasks. OWL-DL is much more expressive than WS-Policy and thus provides a framework for exploring richer policy languages.</p></td></tr><tr><td>422</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_30">Reasoning with Multi-version Ontologies: A Temporal Logic Approach</a></td></tr><tr><td colspan=3><p>In this paper we propose a framework for reasoning with multi-version ontology, in which a temporal logic is developed to serve as its semantic foundation. We show that the temporal logic approach can provide a solid semantic foundation which can support various requirements on multi-version ontology reasoning. We have implemented the prototype of MORE (Multi-version Ontology REasoner), which is based on the proposed framework. We have tested MORE with several realistic ontologies. In this paper, we also discuss the implementation issues and report the experiments with MORE.</p></td></tr><tr><td>423</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_28">Preferential Reasoning on a Web of Trust</a></td></tr><tr><td colspan=3><p>We introduce a framework, based on logic programming, for preferential reasoning with agents on the Semantic Web. Initially, we encode the knowledge of an agent as a logic program equipped with call literals. Such call literals enable the agent to pose yes/no queries to arbitrary knowledge sources on the Semantic Web, without conditions on, e.g., the representation language of those sources. As conflicts may arise from reasoning with different knowledge sources, we use the extended answer set semantics, which can provide different strategies for solving those conflicts. Allowing, in addition, for an agent to express its preference for the satisfaction of certain rules over others, we can then induce a preference order on those strategies. However, since it is natural for an agent to believe its own knowledge (encoded in the program) but consider some sources more reliable than others, it can alternatively express preferences on call literals. Finally, we show how an agent can learn preferences on call literals if it is part of a web of trusted agents.</p></td></tr><tr><td>424</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_32">BRAHMS: A WorkBench RDF Store and High Performance Memory System for Semantic Association Discovery</a></td></tr><tr><td colspan=3><p>Discovery of semantic associations in Semantic Web ontologies is an important task in various analytical activities. Several query languages and storage systems have been designed and implemented for storage and retrieval of information in RDF ontologies. However, they are inadequate for semantic association discovery. In this paper we present the design and implementation of BRAHMS, an efficient RDF storage system, specifically designed to support fast semantic association discovery in large RDF bases. We present memory usage and timing results of several tests performed with BRAHMS and compare them to similar tests performed using Jena, Sesame, and Redland, three of the well-known RDF storage systems. Our results show that BRAHMS handles basic association discovery well, while the RDF query languages and even the low-level APIs in the other three tested systems are not suitable for the implementation of semantic association discovery algorithms.</p></td></tr><tr><td>425</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_33">A Template-Based Markup Tool for Semantic Web Content</a></td></tr><tr><td colspan=3><p>The Intelligence Community, among others, is increasingly using document metadata to improve document search and discovery on intranets and extranets. Document markup is still often incomplete, inconsistent, incorrect, and limited to keywords via HTML and XML tags. OWL promises to bring semantics to this markup to improve its machine understandability. A usable markup tool is becoming a barrier to the more widespread use of OWL markup in operational settings. This paper describes some of our attempts at building markup tools, lessons learned, and our latest markup tool, the Semantic Markup Tool (SMT). SMT uses automatic text extractors and templates to hide ontological complexity from end users and helps them quickly specify events and relationships of interest in the document. SMT automatically generates correct and consistent OWL markup. This comes at a cost to expressivity. We are evaluating SMT on several pilot semantic web efforts.</p></td></tr><tr><td>426</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_31">Piggy Bank: Experience the Semantic Web Inside Your Web Browser</a></td></tr><tr><td colspan=3><p>The Semantic Web Initiative envisions a Web wherein information is offered free of presentation, allowing more effective exchange and mixing across web sites and across web pages. But without substantial Semantic Web content, few tools will be written to consume it; without many such tools, there is little appeal to publish Semantic Web content.</p></td></tr><tr><td>427</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_29">Resolution-Based Approximate Reasoning for OWL DL</a></td></tr><tr><td colspan=3><p>We propose a new technique for approximate ABox reasoning with OWL DL ontologies. Essentially, we obtain substantially improved reasoning performance by disregarding non-Horn features of OWL DL. Our approach comes as a side-product of recent research results concerning a new transformation of OWL DL ontologies into negation-free disjunctive datalog [1, 2, 3, 4], and rests on the idea of performing standard resolution over disjunctive rules by treating them as if they were non-disjunctive ones. We analyse our reasoning approach by means of non-monotonic reasoning techniques, and present an implementation, called </p></td></tr><tr><td>428</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_27">A Framework for Handling Inconsistency in Changing Ontologies</a></td></tr><tr><td colspan=3><p>One of the major problems of large scale, distributed and evolving ontologies is the potential introduction of inconsistencies. In this paper we survey four different approaches to handling inconsistency in DL-based ontologies: consistent ontology evolution, repairing inconsistencies, reasoning in the presence of inconsistencies and multi-version reasoning. We present a common formal basis for all of them, and use this common basis to compare these approaches. We discuss the different requirements for each of these methods, the conditions under which each of them is applicable, the knowledge requirements of the various methods, and the different usage scenarios to which they would apply.</p></td></tr><tr><td>429</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_26">On Logical Consequence for Collections of OWL Documents</a></td></tr><tr><td colspan=3><p>In this paper, we investigate the (in)dependence among OWL documents with respect to the logical consequence when they are combined, in particular the inference of concept and role assertions about individuals. On the one hand, we present a systematic approach to identifying those documents that affect the inference of a given fact. On the other hand, we consider ways for fast detection of independence. First, we demonstrate several special cases in which two documents are independent of each other. Secondly, we introduce an algorithm for checking the independence in the general case. In addition, we describe two applications in which the above results have allowed us to develop novel approaches to overcome some difficulties in reasoning with large scale OWL data. Both applications demonstrate the usefulness of this work for improving the scalability of a practical Semantic Web system that relies on the reasoning about individuals.</p></td></tr><tr><td>430</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_48">Signal/Collect: Graph Algorithms for the (Semantic) Web</a></td></tr><tr><td colspan=3><p>The Semantic Web graph is growing at an incredible pace, enabling opportunities to discover new knowledge by interlinking and analyzing previously unconnected data sets. This confronts researchers with a conundrum: Whilst the data is available the programming models that facilitate scalability and the infrastructure to run various algorithms on the graph are missing.</p></td></tr><tr><td>431</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_47">Completeness Guarantees for Incomplete Reasoners</a></td></tr><tr><td colspan=3><p>We extend our recent work on evaluating incomplete reasoners by introducing </p></td></tr><tr><td>432</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_25">Seven Bottlenecks to Workflow Reuse and Repurposing</a></td></tr><tr><td colspan=3><p>To date on-line processes (</p></td></tr><tr><td>433</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_23">Semantically Rich Recommendations in Social Networks for Sharing, Exchanging and Ranking Semantic Context</a></td></tr><tr><td colspan=3><p>Recommender algorithms have been quite successfully employed in a variety of scenarios from filtering applications to recommendations of movies and books at Amazon.com. However, all these algorithms focus on single item recommendations and do not consider any more complex recommendation structures. This paper explores how semantically rich complex recommendation structures, represented as RDF graphs, can be exchanged and shared in a distributed social network. After presenting a motivating scenario we define several annotation ontologies we use in order to describe context information on the user’s desktop and show how our ranking algorithm can exploit this information. We discuss how social distributed networks and interest groups are specified using extended FOAF vocabulary, and how members of these interest groups share semantically rich recommendations in such a network. These recommendations transport shared context as well as ranking information, described in annotation ontologies. We propose an algorithm to compute these rankings which exploits available context information and show how rankings are influenced by the context received from other users as well as by the reputation of the members of the social network with whom the context is exchanged.</p></td></tr><tr><td>434</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_49">Summary Models for Routing Keywords to Linked Data Sources</a></td></tr><tr><td colspan=3><p>The proliferation of linked data on the Web paves the way to a new generation of applications that exploit heterogeneous data from different sources. However, because this Web of data is large and continuously evolving, it is non-trivial to identify the relevant link data sources and to express some given information needs as structured queries against these sources. In this work, we allow users to express needs in terms of simple keywords. Given the keywords, we define the problem of finding the relevant sources as the one of keyword query routing. As a solution, we present a family of summary models, which compactly represents the Web of linked data and allows to quickly find relevant sources. The proposed models capture information at different levels, representing summaries of varying granularity. They represent different trade-offs between effectiveness and efficiency. We provide a theoretical analysis of these trade-offs and also, verify them in experiments carried out in a real-world setting using more than 150 publicly available datasets.</p></td></tr><tr><td>435</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_22">Guidelines for Benchmarking the Performance of Ontology Management APIs</a></td></tr><tr><td colspan=3><p>Ontology tools performance and scalability are critical to both the growth of the Semantic Web and the establishment of these tools in the industry. In this paper, we present briefly the benchmarking methodology used to improve the performance and the scalability of ontology development tools. We focus on the definition of the infrastructure for evaluating the performance of these tools’ ontology management APIs in terms of its execution efficiency. We also present the results of applying the methodology for evaluating the API of the WebODE ontology engineering workbench.</p></td></tr><tr><td>436</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_24">On Partial Encryption of RDF-Graphs</a></td></tr><tr><td colspan=3><p>In this paper a method for Partial RDF Encryption (PRE) is proposed in which sensitive data in an RDF-graph is encrypted for a set of recipients while all non-sensitive data remain publicly readable. The result is an RDF-compliant self-describing graph containing encrypted data, encryption metadata, and plaintext data. For the representation of encrypted data and encryption metadata, the XML-Encryption and XML-Signature recommendations are used. The proposed method allows for fine-grained encryption of arbitrary subjects, predicates, objects and subgraphs of an RDF-graph. An XML vocabulary for specifying encryption policies is introduced.</p></td></tr><tr><td>437</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_21">Ontology Design Patterns for Semantic Web Content</a></td></tr><tr><td colspan=3><p>The paper presents a framework for introducing design patterns that facilitate or improve the techniques used during ontology lifecycle. Some distinctions are drawn between kinds of ontology design patterns. Some content-oriented patterns are presented in order to illustrate their utility at different degrees of abstraction, and how they can be specialized or composed. The proposed framework and the initial set of patterns are designed in order to function as a pipeline connecting domain modelling, user requirements, and ontology-driven tasks/queries to be executed.</p></td></tr><tr><td>438</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_20">Graph-Based Inferences in a Semantic Web Server for the Cartography of Competencies in a Telecom Valley</a></td></tr><tr><td colspan=3><p>We introduce an experience in building a public semantic web server maintaining annotations about the actors of a Telecom Valley. We then focus on an example of inference used in building one type of cartography of the competences of the economic actors of the Telecom Valley. We detailed how this inference exploits the graph model of the semantic web using ontology-based metrics and conceptual clustering. We prove the characteristics of theses metrics and inferences and we give the associated interpretations.</p></td></tr><tr><td>439</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_19">A General Diagnosis Method for Ontologies</a></td></tr><tr><td colspan=3><p>The effective debugging of ontologies is an important prerequisite for their successful application and impact on the semantic web. The heart of this debugging process is the diagnosis of faulty knowledge bases. In this paper we define general concepts for the diagnosis of ontologies. Based on these concepts, we provide correct and complete algorithms for the computation of minimal diagnoses of knowledge bases. These concepts and algorithms are broadly applicable since they are independent of a particular variant of an underlying logic (with monotonic semantics) and independent of a particular reasoning system. The practical feasibility of our method is shown by extensive test evaluations.</p></td></tr><tr><td>440</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_17">A Strategy for Automated Meaning Negotiation in Distributed Information Retrieval</a></td></tr><tr><td colspan=3><p>The paper reports on the development of the formal framework to design strategies for multi-issue non-symmetric meaning negotiations among software agents in a distributed information retrieval system. The advancements of the framework are the following. A resulting strategy compares the contexts of two background domain theories not concept by concept, but the whole context to the other context by accounting the relationships among concepts, the properties, the constraints over properties, and the available instances. It contains the mechanisms for measuring contextual similarity through assessing propositional substitutions and to provide argumentation through generating extra contexts. It uses presuppositions for choosing the best similarity hypotheses and to make the mutual concession to the common sense monotonic. It provides the means to evaluate the possible eagerness to concede through semantic commitments and related notions of knowledgeability and degree of reputation.</p></td></tr><tr><td>441</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_16">Bootstrapping Ontology Alignment Methods with APFEL</a></td></tr><tr><td colspan=3><p>Ontology alignment is a prerequisite in order to allow for interoperation between different ontologies and many alignment strategies have been proposed to facilitate the alignment task by (semi-)automatic means. Due to the complexity of the alignment task, manually defined methods for (semi-)automatic alignment rarely constitute an optimal configuration of substrategies from which they have been built. In fact, scrutinizing current ontology alignment methods, one may recognize that most are not optimized for given ontologies. Some few include machine learning for automating the task, but their optimization by machine learning means is mostly restricted to the extensional definition of ontology concepts. With APFEL (Alignment Process Feature Estimation and Learning) we present a machine learning approach that explores the user validation of initial alignments for optimizing alignment methods. The methods are based on extensional and intensional ontology definitions. Core to APFEL is the idea of a generic alignment process, the steps of which may be represented explicitly. APFEL then generates new hypotheses for what might be useful features and similarity assessments and weights them by machine learning approaches. APFEL compares favorably in our experiments to competing approaches.</p></td></tr><tr><td>442</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_14">Finding and Ranking Knowledge on the Semantic Web</a></td></tr><tr><td colspan=3><p>Swoogle helps software agents and knowledge engineers find Semantic Web knowledge encoded in RDF and OWL documents on the Web. Navigating such a Semantic Web on the Web is difficult due to the paucity of explicit </p></td></tr><tr><td>443</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_9">RDF Entailment as a Graph Homomorphism</a></td></tr><tr><td colspan=3><p>Semantic consequence (entailment) in RDF is ususally computed using Pat Hayes Interpolation Lemma. In this paper, we reformulate this mechanism as a graph homomorphism known as projection in the conceptual graphs community.</p></td></tr><tr><td>444</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_11">Querying Ontologies: A Controlled English Interface for End-Users</a></td></tr><tr><td colspan=3><p>The semantic web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expressions. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. So how can we help users to query a web of logic that they do not seem to understand? We address this problem by presenting a natural language interface to semantic web querying. The interface allows formulating queries in Attempto Controlled English (ACE), a subset of natural English. Each ACE query is translated into a discourse representation structure – a variant of the language of first-order logic – that is then translated into an N3-based semantic web querying language using an ontology-based rewriting framework. As the validation shows, our approach offers great potential for bridging the gap between the logic-based semantic web and its real-world users, since it allows users to query the semantic web without having to learn an unfamiliar formal language. Furthermore, we found that users liked our approach and designed good queries resulting in a very good retrieval performance (100% precision and 90% recall).</p></td></tr><tr><td>445</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_10">RitroveRAI: A Web Application for Semantic Indexing and Hyperlinking of Multimedia News</a></td></tr><tr><td colspan=3><p>In this paper, a system, RitroveRAI, addressing the general problem of enriching a multimedia news stream with semantic metadata is presented. News metadata here are explicitly derived from transcribed sentences or implicitly expressed into a topical category automatically detected. The enrichment process is accomplished by searching the same news expressed by different agencies reachable over the Web. Metadata extraction from the alternative sources (i.e. Web pages) is similarly applied and finally integration of the sources (according to some heuristic of pertinence) is carried out. Performance evaluation of the current system prototype has been carried out on a large scale. It confirms the viability of the RitroveRAI approach for realistic (i.e. 24 hours) applications and continuous monitoring and metadata extraction from multimedia news data.</p></td></tr><tr><td>446</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_15">Choreography in IRS-III – Coping with Heterogeneous Interaction Patterns in Web Services</a></td></tr><tr><td colspan=3><p>In this paper we describe how we handle heterogeneity in web service interaction through a choreography mechanism that we have developed for IRS-III. IRS-III is a framework and platform for developing semantic web services which utilizes the WSMO ontology. The overall design of our choreography framework is based on: the use of ontologies and state, IRS-III playing the role of a broker, differentiating between communication direction and which actor has the initiative, having representations which can be executed, a formal semantics, and the ability to suspend communication. Our framework has a full implementation which we illustrate through an example application.</p></td></tr><tr><td>447</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_18">On Applying the AGM Theory to DLs and OWL</a></td></tr><tr><td colspan=3><p>It is generally acknowledged that any Knowledge Base (KB) should be able to adapt itself to new information received. This problem has been extensively studied in the field of belief change, the dominating approach being the AGM theory. This theory set the standard for determining the rationality of a given belief change mechanism but was placed in a certain context which makes it inapplicable to logics used in the Semantic Web, such as Description Logics (DLs) and OWL. We believe the Semantic Web community would benefit from the application of the AGM theory to such logics. This paper is a preliminary study towards the feasibility of this application. Our approach raises interesting theoretical challenges and has an important practical impact too, given the central role that DLs and OWL play in the Semantic Web.</p></td></tr><tr><td>448</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_12">Semantic Browsing of Digital Collections</a></td></tr><tr><td colspan=3><p>Visiting museums is an increasingly popular pastime. Studies have shown that visitors can draw on their museum experience, long after their visit, to learn new things in practical situations. Rather than viewing a visit as a single learning event, we are interested in ways of extending the experience to allow visitors to access online resources tailored to their interests. Museums typically have extensive archives that can be made available online, the challenge is to match these resources to the visitor’s interests and present them in a manner that facilitates exploration and engages the visitor. We propose the use of knowledge level resource descriptions to identify relevant resources and create structured presentations. A system that embodies this approach, which is in use in a UK museum, is presented and the applicability of the approach to the broader semantic web is discussed.</p></td></tr><tr><td>449</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_8">A Large Scale Taxonomy Mapping Evaluation</a></td></tr><tr><td colspan=3><p>Matching hierarchical structures, like taxonomies or web directories, is the premise for enabling interoperability among heterogenous data organizations. While the number of new matching solutions is increasing the evaluation issue is still open. This work addresses the problem of comparison for pairwise matching solutions. A methodology is proposed to overcome the issue of scalability. A large scale dataset is developed based on real world case study namely, the web directories of Google, Looksmart and Yahoo!. Finally, an empirical evaluation is performed which compares the most representative solutions for taxonomy matching. We argue that the proposed dataset can play a key role in supporting the empirical analysis for the research effort in the area of taxonomy matching.</p></td></tr><tr><td>450</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_7">Web Service Composition with Volatile Information</a></td></tr><tr><td colspan=3><p>In many Web service composition problems, information may be needed from Web services during the composition process. Existing research on Web service composition (WSC) procedures has generally assumed that this information will not change. We describe two ways to take such WSC procedures and systematically modify them to deal with volatile information.</p></td></tr><tr><td>451</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_6">Towards a Formal Verification of OWL-S Process Models</a></td></tr><tr><td colspan=3><p>In this paper, we apply automatic tools to the verification of interaction protocols of Web services described in OWL-S. Specifically, we propose a modeling procedure that preserves the control flow and the data flow of OWL-S Process Models. The result of our work provides complete modeling and verification of OWL-S Process Models.</p></td></tr><tr><td>452</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_5">Stable Model Theory for Extended RDF Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies and automated reasoning are the building blocks of the Semantic Web initiative. Derivation rules can be included in an ontology to define derived concepts based on base concepts. For example, rules allow to define the extension of a class or property based on a complex relation between the extensions of the same or other classes and properties. On the other hand, the inclusion of negative information both in the form of negation-as-failure and explicit negative information is also needed to enable various forms of reasoning. In this paper, we extend RDF graphs with weak and strong negation, as well as derivation rules. The </p></td></tr><tr><td>453</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_4">Constructing Complex Semantic Mappings Between XML Data and Ontologies</a></td></tr><tr><td colspan=3><p>Much data is published on the Web in XML format satisfying schemas, and to make the Semantic Web a reality, such data needs to be interpreted with respect to ontologies. Interpretation is achieved through a </p></td></tr><tr><td>454</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_13">Decentralized Case-Based Reasoning for the Semantic Web</a></td></tr><tr><td colspan=3><p>Decentralized case-based reasoning (DzCBR) is a reasoning framework that addresses the problem of adaptive reasoning in a multi-ontology environment. It is a case-based reasoning (CBR) approach which relies on contextualized ontologies in the C-OWL formalism for the representation of domain knowledge and adaptation knowledge. A context in C-OWL is used to represent a particular viewpoint, containing the knowledge needed to solve a particular local problem. Semantic relations between contexts and the associated reasoning mechanisms allow the CBR process in a particular viewpoint to reuse and share information about the problem and the already found solutions in the other viewpoints.</p></td></tr><tr><td>455</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_2">Semantic Acceleration Helping Realize the Semantic Web Vision or “The Practical Web”</a></td></tr><tr><td colspan=3><p>The Semantic Web envisions a future where applications (computer programs) can make sense and therefore more productive use of all the information on the web by assigning common “meaning” to the millions of terms and phrases used in billions of documents. AI and knowledge representation must rise to the occasion and work with decentralized representations, imprecision and incompleteness. Standard web-based representations are an essential enabler and we have made good progress in their design. But we still rely on humans to assign semantics and here there is a big leap of faith: The World Wide Web has grown at startling rates because humans are prolific at producing enormous volumes of unstructured information, that is, information without explicit semantics; on the other hand navigating this mass of information has proven to be both possible and profitable to the point that there is a $6 B search advertising industry. It’s is not practical to expect the same will automatically happen for semantically enriched content. And yet we need semantics to better leverage the huge value on the web.</p></td></tr><tr><td>456</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_45">Preference-Based Web Service Composition: A Middle Ground between Execution and Search</a></td></tr><tr><td colspan=3><p>Much of the research on automated Web Service Composition (WSC) relates it to an AI planning task, where the composition is primarily done offline prior to execution. Recent research on WSC has argued convincingly for the importance of optimizing quality of service, trust, and user preferences. While some of this optimization can be done offline, many interesting and useful optimizations are data-dependent, and must be done following execution of at least some information-gathering services. In this paper, we examine this class of WSC problems, attempting to balance the trade-off between offline composition and online information gathering with a view to producing high-quality compositions efficiently and without excessive data gathering. Our investigation is performed in the context of the semantic web employing an existing preference-based Hierarchical Task Network WSC system. Our experiments illustrate the potential improvement in both the quality and speed of composition generation afforded by our approach.</p></td></tr><tr><td>457</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_3">Semantic Web Public Policy Challenges: Privacy, Provenance, Property and Personhood</a></td></tr><tr><td colspan=3><p>The growing inferencing and knowledge linking power of the Semantic Web will, we all hope, make the world a better place: enrich democratic discourse, support more rapid scientific discovery, enable new forms of personal communication and culture, and generally enhance critical analysis of information. However, with this greater inferencing power comes daunting social and public policy questions that must be faced as first class technical design challenges, not just as issues to be resolved in courts and legislatures. How will we maintain fundamental privacy values in the face of inferencing and searching power that can systematically uncover sensitive facts about us even has we try to keep such data secret? Today’s Web has enabled a departure from traditional editorial control and historically-trusted information sources. Will attention to provenance on the Semantic Web enable us to develop new mechanisms for assessing the reliability of information? What new challenges to already frayed intellectual property regimes will the Semantic Web bring? Finally, how will we assert and represent personal identity on the Semantic Web? At this early stage of the development of the Semantic Web, it’s hard enough to have problems in focus, much less solutions. However, we believe that transparent reasoning and accountability mechanisms will play a critical role in enabling systems and services built on the Semantic Web to be more responsive to social and policy needs.</p></td></tr><tr><td>458</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_46">A Self-Policing Policy Language</a></td></tr><tr><td colspan=3><p>Formal policies allow the non-ambiguous definition of situations in which usage of certain entities are allowed, and enable the automatic evaluation whether a situation is compliant. This is useful for example in applications using data provided via standardized interfaces. The low technical barriers of integrating such data sources is in contrast to the manual evaluation of natural language policies as they currently exist. Usage situations can themselves be regulated by policies, which can be restricted by the policy of a used entity. Consider for example the Google Maps API, which requires that applications using the API must be available without a fee, i.e. the application’s policy must not require a payment. In this paper we present a policy language that can express such constraints on other policies, i.e. a self-policing policy language. We validate our approach by realizing a use case scenario, using a policy engine developed for our language.</p></td></tr><tr><td>459</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_44">Query Strategy for Sequential Ontology Debugging</a></td></tr><tr><td colspan=3><p>Debugging is an important prerequisite for the wide-spread application of ontologies, especially in areas that rely upon everyday users to create and maintain knowledge bases, such as the Semantic Web. Most recent approaches use diagnosis methods to identify sources of inconsistency. However, in most debugging cases these methods return many alternative diagnoses, thus placing the burden of fault localization on the user. This paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by querying an oracle about entailments of the target ontology. We exploit probabilities of typical user errors to formulate information theoretic concepts for query selection. Our evaluation showed that the suggested method reduces the number of required observations compared to myopic strategies.</p></td></tr><tr><td>460</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_39">A Feature and Information Theoretic Framework for Semantic Similarity and Relatedness</a></td></tr><tr><td colspan=3><p>Semantic similarity and relatedness measures between ontology concepts are useful in many research areas. While similarity only considers subsumption relations to assess how two objects are alike, relatedness takes into account a broader range of relations (e.g., part-of). In this paper, we present a framework, which maps the feature-based model of similarity into the information theoretic domain. A new way of computing IC values directly from an ontology structure is also introduced. This new model, called Extended Information Content (</p></td></tr><tr><td>461</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_42">How to </a></td></tr><tr><td colspan=3><p>There are ontology domain concepts that can be represented according to multiple alternative classification criteria. Current ontology modeling guidelines do not explicitly consider this aspect in the representation of such concepts. To assist with this issue, we examined a domain-specific simplified model for facet analysis used in Library Science. This model produces a Faceted Classification Scheme (FCS) which accounts for the multiple alternative classification criteria of the domain concept under scrutiny. A comparative analysis between a FCS and the Normalisation Ontology Design Pattern (ODP) indicates the existence of key similarities between the elements in the generic structure of both knowledge representation models. As a result, a mapping is identified that allows to transform a FCS into an OWL DL ontology applying the Normalisation ODP. Our contribution is illustrated with an existing FCS example in the domain of “Dishwashing Detergent” that benefits from the outcome of this study.</p></td></tr><tr><td>462</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_36">Enhancing the Open-Domain Classification of Named Entity Using Linked Open Data</a></td></tr><tr><td colspan=3><p>Many applications make use of named entity classification. Machine learning is the preferred technique adopted for many named entity classification methods where the choice of features is critical to final performance. Existing approaches explore only the features derived from the characteristic of the named entity itself or its linguistic context. With the development of the Semantic Web, a large number of data sources are published and connected across the Web as Linked Open Data (LOD). LOD provides rich a priori knowledge about entity type information, knowledge that can be a valuable asset when used in connection with named entity classification. In this paper, we explore the use of LOD to enhance named entity classification. Our method extracts information from LOD and builds a type knowledge base which is used to score a (named entity string, type) pair. This score is then injected as one or more features into the existing classifier in order to improve its performance. We conducted a thorough experimental study and report the results, which confirm the effectiveness of our proposed method.</p></td></tr><tr><td>463</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_43">OWL-POLAR: Semantic Policies for Agent Reasoning</a></td></tr><tr><td colspan=3><p>Policies are declarations of constraints on the behaviour of components within distributed systems, and are often used to capture norms within agent-based systems. A few machine-processable representations for policies have been proposed, but they tend to be either limited in the types of policies that can be expressed or limited by the complexity of associated reasoning mechanisms. In this paper, we argue for a language that sufficiently expresses the types of policies essential in practical systems, and which enables both policy-governed decision-making and policy analysis within the bounds of decidability. We then propose an OWL-based representation of policies that meets these criteria using and a reasoning mechanism that uses a novel combination of ontology consistency checking and query answering. In this way, agent-based systems can be developed that operate flexibly and effectively in policy-constrainted environments.</p></td></tr><tr><td>464</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_34">Using Semantics for Automating the Authentication of Web APIs</a></td></tr><tr><td colspan=3><p>Recent technology developments in the area of services on the Web are marked by the proliferation of Web applications and APIs. The implementation and evolution of applications based on Web APIs is, however, hampered by the lack of automation that can be achieved with current technologies. Research on semantic Web services is therefore trying to adapt the principles and technologies that were devised for traditional Web services, to deal with this new kind of services. In this paper we show that currently more than 80% of the Web APIs require some form of authentication. Therefore authentication plays a major role for Web API invocation and should not be neglected in the context of mashups and composite data applications. We present a thorough analysis carried out over a body of publicly available APIs that determines the most commonly used authentication approaches. In the light of these results, we propose an ontology for the semantic annotation of Web API authentication information and demonstrate how it can be used to create semantic Web API descriptions. We evaluate the applicability of our approach by providing a prototypical implementation, which uses authentication annotations as the basis for automated service invocation.</p></td></tr><tr><td>465</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_40">Combining Approximation and Relaxation in Semantic Web Path Queries</a></td></tr><tr><td colspan=3><p>We develop query relaxation techniques for regular path queries and combine them with query approximation in order to support flexible querying of RDF data when the user lacks knowledge of its full structure or where the structure is irregular. In such circumstances, it is helpful if the querying system can perform both approximate matching and relaxation of the user’s query and can rank the answers according to how closely they match the original query. Our framework incorporates both standard notions of approximation based on edit distance and RDFS-based inference rules. The query language we adopt comprises conjunctions of regular path queries, thus including extensions proposed for SPARQL to allow for querying paths using regular expressions. We provide an incremental query evaluation algorithm which runs in polynomial time and returns answers to the user in ranked order.</p></td></tr><tr><td>466</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_35">Representing and Querying Validity Time in RDF and OWL: A Logic-Based Approach</a></td></tr><tr><td colspan=3><p>RDF(S) and OWL 2 currently support only static ontologies. In practice, however, the truth of statements often changes with time, and Semantic Web applications often need to represent such changes and reason about them. In this paper we present a logic-based approach for representing validity time in RDF and OWL. Unlike the existing proposals, our approach is applicable to entailment relations that are not deterministic, such as the Direct Semantics or the RDF-Based Semantics of OWL 2. We also extend SPARQL to temporal RDF graphs and present a query evaluation algorithm. Finally, we present an optimization of our algorithm that is applicable to entailment relations characterized by a set of deterministic rules, such RDF(S) and OWL 2 RL/RDF entailment.</p></td></tr><tr><td>467</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_38">Linking and Building Ontologies of Linked Data</a></td></tr><tr><td colspan=3><p>The Web of Linked Data is characterized by linking structured data from different sources using equivalence statements, such as owl:sameAs, as well as other types of linked properties. The ontologies behind these sources, however, remain unlinked. This paper describes an extensional approach to generate alignments between these ontologies. Specifically our algorithm produces equivalence and subsumption relationships between classes from ontologies of different Linked Data sources by exploring the space of hypotheses supported by the existing equivalence statements. We are also able to generate a complementary hierarchy of derived classes within an existing ontology or generate new classes for a second source where the ontology is not as refined as the first. We demonstrate empirically our approach using Linked Data sources from the geospatial, genetics, and zoology domains. Our algorithm discovered about 800 equivalences and 29,000 subset relationships in the alignment of five source pairs from these domains. Thus, we are able to model one Linked Data source in terms of another by aligning their ontologies and understand the semantic relationships between the two sources.</p></td></tr><tr><td>468</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_33">AnQL: SPARQLing Up Annotated RDFS</a></td></tr><tr><td colspan=3><p>Starting from the general framework for Annotated RDFS which we presented in previous work (extending Udrea et al’s Annotated RDF), we address the development of a query language – AnQL – that is inspired by SPARQL, including several features of SPARQL 1.1. As a side effect we propose formal definitions of the semantics of these features (subqueries, aggregates, assignment, solution modifiers) which could serve as a basis for the ongoing work in SPARQL 1.1. We demonstrate the value of such a framework by comparing our approach to previously proposed extensions of SPARQL and show that AnQL generalises and extends them.</p></td></tr><tr><td>469</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_37">Forgetting Fragments from Evolving Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies underpin the semantic web; they define the concepts and their relationships contained in a data source. An increasing number of ontologies are available on-line, but an ontology that combines information from many different sources can grow extremely large. As an ontology grows larger, more resources are required to use it, and its response time becomes slower. Thus, we present and evaluate an on-line approach that forgets fragments from an OWL ontology that are infrequently or no longer used, or are cheap to relearn, in terms of time and resources. In order to evaluate our approach, we situate it in a controlled simulation environment, RoboCup OWLRescue, which is an extension of the widely used RoboCup Rescue platform, which enables agents to build ontologies automatically based on the tasks they are required to perform. We benchmark our approach against other comparable techniques and show that agents using our approach spend less time forgetting concepts from their ontology, allowing them to spend more time deliberating their actions, to achieve a higher average score in the simulation environment.</p></td></tr><tr><td>470</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_32">Using Reformulation Trees to Optimize Queries over Distributed Heterogeneous Sources</a></td></tr><tr><td colspan=3><p>In order to effectively and quickly answer queries in environments with distributed RDF/OWL, we present a query optimization algorithm to identify the potentially relevant Semantic Web data sources using structural query features and a term index. This algorithm is based on the observation that the join selectivity of a pair of query triple patterns is often higher than the overall selectivity of these two patterns treated independently. Given a rule goal tree that expresses the reformulation of a conjunctive query, our algorithm uses a bottom-up approach to estimate the selectivity of each node. It then prioritizes loading of selective nodes and uses the information from these sources to further constrain other nodes. Finally, we use an OWL reasoner to answer queries over the selected sources and their corresponding ontologies. We have evaluated our system using both a synthetic data set and a subset of the real-world Billion Triple Challenge data.</p></td></tr><tr><td>471</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_29">Linked Data Query Processing Strategies</a></td></tr><tr><td colspan=3><p>Recently, processing of queries on linked data has gained attention. We identify and systematically discuss three main strategies: a bottom-up strategy that discovers new sources during query processing by following links between sources, a top-down strategy that relies on complete knowledge about the sources to select and process relevant sources, and a mixed strategy that assumes some incomplete knowledge and discovers new sources at run-time. To exploit knowledge discovered at run-time, we propose an additional step, explicitly scheduled during query processing, called </p></td></tr><tr><td>472</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_31">Optimize First, Buy Later: Analyzing Metrics to Ramp-Up Very Large Knowledge Bases</a></td></tr><tr><td colspan=3><p>As knowledge bases move into the landscape of larger ontologies and have terabytes of related data, we must work on optimizing the performance of our tools. We are easily tempted to buy bigger machines or to fill rooms with armies of little ones to address the scalability problem. Yet, careful analysis and evaluation of the characteristics of our data—using metrics—often leads to dramatic improvements in performance. Firstly, are current scalable systems scalable enough? We found that for large or deep ontologies (some as large as 500,000 classes) it is hard to say because benchmarks obscure the load-time costs for materialization. Therefore, to expose those costs, we have synthesized a set of more representative ontologies. Secondly, in designing for scalability, how do we manage knowledge over time? By optimizing for data distribution and ontology evolution, we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations linking 2.4 million terms from 200 ontologies to 3.5 million data elements, from one week to less than one hour for one of the large datasets on the same machine.</p></td></tr><tr><td>473</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_30">Making Sense of Twitter</a></td></tr><tr><td colspan=3><p>Twitter enjoys enormous popularity as a micro-blogging service largely due to its simplicity. On the downside, there is little organization to the Twitterverse and making sense of the stream of messages passing through the system has become a significant challenge for everyone involved. As a solution, Twitter users have adopted the convention of adding a hash at the beginning of a word to turn it into a </p></td></tr><tr><td>474</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_27">SPARQL Query Optimization on Top of DHTs</a></td></tr><tr><td colspan=3><p>We study the problem of SPARQL query optimization on top of distributed hash tables. Existing works on SPARQL query processing in such environments have never been implemented in a real system, or do not utilize any optimization techniques and thus exhibit poor performance. Our goal in this paper is to propose efficient and scalable algorithms for optimizing SPARQL basic graph pattern queries. We augment a known distributed query processing algorithm with query optimization strategies that improve performance in terms of query response time and bandwidth usage. We implement our techniques in the system Atlas and study their performance experimentally in a local cluster.</p></td></tr><tr><td>475</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_28">Optimizing Enterprise-Scale OWL 2 RL Reasoning in a Relational Database System</a></td></tr><tr><td colspan=3><p>OWL 2 RL was standardized as a less expressive but scalable subset of OWL 2 that allows a forward-chaining implementation. However, building an enterprise-scale forward-chaining based inference engine that can 1) take advantage of modern multi-core computer architectures, and 2) efficiently update inference for additions remains a challenge. In this paper, we present an OWL 2 RL inference engine implemented inside the Oracle database system, using novel techniques for parallel processing that can readily scale on multi-core machines and clusters. Additionally, we have added support for efficient incremental maintenance of the inferred graph after triple additions. Finally, to handle the increasing number of owl:sameAs relationships present in Semantic Web datasets, we have provided a hybrid in-memory/disk based approach to efficiently compute compact equivalence closures. We have done extensive testing to evaluate these new techniques; the test results demonstrate that our inference engine is capable of performing efficient inference over ontologies with billions of triples using a modest hardware configuration.</p></td></tr><tr><td>476</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_41">EvoPat – Pattern-Based Evolution and Refactoring of RDF Knowledge Bases</a></td></tr><tr><td colspan=3><p>Facilitating the seamless evolution of RDF knowledge bases on the Semantic Web presents still a major challenge. In this work we devise </p></td></tr><tr><td>477</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_25">An Expressive and Efficient Solution to the Service Selection Problem</a></td></tr><tr><td colspan=3><p>Given the large number of Semantic Web Services that can be created from online sources by using existing annotation tools, expressive formalisms and efficient and scalable approaches to solve the service selection problem are required to make these services widely available to the users. In this paper, we propose a framework that is grounded on logic and the Local-As-View approach for representing instances of the service selection problem. In our approach, Web services are semantically described using LAV mappings in terms of generic concepts from an ontology, user requests correspond to conjunctive queries on the generic concepts and, in addition, the user may specify a set of preferences that are used to rank the possible solutions to the given request. The LAV formulation allows us to cast the service selection problem as a query rewriting problem that must consider the relationships among the concepts in the ontology and the ranks induced by the preferences. Then, building on related work, we devise an encoding of the resulting query rewriting problem as a logical theory whose models are in correspondence with the solutions of the user request, and in presence of preferences, whose best models are in correspondence with the best-ranked solutions. Thus, by exploiting known properties of modern SAT solvers, we provide an efficient and scalable solution to the service selection problem. The approach provides the basis to represent a large number of real-world situations and interesting user requests.</p></td></tr><tr><td>478</td><td>2005</td><td><a href="https://link.springer.com/chapter/10.1007/11574620_1">Using the Semantic Web for e-Science: Inspiration, Incubation, Irritation</a></td></tr><tr><td colspan=3><p>We are familiar with the idea of e-Commerce – the electronic trading between consumers and suppliers. In recent years there has been a commensurate paradigm shift in the way that science is conducted. e-Science is science performed through distributed global collaborations between scientists and their resources enabled by electronic means, in order to solve scientific problems. No one scientific laboratory has the resources or tools, the raw data or derived understanding or the expertise to harness the knowledge available to a scientific community. Real progress depends on pooling know-how and results. It depends on collaboration and making connections between ideas, people, and data. It depends on finding and interpreting results and knowledge generated by scientific colleagues you do not know and who do not know you, to be analysed in ways they did not anticipate, to generate new hypotheses to be pooled in their turn. The importance of e-Science has been highlighted in the UK, for example, by an investment of over £240 million pounds over the past five years to specifically address the research and development issues that have to be tacked to develop a sustainable and effective e-Science e-Infrastructure.</p></td></tr><tr><td>479</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_23">Justification Oriented Proofs in OWL</a></td></tr><tr><td colspan=3><p>Justifications — that is, minimal entailing subsets of an ontology — are currently the dominant form of explanation provided by ontology engineering environments, especially those focused on the Web Ontology Language (OWL). Despite this, there are naturally occurring justifications that can be very difficult to understand. In essence, justifications are merely the premises of a proof and, as such, do not articulate the (often non-obvious) reasoning which connect those premises with the conclusion. This paper presents justification oriented proofs as a potential solution to this problem.</p></td></tr><tr><td>480</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_24">Toponym Resolution in Social Media</a></td></tr><tr><td colspan=3><p>Increasingly user-generated content is being utilised as a source of information, however each individual piece of content tends to contain low levels of information. In addition, such information tends to be informal and imperfect in nature; containing imprecise, subjective, ambiguous expressions. However the content does not have to be interpreted in isolation as it is linked, either explicitly or implicitly, to a network of interrelated content; it may be grouped or tagged with similar content, comments may be added by other users or it may be related to other content posted at the same time or by the same author or members of the author’s social network. This paper generally examines how ambiguous concepts within user-generated content can be assigned a specific/formal meaning by considering the expanding context of the information, i.e. other information contained within directly or indirectly related content, and specifically considers the issue of toponym resolution of locations.</p></td></tr><tr><td>481</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_26">Ontology Alignment for Linked Open Data</a></td></tr><tr><td colspan=3><p>The Web of Data currently coming into existence through the Linked Open Data (LOD) effort is a major milestone in realizing the Semantic Web vision. However, the development of applications based on LOD faces difficulties due to the fact that the different LOD datasets are rather loosely connected pieces of information. In particular, links between LOD datasets are almost exclusively on the level of instances, and schema-level information is being ignored. In this paper, we therefore present a system for finding schema-level links between LOD datasets in the sense of ontology alignment. Our system, called BLOOMS, is based on the idea of bootstrapping information already present on the LOD cloud. We also present a comprehensive evaluation which shows that BLOOMS outperforms state-of-the-art ontology alignment systems on LOD datasets. At the same time, BLOOMS is also competitive compared with these other systems on the Ontology Evaluation Alignment Initiative Benchmark datasets.</p></td></tr><tr><td>482</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_22">SAOR: Template Rule Optimisations for Distributed Reasoning over 1 Billion Linked Data Triples</a></td></tr><tr><td colspan=3><p>In this paper, we discuss optimisations of rule-based materialisation approaches for reasoning over large static RDF datasets. We generalise and re-formalise what we call the “partial-indexing” approach to scalable rule-based materialisation: the approach is based on a separation of terminological data, which has been shown in previous and related works to enable highly scalable and distributable reasoning for specific rulesets; in so doing, we provide some completeness propositions with respect to semi-naïve evaluation. We then show how related work on template rules – T-Box-specific dynamic rulesets created by binding the terminological patterns in the static ruleset – can be incorporated and optimised for the partial-indexing approach. We evaluate our methods using LUBM(10) for RDFS, pD* (OWL Horst) and OWL 2 RL, and thereafter demonstrate pragmatic distributed reasoning over 1.12 billion Linked Data statements for a subset of OWL 2 RL/RDF rules we argue to be suitable for Web reasoning.</p></td></tr><tr><td>483</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_21">Semantic Need: Guiding Metadata Annotations by Questions People #ask</a></td></tr><tr><td colspan=3><p>In its core, the Semantic Web is about the creation, collection and interlinking of metadata on which agents can perform tasks for human users. While many tools and approaches support either the creation </p></td></tr><tr><td>484</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_19">Finding the Achilles Heel of the Web of Data: Using Network Analysis for Link-Recommendation</a></td></tr><tr><td colspan=3><p>The Web of Data is increasingly becoming an important infrastructure for such diverse sectors as entertainment, government, e-commerce and science. As a result, the </p></td></tr><tr><td>485</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_18">Semantic Recognition of Ontology Refactoring</a></td></tr><tr><td colspan=3><p>Ontologies are used for sharing information and are often collaboratively developed. They are adapted for different applications and domains resulting in multiple versions of an ontology that are caused by changes and refactorings. Quite often, ontology versions (or parts of them) are syntactical very different but semantically equivalent. While there is existing work on detecting syntactical and structural changes in ontologies, there is still a need in analyzing and recognizing ontology changes and refactorings by a semantically comparison of ontology versions. In our approach, we start with a classification of model refactorings found in software engineering for identifying such refactorings in OWL ontologies using DL reasoning to recognize these refactorings.</p></td></tr><tr><td>486</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_20">When owl:sameAs Isn’t the Same: An Analysis of Identity in Linked Data</a></td></tr><tr><td colspan=3><p>In Linked Data, the use of </p></td></tr><tr><td>487</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_14">Assessing Trust in Uncertain Information</a></td></tr><tr><td colspan=3><p>On the Semantic Web, decision makers (humans or software agents alike) are faced with the challenge of examining large volumes of information originating from heterogeneous sources with the goal of ascertaining trust in various pieces of information. While previous work has focused on simple models for review and rating systems, we introduce a new trust model for rich, complex and uncertain information.We present the challenges raised by the new model, and the results of an evaluation of the first prototype implementation under a variety of scenarios.</p></td></tr><tr><td>488</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_12">One Size Does Not Fit All: Customizing Ontology Alignment Using User Feedback</a></td></tr><tr><td colspan=3><p>A key problem in ontology alignment is that different ontological features (</p></td></tr><tr><td>489</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_11">Deciding Agent Orientation on Ontology Mappings</a></td></tr><tr><td colspan=3><p>Effective communication in open environments relies on the ability of agents to reach a mutual understanding of the exchanged message by reconciling the vocabulary (ontology) used. Various approaches have considered how mutually acceptable mappings between corresponding concepts in the agents’ own ontologies may be determined dynamically through argumentation-based negotiation (such as Meaning-based Argumentation, </p></td></tr><tr><td>490</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_13">Compact Representation of Large RDF Data Sets for Publishing and Exchange</a></td></tr><tr><td colspan=3><p>Increasingly huge RDF data sets are being published on the Web. Currently, they use different syntaxes of RDF, contain high levels of redundancy and have a plain indivisible structure. All this leads to fuzzy publications, inefficient management, complex processing and lack of scalability. This paper presents a novel RDF representation (</p></td></tr><tr><td>491</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_15">Optimising Ontology Classification</a></td></tr><tr><td colspan=3><p>Ontology classification—the computation of subsumption hierarchies for classes and properties—is one of the most important tasks for OWL reasoners. Based on the algorithm by Shearer and Horrocks [9], we present a new classification procedure that addresses several open issues of the original algorithm, and that uses several novel optimisations in order to achieve superior performance. We also consider the classification of (object and data) properties. We show that algorithms commonly used to implement that task are incomplete even for relatively weak ontology languages. Furthermore, we show how to reduce the property classification problem into a standard (class) classification problem, which allows reasoners to classify properties using our optimised procedure. We have implemented our algorithms in the OWL HermiT reasoner, and we present the results of a performance evaluation.</p></td></tr><tr><td>492</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_10">SameAs Networks and Beyond: Analyzing Deployment Status and Implications of owl:sameAs in Linked Data</a></td></tr><tr><td colspan=3><p>Millions of owl:sameAs statements have been published on the Web of Data. Due to its unique role and heavy usage in Linked Data integration, owl:sameAs has become a topic of increasing interest and debate. This paper provides a quantitative analysis of owl:sameAs deployment status and uses these statistics to focus discussion around its usage in Linked Data.</p></td></tr><tr><td>493</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_9">Ontology Similarity in the Alignment Space</a></td></tr><tr><td colspan=3><p>Measuring similarity between ontologies can be very useful for different purposes, e.g., finding an ontology to replace another, or finding an ontology in which queries can be translated. Classical measures compute similarities or distances in an ontology space by directly comparing the content of ontologies. We introduce a new family of ontology measures computed in an alignment space: they evaluate the similarity between two ontologies with regard to the available alignments between them. We define two sets of such measures relying on the existence of a path between ontologies or on the ontology entities that are preserved by the alignments. The former accounts for known relations between ontologies, while the latter reflects the possibility to perform actions such as instance import or query translation. All these measures have been implemented in the OntoSim library, that has been used in experiments which showed that entity preserving measures are comparable to the best ontology space measures. Moreover, they showed a robust behaviour with respect to the alteration of the alignment space.</p></td></tr><tr><td>494</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_16">SPARQL beyond Subgraph Matching</a></td></tr><tr><td colspan=3><p>We extend the Semantic Web query language SPARQL by defining the semantics of SPARQL queries under the entailment regimes of RDF, RDFS, and OWL. The proposed extensions are part of the SPARQL 1.1 Entailment Regimes working draft which is currently being developed as part of the W3C standardization process of SPARQL 1.1. We review the conditions that SPARQL imposes on such extensions, discuss the practical difficulties of this task, and explicate the design choices underlying our proposals. In addition, we include an overview of current implementations and their underlying techniques.</p></td></tr><tr><td>495</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_17">Integrated Metamodeling and Diagnosis in OWL 2</a></td></tr><tr><td colspan=3><p>Ontological metamodeling has a variety of applications yet only very restricted forms are supported by OWL 2 directly. We propose a novel encoding scheme enabling class-based metamodeling inside the domain ontology with full reasoning support through standard OWL 2 reasoning systems. We demonstrate the usefulness of our method by applying it to the OntoClean methodology. En passant, we address performance problems arising from the inconsistency diagnosis strategy originally proposed for OntoClean by introducing an alternative technique where sources of conflicts are indicated by means of marker predicates.</p></td></tr><tr><td>496</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_7">Enabling Ontology-Based Access to Streaming Data Sources</a></td></tr><tr><td colspan=3><p>The availability of streaming data sources is progressively increasing thanks to the development of ubiquitous data capturing technologies such as sensor networks. The heterogeneity of these sources introduces the requirement of providing data access in a unified and coherent manner, whilst allowing the user to express their needs at an ontological level. In this paper we describe an ontology-based streaming data access service. Sources link their data content to ontologies through </p></td></tr><tr><td>497</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_8">Evolution of </a></td></tr><tr><td colspan=3><p>We study the problem of evolution for Knowledge Bases (KBs) expressed in Description Logics (DLs) of the </p></td></tr><tr><td>498</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_6">Supporting Natural Language Processing with Background Knowledge: Coreference Resolution Case</a></td></tr><tr><td colspan=3><p>Systems based on statistical and machine learning methods have been shown to be extremely effective and scalable for the analysis of large amount of textual data. However, in the recent years, it becomes evident that one of the most important directions of improvement in natural language processing (NLP) tasks, like word sense disambiguation, coreference resolution, relation extraction, and other tasks related to knowledge extraction, is by exploiting semantics. While in the past, the unavailability of rich and complete semantic descriptions constituted a serious limitation of their applicability, nowadays, the Semantic Web made available a large amount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitutes a valuable source of semantics. However, web semantics cannot be easily plugged into machine learning systems. Therefore the objective of this paper is to define a reference methodology for combining semantic information available in the web under the form of logical theories, with statistical methods for NLP. The major problems that we have to solve to implement our methodology concern (i) the selection of the correct and minimal knowledge among the large amount available in the web, (ii) the representation of uncertain knowledge, and (iii) the resolution and the encoding of the rules that combine knowledge retrieved from Semantic Web sources with semantics in the text. In order to evaluate the appropriateness of our approach, we present an application of the methodology to the problem of intra-document coreference resolution, and we show by means of some experiments on the standard dataset, how the injection of knowledge leads to the improvement of this task performance.</p></td></tr><tr><td>499</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_4">Talking about Data: Sharing Richly Structured Information through Blogs and Wikis</a></td></tr><tr><td colspan=3><p>Several projects have brought rich data semantics to collaborative wikis, but blogging platforms remain primarily limited to text. As blogs comprise a significant portion of the web’s content, engagement of the blogging community is crucial to the development of the semantic web. We provide a study of blog content to show a latent need for better data publishing and visualization support in blogging software. We then present DataPress, an extension to the WordPress blogging platform that enables users to publish, share, aggregate, and visualize structured information using the same workflow that they already apply to text-based content. In particular, we aim to preserve those attributes that make blogs such a successful publication medium: one-click access to the information, one-click publishing of it, natural authoring interfaces, and easy copy and paste of information (and visualizations) from other sources. We reflect on how our designs make progress toward these goals with a study of how users who installed DataPress made use of various features.</p></td></tr><tr><td>500</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_5"> with Default Attributes and Overriding</a></td></tr><tr><td colspan=3><p>Biomedical ontologies and semantic web policy languages based on description logics (DLs) provide fresh motivations for extending DLs with nonmonotonic inferences—a topic that has attracted a significant amount of attention along the years. Despite this, nonmonotonic inferences are not yet supported by the existing DL engines. One reason is the high computational complexity of the existing decidable fragments of nonmonotonic DLs. In this paper we identify a fragment of circumscribed </p></td></tr><tr><td>501</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_3">JustBench: A Framework for OWL Benchmarking</a></td></tr><tr><td colspan=3><p>Analysing the performance of OWL reasoners on expressive OWL ontologies is an ongoing challenge. In this paper, we present a new approach to performance analysis based on </p></td></tr><tr><td>502</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_2">Converting and Annotating Quantitative Data Tables</a></td></tr><tr><td colspan=3><p>Companies, governmental agencies and scientists produce a large amount of quantitative (research) data, consisting of measurements ranging from e.g. the surface temperatures of an ocean to the viscosity of a sample of mayonnaise. Such measurements are stored in tables in e.g. spreadsheet files and research reports. To integrate and reuse such data, it is necessary to have a semantic description of the data. However, the notation used is often ambiguous, making automatic interpretation and conversion to RDF or other suitable format difficult. For example, the table header cell “f (Hz)” refers to frequency measured in Hertz, but the symbol “f” can also refer to the unit farad or the quantities force or luminous flux. Current annotation tools for this task either work on less ambiguous data or perform a more limited task. We introduce new disambiguation strategies based on an ontology, which allows to improve performance on “sloppy” datasets not yet targeted by existing systems.</p></td></tr><tr><td>503</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_48">Produce and Consume Linked Data with Drupal!</a></td></tr><tr><td colspan=3><p>Currently a large number of Web sites are driven by Content Management Systems (CMS) which manage textual and multimedia content but also - inherently - carry valuable information about a site’s structure and content model. Exposing this structured information to the Web of Data has so far required considerable expertise in RDF and OWL modelling and additional programming effort. In this paper we tackle one of the most popular CMS: Drupal. We enable site administrators to export their site content model and data to the Web of Data without requiring extensive knowledge on Semantic Web technologies. Our modules create RDFa annotations and – optionally – a SPARQL endpoint for any Drupal site out of the box. Likewise, we add the means to map the site data to existing ontologies on the Web with a search interface to find commonly used ontology terms. We also allow a Drupal site administrator to include existing RDF data from remote SPARQL endpoints on the Web in the site. When brought together, these features allow networked RDF Drupal sites that reuse and enrich Linked Data. We finally discuss the adoption of our modules and report on a use case in the biomedical field and the current status of its deployment.</p></td></tr><tr><td>504</td><td>2010</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-17746-0_1">Fusion – Visually Exploring and Eliciting Relationships in Linked Data</a></td></tr><tr><td colspan=3><p>Building applications over Linked Data often requires a mapping between the application model and the ontology underlying the source dataset in the Linked Data cloud. This mapping can be defined in many ways. For instance, by describing the application model as a view over the source dataset, by giving mappings in the form of dependencies between the two datasets, or by inference rules that infer the application model from the source dataset. Explicitly formulating these mappings demands a comprehensive understanding of the underlying schemas (RDF ontologies) of the source and target datasets. This task can be supported by integrating the process of schema exploration into the mapping process and help the application designer with finding the implicit relationships that she wants to map. This paper describes Fusion - a framework for closing the gap between the application model and the underlying ontologies in the Linked Data cloud. Fusion simplifies the definition of mappings by providing a visual user interface that integrates the exploratory process and the mapping process. Its architecture allows the creation of new applications through the extension of existing Linked Data with additional data.</p></td></tr><tr><td>505</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_47">Enrichment and Ranking of the YouTube Tag Space and Integration with the Linked Data Cloud</a></td></tr><tr><td colspan=3><p>The increase of personal digital cameras with video functionality and video-enabled camera phones has increased the amount of user-generated videos on the Web. People are spending more and more time viewing online videos as a major source of entertainment and “infotainment”. Social websites allow users to assign shared free-form tags to user-generated multimedia resources, thus generating annotations for objects with a minimum amount of effort. Tagging allows communities to organise their multimedia items into browseable sets, but these tags may be poorly chosen and related tags may be omitted. Current techniques to retrieve, integrate and present this media to users are deficient and could do with improvement. In this paper, we describe a framework for semantic enrichment, ranking and integration of web video tags using Semantic Web technologies. Semantic enrichment of folksonomies can bridge the gap between the uncontrolled and flat structures typically found in user-generated content and structures provided by the Semantic Web. The enhancement of tag spaces with semantics has been accomplished through two major tasks: (1) a tag space expansion and ranking step; and (2) through concept matching and integration with the Linked Data cloud. We have explored social, temporal and spatial contexts to enrich and extend the existing tag space. The resulting semantic tag space is modelled via a local graph based on co-occurrence distances for ranking. A ranked tag list is mapped and integrated with the Linked Data cloud through the DBpedia resource repository. Multi-dimensional context filtering for tag expansion means that tag ranking is much easier and it provides less ambiguous tag to concept matching.</p></td></tr><tr><td>506</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_49">Extracting Enterprise Vocabularies Using Linked Open Data</a></td></tr><tr><td colspan=3><p>A common vocabulary is vital to smooth business operation, yet codifying and maintaining an enterprise vocabulary is an arduous, manual task. We describe a process to automatically extract a domain specific vocabulary (terms and types) from unstructured data in the enterprise guided by term definitions in Linked Open Data (LOD). We validate our techniques by applying them to the IT (Information Technology) domain, taking 58 Gartner analyst reports and using two specific LOD sources – DBpedia and Freebase. We show initial findings that address the generalizability of these techniques for vocabulary extraction in new domains, such as the energy industry.</p></td></tr><tr><td>507</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_44">Live Social Semantics</a></td></tr><tr><td colspan=3><p>Social interactions are one of the key factors to the success of conferences and similar community gatherings. This paper describes a novel application that integrates data from the semantic web, online social networks, and a real-world contact sensing platform. This application was successfully deployed at ESWC09, and actively used by 139 people. Personal profiles of the participants were automatically generated using several Web 2.0 systems and semantic academic data sources, and integrated in real-time with face-to-face contact networks derived from wearable sensors. Integration of all these heterogeneous data layers made it possible to offer various services to conference attendees to enhance their social experience such as visualisation of contact data, and a site to explore and connect with other participants. This paper describes the architecture of the application, the services we provided, and the results we achieved in this deployment.</p></td></tr><tr><td>508</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_46">LinkedGeoData: Adding a Spatial Dimension to the Web of Data</a></td></tr><tr><td colspan=3><p>In order to employ the Web as a medium for data and information integration, comprehensive datasets and vocabularies are required as they enable the disambiguation and alignment of other data and information. Many real-life information integration and aggregation tasks are impossible without comprehensive background knowledge related to spatial features of the ways, structures and landscapes surrounding us. In this paper we contribute to the generation of a spatial dimension for the Data Web by elaborating on how the collaboratively collected OpenStreetMap data can be transformed and represented adhering to the RDF data model. We describe how this data can be interlinked with other spatial data sets, how it can be made accessible for machines according to the linked data paradigm and for humans by means of a faceted geo-data browser.</p></td></tr><tr><td>509</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_45">RAPID: Enabling Scalable Ad-Hoc Analytics on the Semantic Web</a></td></tr><tr><td colspan=3><p>As the amount of available RDF data continues to increase steadily, there is growing interest in developing efficient methods for analyzing such data. While recent efforts have focused on developing efficient methods for traditional data processing, analytical processing which typically involves more complex queries has received much less attention. The use of cost effective parallelization techniques such as Google’s Map-Reduce offer significant promise for achieving Web scale analytics. However, currently available implementations are designed for simple data processing on structured data.</p></td></tr><tr><td>510</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_43">Parallel Materialization of the Finite RDFS Closure for Hundreds of Millions of Triples</a></td></tr><tr><td colspan=3><p>In this paper, we consider the problem of materializing the complete finite RDFS closure in a scalable manner; this includes those parts of the RDFS closure that are often ignored such as literal generalization and container membership properties. We point out characteristics of RDFS that allow us to derive an embarrassingly parallel algorithm for producing said closure, and we evaluate our C/MPI implementation of the algorithm on a cluster with 128 cores using different-size subsets of the LUBM 10,000-university data set. We show that the time to produce inferences scales linearly with the number of processes, evaluating this behavior on up to hundreds of millions of triples. We also show the number of inferences produced for different subsets of LUBM10k. To the best of our knowledge, our work is the first to provide RDFS inferencing on such large data sets in such low times. Finally, we discuss future work in terms of promising applications of this approach including OWL2RL rules, MapReduce implementations, and massive scaling on supercomputers.</p></td></tr><tr><td>511</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_42">Concept and Role Forgetting in </a></td></tr><tr><td colspan=3><p>Forgetting is an important tool for reducing ontologies by eliminating some concepts and roles while preserving sound and complete reasoning. Attempts have previously been made to address the problem of forgetting in relatively simple description logics (DLs) such as DL-Lite and extended </p></td></tr><tr><td>512</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_41">Discovering and Maintaining Links on the Web of Data</a></td></tr><tr><td colspan=3><p>The Web of Data is built upon two simple ideas: Employ the RDF data model to publish structured data on the Web and to create explicit data links between entities within different data sources. This paper presents the Silk – Linking Framework, a toolkit for discovering and maintaining data links between Web data sources. Silk consists of three components: 1. A link discovery engine, which computes links between data sources based on a declarative specification of the conditions that entities must fulfill in order to be interlinked; 2. A tool for evaluating the generated data links in order to fine-tune the linking specification; 3. A protocol for maintaining data links between continuously changing data sources. The protocol allows data sources to exchange both linksets as well as detailed change information and enables continuous link recomputation. The interplay of all the components is demonstrated within a life science use case.</p></td></tr><tr><td>513</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_40">Scalable Distributed Reasoning Using MapReduce</a></td></tr><tr><td colspan=3><p>We address the problem of scalable distributed reasoning, proposing a technique for materialising the closure of an RDF graph based on MapReduce. We have implemented our approach on top of Hadoop and deployed it on a compute cluster of up to 64 commodity machines. We show that a naive implementation on top of MapReduce is straightforward but performs badly and we present several non-trivial optimisations. Our algorithm is scalable and allows us to compute the RDFS closure of 865M triples from the Web (producing 30B triples) in less than two hours, faster than any other published approach.</p></td></tr><tr><td>514</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_38">Optimizing Web Service Composition While Enforcing Regulations</a></td></tr><tr><td colspan=3><p>To direct automated Web service composition, it is compelling to provide a template, workflow or scaffolding that dictates the ways in which services can be composed. In this paper we present an approach to Web service composition that builds on work using AI planning, and more specifically Hierarchical Task Networks (HTNs), for Web service composition. A significant advantage of our approach is that it provides much of the how-to knowledge of a choreography while enabling customization and optimization of integrated Web service selection and composition based upon the needs of the specific problem, the preferences of the customer, and the available services. Many customers must also be concerned with enforcement of regulations, perhaps in the form of corporate policies and/or government regulations. Regulations are traditionally enforced at design time by verifying that a workflow or composition adheres to regulations. Our approach supports customization, optimization and regulation enforcement all at composition construction time. To maximize efficiency, we have developed novel search heuristics together with a branch and bound search algorithm that enable the generation of high quality compositions with the performance of state-of-the-art planning systems.</p></td></tr><tr><td>515</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_39">A Weighted Approach to Partial Matching for Mobile Reasoning</a></td></tr><tr><td colspan=3><p>Due to significant improvements in the capabilities of small devices such as PDAs and smart phones, these devices can not only consume but also provide Web Services. The dynamic nature of mobile environment means that users need accurate and fast approaches for service discovery. In order achieve high accuracy semantic languages can be used in conjunction with logic reasoners. Since powerful broker nodes are not always available (due to lack of long range connectivity), create a bottleneck (since mobile devices are all trying to access the same server) and single point of failure (in the case that a central server fails), on-board mobile reasoning must be supported. However, reasoners are notoriously resource intensive and do not scale to small devices. Therefore, in this paper we provide an efficient mobile reasoner which relaxes the current strict and complete matching approaches to support anytime reasoning. Our approach matches the most important request conditions (deemed by the user) first and provides a degree of match and confidence result to the user. We provide a prototype implementation and performance evaluation of our work.</p></td></tr><tr><td>516</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_35">Policy-Aware Content Reuse on the Web</a></td></tr><tr><td colspan=3><p>The Web allows users to share their work very effectively leading to the rapid re-use and remixing of content on the Web including text, images, and videos. Scientific research data, social networks, blogs, photo sharing sites and other such applications known collectively as the Social Web have lots of increasingly complex information. Such information from several Web pages can be very easily aggregated, mashed up and presented in other Web pages. Content generation of this nature inevitably leads to many copyright and license violations, motivating research into effective methods to detect and prevent such violations.</p></td></tr><tr><td>517</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_30">On Detecting High-Level Changes in RDF/S KBs</a></td></tr><tr><td colspan=3><p>An increasing number of scientific communities rely on Semantic Web ontologies to share and interpret data within and across research domains. These common knowledge representation resources are usually developed and maintained manually and essentially co-evolve along with experimental evidence produced by scientists worldwide. Detecting automatically the differences between (two) versions of the same ontology in order to store or visualize their deltas is a challenging task for e-science. In this paper, we focus on languages allowing the formulation of concise and intuitive deltas, which are expressive enough to describe unambiguously any possible change and that can be effectively and efficiently detected. We propose a specific language that provably exhibits those characteristics and provide a change detection algorithm which is sound and complete with respect to the proposed language. Finally, we provide a promising experimental evaluation of our framework using real ontologies from the cultural and bioinformatics domains.</p></td></tr><tr><td>518</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_33">A Conflict-Based Operator for Mapping Revision</a></td></tr><tr><td colspan=3><p>Ontology matching is one of the key research topics in the field of the Semantic Web. There are many matching systems that generate mappings between different ontologies either automatically or semi-automatically. However, the mappings generated by these systems may be inconsistent with the ontologies. Several approaches have been proposed to deal with the inconsistencies between mappings and ontologies. This problem is often called a mapping revision problem, as the ontologies are assumed to be correct, whereas the mappings are repaired when resolving the inconsistencies. In this paper, we first propose a conflict-based mapping revision operator and show that it can be characterized by two logical postulates adapted from some existing postulates for belief base revision. We then provide an algorithm for iterative mapping revision by using an ontology revision operator and show that this algorithm defines a conflict-based mapping revision operator. Three concrete ontology revision operators are given to instantiate the iterative algorithm, which result in three different mapping revision algorithms. We implement these algorithms and provide some preliminary but interesting evaluation results.</p></td></tr><tr><td>519</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_31">Efficient Query Answering for OWL 2</a></td></tr><tr><td colspan=3><p>The QL profile of OWL 2 has been designed so that it is possible to use database technology for query answering via query rewriting. We present a comparison of our resolution based rewriting algorithm with the standard algorithm proposed by Calvanese et al., implementing both and conducting an empirical evaluation using ontologies and queries derived from realistic applications. The results indicate that our algorithm produces significantly smaller rewritings in most cases, which could be important for practicality in realistic applications.</p></td></tr><tr><td>520</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_36">Exploiting Partial Information in Taxonomy Construction</a></td></tr><tr><td colspan=3><p>One of the core services provided by OWL reasoners is </p></td></tr><tr><td>521</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_34">Functions over RDF Language Elements</a></td></tr><tr><td colspan=3><p>RDF data are usually accessed using one of two methods: either, graphs are rendered in forms perceivable by human users (e.g., in tabular or in graphical form), which are difficult to handle for large data sets. Alternatively, query languages like SPARQL provide means to express information needs in structured form; hence they are targeted towards developers and experts. Inspired by the concept of spreadsheet tools, where users can perform relatively complex calculations by splitting formulas and values across multiple cells, we have investigated mechanisms that allow us to access RDF graphs in a more intuitive and manageable, yet formally grounded manner. In this paper, we make three contributions towards this direction. First, we present RDFunctions, an algebra that consists of mappings between sets of RDF language elements (URIs, blank nodes, and literals) under consideration of the triples contained in a background graph. Second, we define a syntax for expressing RDFunctions, which can be edited, parsed and evaluated. Third, we discuss Tripcel, an implementation of RDFunctions using a spreadsheet metaphor. Using this tool, users can easily edit and execute function expressions and perform analysis tasks on the data stored in an RDF graph.</p></td></tr><tr><td>522</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_29">Towards Lightweight and Robust Large Scale Emergent Knowledge Processing</a></td></tr><tr><td colspan=3><p>We present a lightweight framework for processing uncertain emergent knowledge that comes from multiple resources with varying relevance. The framework is essentially RDF-compatible, but allows also for direct representation of contextual features (e.g., provenance). We support soft integration and robust querying of the represented content based on well-founded notions of aggregation, similarity and ranking. A proof-of-concept implementation is presented and evaluated within large scale knowledge-based search in life science articles.</p></td></tr><tr><td>523</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_28">Investigating the Semantic Gap through Query Log Analysis</a></td></tr><tr><td colspan=3><p>Significant efforts have focused in the past years on bringing large amounts of metadata online and the success of these efforts can be seen by the impressive number of web sites exposing data in RDFa or RDF/XML. However, little is known about the extent to which this data fits the needs of ordinary web users with everyday information needs. In this paper we study what we perceive as the semantic gap between the supply of data on the Semantic Web and the needs of web users as expressed in the queries submitted to a major Web search engine. We perform our analysis on both the level of instances and ontologies. First, we first look at how much data is actually relevant to Web queries and what kind of data is it. Second, we provide a generic method to extract the attributes that Web users are searching for regarding particular classes of entities. This method allows to contrast class definitions found in Semantic Web vocabularies with the attributes of objects that users are interested in. Our findings are crucial to measuring the potential of semantic search, but also speak to the state of the Semantic Web in general.</p></td></tr><tr><td>524</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_37">Actively Learning Ontology Matching via User Interaction</a></td></tr><tr><td colspan=3><p>Ontology matching plays a key role for semantic interoperability. Many methods have been proposed for automatically finding the alignment between heterogeneous ontologies. However, in many real-world applications, finding the alignment in a completely automatic way is </p></td></tr><tr><td>525</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_27">Learning Semantic Query Suggestions</a></td></tr><tr><td colspan=3><p>An important application of semantic web technology is recognizing human-defined concepts in text. Query transformation is a strategy often used in search engines to derive queries that are able to return more useful search results than the original query and most popular search engines provide facilities that let users complete, specify, or reformulate their queries. We study the problem of </p></td></tr><tr><td>526</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_26">A Practical Approach for Scalable Conjunctive Query Answering on Acyclic </a></td></tr><tr><td colspan=3><p>Conjunctive query answering for </p></td></tr><tr><td>527</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_32">Multi Visualization and Dynamic Query for Effective Exploration of Semantic Data</a></td></tr><tr><td colspan=3><p>Semantic formalisms represent content in a uniform way according to ontologies. This enables manipulation and reasoning via automated means (e.g. Semantic Web services), but limits the user’s ability to explore the semantic data from a point of view that originates from knowledge representation motivations. We show how, for user consumption, a visualization of semantic data according to some easily graspable dimensions (e.g. space and time) provides effective sense-making of data. In this paper, we look holistically at the interaction between users and semantic data, and propose multiple visualization strategies and dynamic filters to support the exploration of semantic-rich data. We discuss a user evaluation and how interaction challenges could be overcome to create an effective user-centred framework for the visualization and manipulation of semantic data. The approach has been implemented and evaluated on a real company archive.</p></td></tr><tr><td>528</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_22">Semantic Web Service Composition in Social Environments</a></td></tr><tr><td colspan=3><p>This paper describes how to generate compositions of semantic Web services using social trust information from user ratings of the services. We present a taxonomy of features, such as interoperability, availability, privacy, security, and others. We describe a way to compute social trust in OWL-S style semantic Web services. Our formalism exploits the users’ ratings of the services and execution characteristics of those services. We describe our service-composition algorithm, called </p></td></tr><tr><td>529</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_23">XLWrap – Querying and Integrating Arbitrary Spreadsheets with SPARQL</a></td></tr><tr><td colspan=3><p>In this paper a novel approach is presented for generating RDF graphs of arbitrary complexity from various spreadsheet layouts. Currently, none of the available spreadsheet-to-RDF wrappers supports cross tables and tables where data is not aligned in rows. Similar to RDF123, XLWrap is based on template graphs where fragments of triples can be mapped to specific cells of a spreadsheet. Additionally, it features a full expression algebra based on the syntax of OpenOffice Calc and various shift operations, which can be used to repeat similar mappings in order to wrap cross tables including multiple sheets and spreadsheet files. The set of available expression functions includes most of the native functions of OpenOffice Calc and can be easily extended by users of XLWrap.</p></td></tr><tr><td>530</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_18">Using Naming Authority to Rank Data and Ontologies for Web Search</a></td></tr><tr><td colspan=3><p>The focus of web search is moving away from returning relevant documents towards returning structured data as results to user queries. A vital part in the architecture of search engines are link-based ranking algorithms, which however are targeted towards hypertext documents. Existing ranking algorithms for structured data, on the other hand, require manual input of a domain expert and are thus not applicable in cases where data integrated from a large number of sources exhibits enormous variance in vocabularies used. In such environments, the authority of data sources is an important signal that the ranking algorithm has to take into account. This paper presents algorithms for prioritising data returned by queries over web datasets expressed in RDF. We introduce the notion of naming authority which provides a correspondence between identifiers and the sources which can speak authoritatively for these identifiers. Our algorithm uses the original PageRank method to assign authority values to data sources based on a naming authority graph, and then propagates the authority values to identifiers referenced in the sources. We conduct performance and quality evaluations of the method on a large web dataset. Our method is schema-independent, requires no manual input, and has applications in search, query processing, reasoning, and user interfaces over integrated datasets.</p></td></tr><tr><td>531</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_19">Executing SPARQL Queries over the Web of Linked Data</a></td></tr><tr><td colspan=3><p>The Web of Linked Data forms a single, globally distributed dataspace. Due to the openness of this dataspace, it is not possible to know in advance all data sources that might be relevant for query answering. This openness poses a new challenge that is not addressed by traditional research on federated query processing. In this paper we present an approach to execute SPARQL queries over the Web of Linked Data. The main idea of our approach is to discover data that might be relevant for answering a query during the query execution itself. This discovery is driven by following RDF links between data sources based on URIs in the query and in partial results. The URIs are resolved over the HTTP protocol into RDF data which is continuously added to the queried dataset. This paper describes concepts and algorithms to implement our approach using an iterator-based pipeline. We introduce a formalization of the pipelining approach and show that classical iterators may cause blocking due to the latency of HTTP requests. To avoid blocking, we propose an extension of the iterator paradigm. The evaluation of our approach shows its strengths as well as the still existing challenges.</p></td></tr><tr><td>532</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_21">Decidable Order-Sorted Logic Programming for Ontologies and Rules with Argument Restructuring</a></td></tr><tr><td colspan=3><p>This paper presents a decidable fragment for combining ontologies and rules in order-sorted logic programming. We describe order-sorted logic programming with sort, predicate, and meta-predicate hierarchies for deriving predicate and meta-predicate assertions. Meta-level predicates (predicates of predicates) are useful for representing relationships between predicate formulas, and further, they conceptually yield a hierarchy similar to the hierarchies of sorts and predicates. By extending the order-sorted Horn-clause calculus, we develop a query-answering system that can answer queries such as atoms and meta-atoms generalized by containing predicate variables. We show that the expressive query-answering system computes every generalized query in single exponential time, i.e., the complexity of our query system is equal to that of DATALOG.</p></td></tr><tr><td>533</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_25">Synthesizing Semantic Web Service Compositions with jMosel and Golog</a></td></tr><tr><td colspan=3><p>In this paper we investigate different technologies to attack the automatic solution of orchestration problems based on synthesis from declarative specifications, a semantically enriched description of the services, and a collection of services available on a testbed. In addition to our previously presented tableaux-based synthesis technology, we consider two structurally rather different approaches here: using </p></td></tr><tr><td>534</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_17">Context and Domain Knowledge Enhanced Entity Spotting in Informal Text</a></td></tr><tr><td colspan=3><p>This paper explores the application of restricted relationship graphs (RDF) and statistical NLP techniques to improve named entity annotation in challenging Informal English domains. We validate our approach using on-line forums discussing popular music. Named entity annotation is particularly difficult in this domain because it is characterized by a large number of ambiguous entities, such as the Madonna album “Music” or Lilly Allen’s pop hit “Smile”.</p></td></tr><tr><td>535</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_20">Dynamic Querying of Mass-Storage RDF Data with Rule-Based Entailment Regimes</a></td></tr><tr><td colspan=3><p>RDF Schema (RDFS) as a lightweight ontology language is gaining popularity and, consequently, tools for scalable RDFS inference and querying are needed. SPARQL has become recently a W3C standard for querying RDF data, but it mostly provides means for querying simple RDF graphs only, whereas querying with respect to RDFS or other entailment regimes is left outside the current specification. In this paper, we show that SPARQL faces certain unwanted ramifications when querying ontologies in conjunction with RDF datasets that comprise multiple named graphs, and we provide an extension for SPARQL that remedies these effects. Moreover, since RDFS inference has a close relationship with logic rules, we generalize our approach to select a custom ruleset for specifying inferences to be taken into account in a SPARQL query. We show that our extensions are technically feasible by providing benchmark results for RDFS querying in our prototype system GiaBATA, which uses Datalog coupled with a persistent Relational Database as a back-end for implementing SPARQL with dynamic rule-based inference. By employing different optimization techniques like magic set rewriting our system remains competitive with state-of-the-art RDFS querying systems.</p></td></tr><tr><td>536</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_16">Modeling and Query Patterns for Process Retrieval in OWL</a></td></tr><tr><td colspan=3><p>Process modeling is a core task in software engineering in general and in web service modeling in particular. The explicit management of process models for purposes such as process selection and/or process reuse requires flexible and intelligent retrieval of process structures based on process entities and relationships, i.e. process activities, hierarchical relationship between activities and their parts, temporal relationships between activities, conditions on process flows as well as the modeling of domain knowledge. In this paper, we analyze requirements for modeling and querying of process models and present a pattern-oriented approach exploiting OWL-DL representation and reasoning capabilities for expressive process modeling and retrieval.</p></td></tr><tr><td>537</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_15">What Four Million Mappings Can Tell You about Two Hundred Ontologies</a></td></tr><tr><td colspan=3><p>The field of biomedicine has embraced the Semantic Web probably more than any other field. As a result, there is a large number of biomedical ontologies covering overlapping areas of the field. We have developed BioPortal—an open community-based repository of biomedical ontologies. We analyzed ontologies and terminologies in BioPortal and the Unified Medical Language System (UMLS), creating more than 4 million mappings between concepts in these ontologies and terminologies based on the lexical similarity of concept names and synonyms. We then analyzed the mappings and what they tell us about the ontologies themselves, the structure of the ontology repository, and the ways in which the mappings can help in the process of ontology design and evaluation. For example, we can use the mappings to guide users who are new to a field to the most pertinent ontologies in that field, to identify areas of the domain that are not covered sufficiently by the ontologies in the repository, and to identify which ontologies will serve well as background knowledge in domain-specific tools. While we used a specific (but large) ontology repository for the study, we believe that the lessons we learned about the value of a large-scale set of mappings to ontology users and developers are general and apply in many other domains.</p></td></tr><tr><td>538</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_14">TripleRank: Ranking Semantic Web Data by Tensor Decomposition</a></td></tr><tr><td colspan=3><p>The Semantic Web fosters novel applications targeting a more efficient and satisfying exploitation of the data available on the web, e.g. faceted browsing of linked open data. Large amounts and high diversity of knowledge in the Semantic Web pose the challenging question of appropriate relevance ranking for producing fine-grained and rich descriptions of the available data, e.g. to guide the user along most promising knowledge aspects. Existing methods for graph-based authority ranking lack support for fine-grained latent coherence between resources and predicates (i.e. support for link semantics in the linked data model). In this paper, we present TripleRank, a novel approach for </p></td></tr><tr><td>539</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_12">Analysis of a Real Online Social Network Using Semantic Web Frameworks</a></td></tr><tr><td colspan=3><p>Social Network Analysis (SNA) provides graph algorithms to characterize the structure of social networks, strategic positions in these networks, specific sub-networks and decompositions of people and activities. Online social platforms like Facebook form huge social networks, enabling people to connect, interact and share their online activities across several social applications. We extended SNA operators using semantic web frameworks to include the semantics of these graph-based representations when analyzing such social networks and to deal with the diversity of their relations and interactions. We present here the results of this approach when it was used to analyze a real social network with 60,000 users connecting, interacting and sharing content.</p></td></tr><tr><td>540</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_9">Task Oriented Evaluation of Module Extraction Techniques</a></td></tr><tr><td colspan=3><p>Ontology Modularization techniques identify coherent and often reusable regions within an ontology. The ability to identify such modules, thus potentially reducing the size or complexity of an ontology for a given task or set of concepts is increasingly important in the Semantic Web as domain ontologies increase in terms of size, complexity and expressivity. To date, many techniques have been developed, but evaluation of the results of these techniques is sketchy and somewhat ad hoc. Theoretical properties of modularization algorithms have only been studied in a small number of cases. This paper presents an empirical analysis of a number of modularization techniques, and the modules they identify over a number of diverse ontologies, by utilizing objective, task-oriented measures to evaluate the fitness of the modules for a number of statistical classification problems.</p></td></tr><tr><td>541</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_11">Goal-Directed Module Extraction for Explaining OWL DL Entailments</a></td></tr><tr><td colspan=3><p>Module extraction methods have proved to be effective in improving the performance of some ontology reasoning tasks, including finding justifications to explain why an entailment holds in an OWL DL ontology. However, the existing module extraction methods that compute a </p></td></tr><tr><td>542</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_5">OntoCase-Automatic Ontology Enrichment Based on Ontology Design Patterns</a></td></tr><tr><td colspan=3><p>OntoCase is a framework for semi-automatic pattern-based ontology construction. In this paper we focus on the retain and reuse phases, where an initial ontology is enriched based on content ontology design patterns (Content ODPs), and especially the implementation and evaluation of these phases. Applying Content ODPs within semi-automatic ontology construction, i.e. ontology learning (OL), is a novel approach. The main contributions of this paper are the methods for pattern ranking, selection, and integration, and the subsequent evaluation showing the characteristics of ontologies constructed automatically based on ODPs. We show that it is possible to improve the results of existing OL methods by selecting and reusing Content ODPs. OntoCase is able to introduce a general top structure into the ontologies, and by exploiting background knowledge the ontology is given a richer overall structure.</p></td></tr><tr><td>543</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_7">DOGMA: A Disk-Oriented Graph Matching Algorithm for RDF Databases</a></td></tr><tr><td colspan=3><p>RDF is an increasingly important paradigm for the representation of information on the Web. As RDF databases increase in size to approach tens of millions of triples, and as sophisticated graph matching queries expressible in languages like SPARQL become increasingly important, scalability becomes an issue. To date, there is no graph-based indexing method for RDF data where the index was designed in a way that makes it disk-resident. There is therefore a growing need for indexes that can operate efficiently when the index itself resides on disk. In this paper, we first propose the </p></td></tr><tr><td>544</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_4">A Generic Approach for Large-Scale Ontological Reasoning in the Presence of Access Restrictions to the Ontology’s Axioms</a></td></tr><tr><td colspan=3><p>The framework developed in this paper can deal with scenarios where selected sub-ontologies of a large ontology are offered as views to users, based on criteria like the user’s access right, the trust level required by the application, or the level of detail requested by the user. Instead of materializing a large number of different sub-ontologies, we propose to keep just one ontology, but equip each axiom with a label from an appropriate labeling lattice. The access right, required trust level, etc. is then also represented by a label (called user label) from this lattice, and the corresponding sub-ontology is determined by comparing this label with the axiom labels. For large-scale ontologies, certain consequence (like the concept hierarchy) are often precomputed. Instead of precomputing these consequences for every possible sub-ontology, our approach computes just one label for each consequence such that a comparison of the user label with the consequence label determines whether the consequence follows from the corresponding sub-ontology or not.</p></td></tr><tr><td>545</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_6">Graph-Based Ontology Construction from Heterogenous Evidences</a></td></tr><tr><td colspan=3><p>Ontologies are tools for describing and structuring knowledge, with many applications in searching and analyzing complex knowledge bases. Since building them manually is a costly process, there are various approaches for bootstrapping ontologies automatically through the analysis of appropriate documents. Such an analysis needs to find the concepts and the relationships that should form the ontology. However, since relationship extraction methods are imprecise and cannot homogeneously cover all concepts, the initial set of relationships is usually inconsistent and rather imbalanced - a problem which, to the best of our knowledge, was mostly ignored so far. In this paper, we define the problem of extracting a consistent as well as properly structured ontology from a set of inconsistent and heterogeneous relationships. Moreover, we propose and compare three graph-based methods for solving the ontology extraction problem. We extract relationships from a large-scale data set of more than 325K documents and evaluate our methods against a gold standard ontology comprising more than 12K relationships. Our study shows that an algorithm based on a modified formulation of the dominating set problem outperforms greedy methods.</p></td></tr><tr><td>546</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_10">A Decomposition-Based Approach to Optimizing Conjunctive Query Answering in OWL DL</a></td></tr><tr><td colspan=3><p>Scalable query answering over Description Logic (DL) based ontologies plays an important role for the success of the Semantic Web. Towards tackling the scalability problem, we propose a decomposition-based approach to optimizing existing OWL DL reasoners in evaluating conjunctive queries in OWL DL ontologies. The main idea is to decompose a given OWL DL ontology into a set of target ontologies without duplicated ABox axioms so that the evaluation of a given conjunctive query can be separately performed in every target ontology by applying existing OWL DL reasoners. This approach guarantees sound and complete results for the category of conjunctive queries that the applied OWL DL reasoner correctly evaluates. Experimental results on large benchmark ontologies and benchmark queries show that the proposed approach can significantly improve scalability and efficiency in evaluating general conjunctive queries.</p></td></tr><tr><td>547</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_3">Exploiting User Feedback to Improve Semantic Web Service Discovery</a></td></tr><tr><td colspan=3><p>State-of-the-art discovery of Semantic Web services is based on hybrid algorithms that combine semantic and syntactic matchmaking. These approaches are purely based on similarity measures between parameters of a service request and available service descriptions, which, however, fail to completely capture the actual functionality of the service or the quality of the results returned by it. On the other hand, with the advent of Web 2.0, active user participation and collaboration has become an increasingly popular trend. Users often rate or group relevant items, thus providing valuable information that can be taken into account to further improve the accuracy of search results. In this paper, we tackle this issue, by proposing a method that combines multiple matching criteria with user feedback to further improve the results of the matchmaker. We extend a previously proposed dominance-based approach for service discovery, and describe how user feedback is incorporated in the matchmaking process. We evaluate the performance of our approach using a publicly available collection of OWL-S services.</p></td></tr><tr><td>548</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_2">Automatically Constructing Semantic Web Services from Online Sources</a></td></tr><tr><td colspan=3><p>The work on integrating sources and services in the Semantic Web assumes that the data is either already represented in RDF or OWL or is available through a Semantic Web Service. In practice, there is a tremendous amount of data on the Web that is not available through the Semantic Web. In this paper we present an approach to automatically discover and create new Semantic Web Services. The idea behind this approach is to start with a set of known sources and the corresponding semantic descriptions and then discover similar sources, extract the source data, build semantic descriptions of the sources, and then turn them into Semantic Web Services. We implemented an end-to-end solution to this problem in a system called </p></td></tr><tr><td>549</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_48">An API for Ontology Alignment</a></td></tr><tr><td colspan=3><p>Ontologies are seen as the solution to data heterogeneity on the web. However, the available ontologies are themselves source of heterogeneity. This can be overcome by aligning ontologies, or finding the correspondence between their components. These alignments deserve to be treated as objects: they can be referenced on the web as such, be completed by an algorithm that improves a particular alignment, be compared with other alignments and be transformed into a set of axioms or a translation program. We present here a format for expressing alignments in RDF, so that they can be published on the web. Then we propose an implementation of this format as an Alignment API, which can be seen as an extension of the OWL API and shares some design goals with it. We show how this API can be used for effectively aligning ontologies and completing partial alignments, thresholding alignments or generating axioms and transformations.</p></td></tr><tr><td>550</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_46">Patching Syntax in OWL Ontologies</a></td></tr><tr><td colspan=3><p>An analysis of OWL ontologies represented in RDF/XML on the Web shows that a majority are OWL Full. In many cases this may not be through a desire to use the expressivity provided by OWL Full, but is rather due to syntactic errors or accidental misuse of the vocabulary. We present a “rogues gallery” of common errors encountered, and describe how robust parsers that attempt to cope with such errors can be produced.</p></td></tr><tr><td>551</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_47">QOM – Quick Ontology Mapping</a></td></tr><tr><td colspan=3><p>(Semi-)automatic mapping – also called (semi-)automatic alignment – of ontologies is a core task to achieve interoperability when two agents or services use different ontologies. In the existing literature, the focus has so far been on improving the quality of mapping results. We here consider QOM, Quick Ontology Mapping, as a way to trade off between effectiveness (i.e. quality) and efficiency of the mapping generation algorithms. We show that QOM has lower run-time complexity than existing prominent approaches. Then, we show in experiments that this theoretical investigation translates into practical benefits. While QOM gives up some of the possibilities for producing high-quality results in favor of efficiency, our experiments show that this loss of quality is marginal.</p></td></tr><tr><td>552</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_8">Semantically-Aided Business Process Modeling</a></td></tr><tr><td colspan=3><p>Enriching business process models with semantic annotations taken from an ontology has become a crucial necessity both in service provisioning, integration and composition, and in business processes management. In our work we represent semantically annotated business processes as part of an OWL knowledge base that formalises the business process structure, the business domain, and a set of criteria describing correct semantic annotations. In this paper we show how Semantic Web representation and reasoning techniques can be effectively applied to formalise, and automatically verify, sets of constraints on Business Process Diagrams that involve both knowledge about the domain and the process structure. We also present a tool for the automated transformation of an annotated Business Process Diagram into an OWL ontology. The use of the semantic web techniques and tool presented in the paper results in a novel support for the management of business processes in the phase of process modeling, whose feasibility and usefulness will be illustrated by means of a concrete example.</p></td></tr><tr><td>553</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_44">Opening Up Magpie via Semantic Services</a></td></tr><tr><td colspan=3><p>Magpie is a suite of tools supporting a ‘zero-cost’ approach to semantic web browsing: it avoids the need for manual annotation by automatically associating an ontology-based semantic layer to web resources. An important aspect of Magpie, which differentiates it from superficially similar hypermedia systems, is that the association between items on a web page and semantic concepts is not merely a mechanism for dynamic linking, but it is the enabling condition for locating services and making them available to a user. These services can be manually activated by a user (pull services), or opportunistically triggered when the appropriate web entities are encountered during a browsing session (push services). In this paper we analyze Magpie from the perspective of building semantic web applications and we note that earlier implementations did not fulfill the criterion of “open as to services”, which is a key aspect of the emerging semantic web. For this reason, in the past twelve months we have carried out a radical redesign of Magpie, resulting in a novel architecture, which is open both with respect to ontologies and semantic web services. This new architecture goes beyond the idea of merely providing support for semantic web browsing and can be seen as a software framework for designing and implementing semantic web applications.</p></td></tr><tr><td>554</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_45">Towards a Symptom Ontology for Semantic Web Applications</a></td></tr><tr><td colspan=3><p>As the use of Semantic Web ontologies continues to expand there is a growing need for tools that can validate ontological consistency and provide guidance in the correction of detected defects and errors. A number of tools already exist as evidenced by the ten systems participating in the W3C’s evaluation of the OWL Test Cases. For the most part, these first generation tools focus on experimental approaches to consistency checking, while minimal attention is paid to how the results will be used or how the systems might interoperate. For this reason very few of these systems produce results in a machine-readable format (for example as OWL annotations) and there is no shared notion across the tools of how to identify and describe what it is that makes a specific ontology or annotation inconsistent. In this paper we propose the development of a Symptom Ontology for the Semantic Web that would serve as a common language for identifying and describing semantic errors and warnings that may be indicative of inconsistencies in ontologies and annotations; we refer to such errors and warnings as </p></td></tr><tr><td>555</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_43">Working with Multiple Ontologies on the Semantic Web</a></td></tr><tr><td colspan=3><p>The standardization of the second generation Web Ontology Language, OWL, leaves a crucial issue for Web-based ontologies unsatisfactorily resolved: how to represent and reason with multiple distinct, but linked, ontologies. OWL provides the owl:imports construct which, roughly, allows Web ontologies to include other Web ontologies, but only by merging all the linked ontologies into a single logical “space.” Recent work on multidimensional logics, fusions and other combinations of modal logics, distributed and contextual logics, and the like have tried to find formalisms wherein knowledge bases (and their logic) are kept more distinct but yet affect each other. These formalisms have various degrees of robustness in their computational complexity, their modularity, their expressivity, and their intuitiveness to modelers. In this paper, we explore a family of such formalisms, grounded in </p></td></tr><tr><td>556</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_41">Metadata-Driven Personal Knowledge Publishing</a></td></tr><tr><td colspan=3><p>We propose a personal knowledge publishing system called </p></td></tr><tr><td>557</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_42">An Extensible Directory Enabling Efficient Semantic Web Service Integration</a></td></tr><tr><td colspan=3><p>In an open environment populated by large numbers of heterogeneous information services, integration is a major challenge. In such a setting, the efficient coupling between directory-based service discovery and service composition engines is crucial. In this paper we present a directory service that offers specific functionality in order to enable efficient service integration. The directory implementation relies on a compact numerical encoding of service parameters and on a multidimensional index structure. It supports isolated service integration sessions providing a consistent view of the directory data. During a session a client may issue multiple queries to the directory and retrieve the results incrementally. In order to optimize the interaction of the directory with different service composition algorithms, the directory supports custom ranking functions that are dynamically installed with the aid of mobile code. The ranking functions are written in Java, but the directory service imposes severe restrictions on the programming model in order to protect itself against malicious or erroneous code (e.g., denial-of-service attacks). With the aid of user-defined ranking functions, application-specific ordering heuristics can be deployed directly. Experiments on randomly generated problems show that they significantly reduce the number of query results that have to be transmitted to the client by up to </p></td></tr><tr><td>558</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_13">Coloring RDF Triples to Capture Provenance</a></td></tr><tr><td colspan=3><p>Recently, the W3C Linking Open Data effort has boosted the publication and inter-linkage of large amounts of RDF datasets on the Semantic Web. Various ontologies and knowledge bases with millions of RDF triples from Wikipedia and other sources, mostly in e-science, have been created and are publicly available. Recording provenance information of RDF triples aggregated from different heterogeneous sources is crucial in order to effectively support trust mechanisms, digital rights and privacy policies. Managing provenance becomes even more important when we consider not only explicitly stated but also implicit triples (through RDFS inference rules) in conjunction with declarative languages for querying and updating RDF graphs. In this paper we rely on </p></td></tr><tr><td>559</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_40">On the Emergent Semantic Web and Overlooked Issues</a></td></tr><tr><td colspan=3><p>The emergent Semantic Web, despite being in its infancy, has already received a lot of attention from academia and industry. This resulted in an abundance of prototype systems and discussion most of which are centred around the underlying infrastructure. However, when we critically review the work done to date we realise that there is little discussion with respect to the vision of the Semantic Web. In particular, there is an observed dearth of discussion on how to deliver knowledge sharing in an environment such as the Semantic Web in effective and efficient manners. There are a lot of overlooked issues, associated with agents and trust to hidden assumptions made with respect to knowledge representation and robust reasoning in a distributed environment. These issues could potentially hinder further development if not considered at the early stages of designing Semantic Web systems. In this perspectives’ paper, we aim to help engineers and practitioners of the Semantic Web by raising awareness of these issues.</p></td></tr><tr><td>560</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_39">A Semantic Web Resource Protocol: XPointer and HTTP</a></td></tr><tr><td colspan=3><p>Semantic Web resources — that is, knowledge representation formalisms existing in a distributed hypermedia system — require different addressing and processing models and capacities than the typical kinds of World Wide Web resources. We describe an approach to building a Semantic Web resource protocol — a scalable, extensible logical addressing scheme and transport protocol — by using and extending existing specifications and technologies. We introduce XPointer and some infrequently used, but useful features of HTTP/1.1, in order to support addressing and server side processing of resource and subresource operations. We consider applications of the XPointer Framework for use in the Semantic Web, particularly for RDF and OWL resources and subresources. We describe two initial implementations: filtering of RSS resources by date and item range; RDF subresource selection using RDQL. Finally, we describe possible application to the problem of OWL imports.</p></td></tr><tr><td>561</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_38">Query Answering for OWL-DL with Rules</a></td></tr><tr><td colspan=3><p>Both OWL-DL and function-free Horn rules are decidable logics with interesting, yet orthogonal expressive power: from the rules perspective, OWL-DL is restricted to tree-like rules, but provides both existentially and universally quantified variables and full, monotonic negation. From the description logic perspective, rules are restricted to universal quantification, but allow for the interaction of variables in arbitrary ways. Clearly, a combination of OWL-DL and rules is desirable for building Semantic Web ontologies, and several such combinations have already been discussed. However, such a combination might easily lead to the undecidability of interesting reasoning problems. Here, we present a </p></td></tr><tr><td>562</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_36">Information Retrieval Support for Ontology Construction and Use</a></td></tr><tr><td colspan=3><p>Information retrieval can contribute towards the construction of ontologies and the effective usage of ontologies. We use collocation-based keyword extraction to suggest new concepts, and study the generation of hyperlinks to automate the population of ontologies with instances. We evaluate our methods within the setting of digital library project, using information retrieval evaluation methodology. Within the same setting we study retrieval methods that complement the navigational support offered by the semantic relations in most ontologies to help users explore the ontology.</p></td></tr><tr><td>563</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_37">Rules-By-Example – A Novel Approach to Semantic Indexing and Querying of Images</a></td></tr><tr><td colspan=3><p>Images represent a key source of information in many domains and the ability to exploit them through their discovery, analysis and integration by services and agents on the Semantic Web is a challenging and significant problem. To date the semantic indexing of images has concentrated on applying machine-learning techniques to a set of manually-annotated images in order to automatically label images with keywords. In this paper we propose a new hybrid, user-assisted approach, Rules-By-Example (RBE), which is based on a combination of RuleML and Query-By-Example. Our RBE user interface enables domain-experts to graphically define domain-specific rules that can infer high-level semantic descriptions of images from combinations of low-level visual features (e.g., color, texture, shape, size of regions) which have been specified through examples. Using these rules, the system is able to analyze the visual features of any given image from this domain and generate semantically meaningful labels, using terms defined in the domain-specific ontology. We believe that this approach, in combination with traditional solutions, will enable faster, more flexible, cost-effective and accurate semantic indexing of images and hence maximize their potential for discovery, re-use, integration and processing by Semantic Web services, tools and agents.</p></td></tr><tr><td>564</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_35">A Comparison of RDF Query Languages</a></td></tr><tr><td colspan=3><p>The purpose of this paper is to provide a rigorous comparison of six query languages for RDF. We outline and categorize features that any RDF query language should provide and compare the individual languages along these features. We describe several practical usage examples for RDF queries and conclude with a comparison of the expressiveness of the particular query languages. The use cases, sample data and queries for the respective languages are available on the web [6].</p></td></tr><tr><td>565</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_34">Generating On the Fly Queries for the Semantic Web: The ICS-FORTH Graphical RQL Interface (GRQL)</a></td></tr><tr><td colspan=3><p>Building user-friendly GUIs for browsing and filtering RDF/S description bases while exploiting in a transparent way the expressiveness of declarative query/view languages is vital for various Semantic Web applications (e.g., e-learning, e-science). In this paper we present a novel interface, called GRQL, which relies on the full power of the RDF/S data model for constructing on the fly queries expressed in RQL. More precisely, a user can navigate graphically through the individual RDF/S class and property definitions and generate transparently the RQL path expressions required to access the resources of interest. These expressions capture accurately the meaning of its navigation steps through the class (or property) subsumption and/or associations. Additionally, users can enrich the generated queries with filtering conditions on the attributes of the currently visited class while they can easily specify the resource’s class(es) appearing in the query result. To the best of our knowledge, GRQL is the first application-independent GUI able to generate a unique RQL query which captures the cumulative effect of an entire user navigation session.</p></td></tr><tr><td>566</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_32">Inferring Data Transformation Rules to Integrate Semantic Web Services</a></td></tr><tr><td colspan=3><p>OWL-S allows selecting, composing and invoking Web Services at different levels of abstraction: selection uses high level abstract descriptions, invocation uses low level grounding ones, while composition needs to consider both high and low level descriptions. In our setting, two Web Services are to be composed so that output from the upstream one is used to create input for the downstream one. These Web Services may have different data models but are related to each other through high and low level descriptions. Correspondences must be found between components of the upstream data type and the downstream ones. Low level data transformation functions may be required (e.g. unit conversions, data type conversions). The components may be arranged in different XML tree structures. Thus, multiple data transformations are necessary: reshaping the message tree, matching leaves by corresponding types, translating through ontologies, and calling conversion functions. Our prototype compiles these transformations into a set of data transformation rules, using our tableau-based </p></td></tr><tr><td>567</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_33">Using Vampire to Reason with OWL</a></td></tr><tr><td colspan=3><p>OWL DL corresponds to a Description Logic (DL) that is a fragment of classical first-order predicate logic (FOL). Therefore, the standard methods of automated reasoning for full FOL can potentially be used instead of dedicated DL reasoners to solve OWL DL reasoning tasks. In this paper we report on some experiments designed to explore the feasibility of using existing general-purpose FOL provers to reason with OWL DL. We also extend our approach to SWRL, a proposed rule language extension to OWL.</p></td></tr><tr><td>568</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_31">Knowledge-Intensive Induction of Terminologies from Metadata</a></td></tr><tr><td colspan=3><p>We focus on the induction and revision of terminologies from metadata. Following a Machine Learning approach, this setting can be cast as a search problem to be solved employing operators that traverse the search space expressed in a structural representation, aiming at correct concept definitions. The progressive refinement of such definitions in a terminology is driven by the available extensional knowledge (metadata). A knowledge-intensive inductive approach to this task is presented, that can deal with on the expressive Semantic Web representations based on Description Logics, which are endowed with well-founded reasoning capabilities. The core inferential mechanism, based on multilevel counterfactuals, can be used for either inducing new concept descriptions or refining existing (incorrect) ones. The soundness of the approach and its applicability are also proved and discussed.</p></td></tr><tr><td>569</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_49">OntoNaviERP: Ontology-Supported Navigation in ERP Software Documentation</a></td></tr><tr><td colspan=3><p>The documentation of Enterprise Research Planning (ERP) systems is usually (1) extremely large and (2) combines various views from the business and the technical implementation perspective. Also, a very specific vocabulary has evolved, in particular in the SAP domain (e.g. SAP Solution Maps or SAP software module names). This vocabulary is not clearly mapped to business management terminology and concepts. It is a well-known problem in practice that searching in SAP ERP documentation is difficult, because it requires in-depth knowledge of a large and proprietary terminology. We propose to use ontologies and automatic annotation of such large HTML software documentation in order to improve the usability and accessibility, namely of ERP help files. In order to achieve that, we have developed an ontology and prototype for SAP ERP 6.0. Our approach integrates concepts and lexical resources from (1) business management terminology, (2) SAP business terminology, (3) SAP system terminology, and (4) Wordnet synsets. We use standard GATE/KIM technology to annotate SAP help documentation with respective references to our ontology. Eventually, our approach consolidates the knowledge contained in the SAP help functionality at a conceptual level. This allows users to express their queries using a terminology they are familiar with, e.g. referring to general management terms. Despite a widely automated ontology construction process and a simplistic annotation strategy with minimal human intervention, we experienced convincing results. For an average query linked to an action and a topic, our technology returns more than 3 relevant resources, while a naïve term-based search returns on average only about 0.2 relevant resources.</p></td></tr><tr><td>570</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_30">Applying KAoS Services to Ensure Policy Compliance for Semantic Web Services Workflow Composition and Enactment</a></td></tr><tr><td colspan=3><p>In this paper we describe our experience in applying KAoS services to ensure policy compliance for Semantic Web Services workflow composition and enactment. We are developing these capabilities within the context of two applications: Coalition Search and Rescue (CoSAR-TS) and Semantic Firewall (SFW). We describe how this work has uncovered requirements for increasing the expressivity of policy beyond what can be done with description logic (e.g., role-value-maps), and how we are extending our representation and reasoning mechanisms in a carefully controlled manner to that end. Since KAoS employs OWL for policy representation, it fits naturally with the use of OWL-S workflow descriptions generated by the AIAI I-X planning system in the CoSAR-TS application. The advanced reasoning mechanisms of KAoS are based on the JTP inference engine and enable the analysis of classes and instances of processes from a policy perspective. As the result of analysis, KAoS concludes whether a particular workflow step is allowed by policy and whether the performance of this step would incur additional policy-generated obligations. Issues in the representation of processes within OWL-S are described. Besides what is done during workflow composition, aspects of policy compliance can be checked at runtime when a workflow is enacted. We illustrate these capabilities through two application examples. Finally, we outline plans for future work.</p></td></tr><tr><td>571</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_27">Automated Composition of Semantic Web Services into Executable Processes</a></td></tr><tr><td colspan=3><p>Different planning techniques have been applied to the problem of automated composition of web services. However, in realistic cases, this planning problem is far from trivial: the planner needs to deal with the nondeterministic behavior of web services, the partial observability of their internal status, and with complex goals expressing temporal conditions and preference requirements. We propose a planning technique for the automated composition of web services described in OWL-S process models, which can deal effectively with nondeterminism, partial observability, and complex goals. The technique allows for the synthesis of plans that encode compositions of web services with the usual programming constructs, like conditionals and iterations. The generated plans can thus be translated into executable processes, e.g., BPEL4WS programs. We implement our solution in a planner and do some preliminary experimental evaluations that show the potentialities of our approach, and the gain in performance of automating the composition at the semantic level w.r.t. the automated composition at the level of executable processes.</p></td></tr><tr><td>572</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_25">Applying Semantic Web Services to Bioinformatics: Experiences Gained, Lessons Learnt</a></td></tr><tr><td colspan=3><p>We have seen an increasing amount of interest in the application of Semantic Web technologies to Web services. The aim is to support automated discovery and composition of the services allowing seamless and transparent interoperability. In this paper we discuss three projects that are applying such technologies to bioinformatics: </p></td></tr><tr><td>573</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_26">Automating Scientific Experiments on the Semantic Grid</a></td></tr><tr><td colspan=3><p>We present a framework to facilitate automated synthesis of scientific experiment workflows in Semantic Grids based on high-level goal specification. Our framework has two main features which distinguish it from other work in this area. First, we propose a dynamic and adaptive mechanism for automating the construction of experiment workflows. Second, we distinguish between different levels of abstraction of loosely coupled experiment workflows to facilitate reuse and sharing of experiments. We illustrate our framework using a real world application scenario in the physics domain involving the detection of gravitational waves from astrophysical sources.</p></td></tr><tr><td>574</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_23">ASSAM: A Tool for Semi-automatically Annotating Semantic Web Services</a></td></tr><tr><td colspan=3><p>The semantic Web Services vision requires that each service be annotated with semantic metadata. Manually creating such metadata is tedious and error-prone, and many software engineers, accustomed to tools that automatically generate WSDL, might not want to invest the additional effort. We therefore propose ASSAM, a tool that assists a user in creating semantic metadata for Web Services. ASSAM is intended for service consumers who want to integrate a number of services and therefore must annotate them according to some shared ontology. ASSAM is also relevant for service producers who have deployed a Web Service and want to make it compatible with an existing ontology. ASSAM’s capabilities to automatically create semantic metadata are supported by two machine learning algorithms. First, we have developed an iterative relational classification algorithm for semantically classifying Web Services, their operations, and input and output messages. Second, to aggregate the data returned by multiple semantically related Web Services, we have developed a schema mapping algorithm that is based on an ensemble of string distance metrics.</p></td></tr><tr><td>575</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_24">Information Gathering During Planning for Web Service Composition</a></td></tr><tr><td colspan=3><p>Hierarchical Task-Network (HTN) based planning techniques have been applied to the problem of composing Web Services, especially when described using the </p></td></tr><tr><td>576</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_22">Semantic Web Service Interaction Protocols: An Ontological Approach</a></td></tr><tr><td colspan=3><p>A central requirement for achieving the vision of run-time discovery and dynamic composition of services is the provision of appropriate descriptions of the operation of a service, that is, how the service interacts with agents or other services. In this paper, we use experience gained through the development of real-life Grid applications to produce a set of requirements for such descriptions and then attempt to match those requirements against the offerings of existing work, such as OWL-S [1] and IRS-II [2]. Based on this analysis we identify which requirements are not addressed by current research and, in response, produce a model for describing the </p></td></tr><tr><td>577</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_29">From Software APIs to Web Service Ontologies: A Semi-automatic Extraction Method</a></td></tr><tr><td colspan=3><p>Successful employment of semantic web services depends on the availability of high quality ontologies to describe the domains of these services. As always, building such ontologies is difficult and costly, thus hampering web service deployment. Our hypothesis is that since the functionality offered by a web service is reflected by the underlying software, domain ontologies could be built by analyzing the documentation of that software. We verify this hypothesis in the domain of RDF ontology storage tools. We implemented and fine-tuned a semi-automatic method to extract domain ontologies from software documentation. The quality of the extracted ontologies was verified against a high quality hand-built ontology of the same domain. Despite the low linguistic quality of the corpus, our method allows extracting a considerable amount of information for a domain ontology.</p></td></tr><tr><td>578</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-04930-9_1">Queries to Hybrid MKNF Knowledge Bases through Oracular Tabling</a></td></tr><tr><td colspan=3><p>An important issue for the Semantic Web is how to combine open-world ontology languages with closed-world (non-monotonic) rule paradigms. Several proposals for hybrid languages allow concepts to be simultaneously defined by an ontology and rules, where rules may refer to concepts in the ontology and the ontology may also refer to predicates defined by the rules. Hybrid MKNF knowledge bases are one such proposal, for which both a stable and a well-founded semantics have been defined. The definition of Hybrid MKNF knowledge bases is parametric on the ontology language, in the sense that non-monotonic rules can extend any decidable ontology language. In this paper we define a query-driven procedure for Hybrid MKNF knowledge bases that is sound with respect to the original stable model-based semantics, and is correct with respect to the well-founded semantics. This procedure is able to answer conjunctive queries, and is parametric on an inference engine for reasoning in the on- tology language. Our procedure is based on an extension of a tabled rule evaluation to capture reasoning within an ontology by modeling it as an interaction with an external oracle and, with some assumptions on the complexity of the oracle compared to the complexity of the ontology language, maintains the data complexity of the well-founded semantics for hybrid MKNF knowledge bases.</p></td></tr><tr><td>579</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_21">Structure-Based Partitioning of Large Concept Hierarchies</a></td></tr><tr><td colspan=3><p>The increasing awareness of the benefits of ontologies for information processing has lead to the creation of a number of large ontologies about real-world domains. The size of these ontologies and their monolithic character cause serious problems in handling them. In other areas, e.g. software engineering, these problems are tackled by partitioning monolithic entities into sets of meaningful and mostly self-contained modules. In this paper, we suggest a similar approach for ontologies. We propose a method for automatically partitioning large ontologies into smaller modules based on the structure of the class hierarchy. We show that the structure-based method performs surprisingly well on real-world ontologies. We support this claim by experiments carried out on real-world ontologies including SUMO and the NCI cancer ontology. The results of these experiments are available online at </p></td></tr><tr><td>580</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_20">An Evaluation of Knowledge Base Systems for Large OWL Datasets</a></td></tr><tr><td colspan=3><p>In this paper, we present an evaluation of four knowledge base systems (KBS) with respect to use in large OWL applications. To our knowledge, no experiment has been done with the scale of data used here. The smallest dataset used consists of 15 OWL files totaling 8MB, while the largest dataset consists of 999 files totaling 583MB. We evaluated two memory-based systems (OWLJessKB and memory-based Sesame) and two systems with persistent storage (database-based Sesame and DLDB-OWL). We describe how we have performed the evaluation and what factors we have considered in it. We show the results of the experiment and discuss the performance of each system. In particular, we have concluded that existing systems need to place a greater emphasis on scalability.</p></td></tr><tr><td>581</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_18">: Combining Browsing and Editing with Reasoning and Explaining for OWL Lite Ontologies</a></td></tr><tr><td colspan=3><p> </p></td></tr><tr><td>582</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_19">Tracking Changes During Ontology Evolution</a></td></tr><tr><td colspan=3><p>As ontology development becomes a collaborative process, developers face the problem of maintaining versions of ontologies akin to maintaining versions of software code or versions of documents in large projects. Traditional versioning systems enable users to compare versions, examine changes, and accept or reject changes. However, while versioning systems usually treat software code and text documents as text files, a versioning system for ontologies must compare and present </p></td></tr><tr><td>583</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_16">What Would It Mean to Blog on the Semantic Web?</a></td></tr><tr><td colspan=3><p>The phenomenon known as Web logging (“blogging”) has helped realize an initial goal of the Web: to turn Web content consumers (i.e., end users) into Web content producers. As the Semantic Web unfolds, we feel there are two questions worth posing: (1) do blog entries have semantic structure that can be usefully captured and exploited? (2) is blogging a natural way to encourage growth of the Semantic Web? We explore empirical evidence for answering these questions in the affirmative and propose means to bring blogging into the mainstream of the Semantic Web, including ontologies that extend the RSS 1.0 specification and an XSL transform for handling RSS 0.9x/2.0 files. To demonstrate the validity of our approach we have constructed a semantic blogging environment based on Haystack. We argue that with tools such as Haystack, semantic blogging will be an important paradigm by which metadata authoring will occur in the future.</p></td></tr><tr><td>584</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_15">Visual Modeling of OWL DL Ontologies Using UML</a></td></tr><tr><td colspan=3><p>This paper introduces a visual, UML-based notation for OWL ontologies. We provide a standard MOF2 compliant metamodel which captures the language primitives offered by OWL DL. Similarly, we invent a UML profile, which allows to visually model OWL ontologies in a notation that is close to the UML notation. This allows to develop ontologies using UML tools. Throughout the paper, the significant differences to some earlier proposals for a visual, UML-based notation for ontologies are discussed.</p></td></tr><tr><td>585</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_13">From Tables to Frames</a></td></tr><tr><td colspan=3><p>Turning the current Web into a Semantic Web requires automatic approaches for annotation of existing data since manual approaches will not scale in general. We here present an approach for automatic generation of F-Logic frames out of tables which subsequently supports the automatic population of ontologies from table-like structures. The approach consists of a methodology, an accompanying implementation and a thorough evaluation. It is based on a grounded cognitive table model which is stepwise instantiated by our methodology.</p></td></tr><tr><td>586</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_12">Learning Meta-descriptions of the FOAF Network</a></td></tr><tr><td colspan=3><p>We argue that in a distributed context, such as the Semantic Web, ontology engineers and data creators often cannot control (or even imagine) the possible uses their data or ontologies might have. Therefore ontologies are unlikely to identify every useful or interesting classification possible in a problem domain, for example these might be of a personalised nature and only appropriate for a certain user in a certain context, or they might be of a different granularity than the initial scope of the ontology. We argue that machine learning techniques will be essential within the Semantic Web context to allow these unspecified classifications to be identified. In this paper we explore the application of machine learning methods to FOAF, highlighting the challenges posed by the characteristics of such data. Specifically, we use clustering to identify classes of people and inductive logic programming (ILP) to learn descriptions of these groups. We argue that these descriptions constitute re-usable, first class knowledge that is neither explicitly stated nor deducible from the input data. These new descriptions can be represented as simple OWL class restrictions or more sophisticated descriptions using SWRL. These are then suitable either for incorporation into future versions of ontologies or for on-the-fly use for personalisation tasks.</p></td></tr><tr><td>587</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_14">The Specification of Agent Behavior by Ordinary People: A Case Study</a></td></tr><tr><td colspan=3><p>The development of intelligent agents is a key part of the Semantic Web vision, but how does an ordinary person tell an agent what to do? One approach to this problem is to use RDF </p></td></tr><tr><td>588</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_17">The Protégé OWL Plugin: An Open Development Environment for Semantic Web Applications</a></td></tr><tr><td colspan=3><p>We introduce the OWL Plugin, a Semantic Web extension of the Protégé ontology development platform. The OWL Plugin can be used to edit ontologies in the Web Ontology Language (OWL), to access description logic reasoners, and to acquire instances for semantic markup. In many of these features, the OWL Plugin has created and facilitated new practices for building Semantic Web contents, often driven by the needs of and feedback from our users. Furthermore, Protégé’s flexible open-source platform means that it is easy to integrate custom-tailored components to build real-world applications. This document describes the architecture of the OWL Plugin, walks through its most important features, and discusses some of our design decisions.</p></td></tr><tr><td>589</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_48">Requirements Analysis Tool: A Tool for Automatically Analyzing Software Requirements Documents</a></td></tr><tr><td colspan=3><p>We present a tool, called the </p></td></tr><tr><td>590</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_10">Bibster – A Semantics-Based Bibliographic Peer-to-Peer System</a></td></tr><tr><td colspan=3><p>This paper describes the design and implementation of Bibster, a Peer-to-Peer system for exchanging bibliographic data among researchers. Bibster exploits ontologies in data storage, query formulation, query routing and answer presentation: When bibliographic entries are made available for use in Bibster, they are structured and classified according to two different ontologies. This ontological structure is then exploited to help users formulate their queries. Subsequently, the ontologies are used to improve query routing across the Peer-to-Peer network. Finally, the ontologies are used to post-process the returned answers in order to do duplicate detection. The paper describes each of these ontology-based aspects of Bibster. Bibster is a fully implemented open source solution built on top of the JXTA platform.</p></td></tr><tr><td>591</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_47">An Architecture for Semantic Navigation and Reasoning with Patient Data - Experiences of the Health-e-Child Project</a></td></tr><tr><td colspan=3><p>Medical ontologies have become the standard means of recording and accessing conceptualized biological and medical knowledge. The expressivity of these ontologies goes from simple concept lists through taxonomies to formal logical theories. In the context of patient information, their application is primarily annotation of medical (instance) data. To exploit higher expressivity, we propose an architecture which allows for reasoning on patient data using OWL DL ontologies. The implementation is carried out as part of the Health-e-Child platform prototype. We discuss the use case where ontologies establish a hierarchical classification of patients which in turn is used to aid the visualization of patient data. We briefly discuss the treemap-based patient viewer which has been evaluated in the Health-e-Child project.</p></td></tr><tr><td>592</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_9">GridVine: Building Internet-Scale Semantic Overlay Networks</a></td></tr><tr><td colspan=3><p>This paper addresses the problem of building scalable semantic overlay networks. Our approach follows the principle of data independence by separating a logical layer, the semantic overlay for managing and mapping data and metadata schemas, from a physical layer consisting of a structured peer-to-peer overlay network for efficient routing of messages. The physical layer is used to implement various functions at the logical layer, including attribute-based search, schema management and schema mapping management. The separation of a physical from a logical layer allows us to process logical operations in the semantic overlay using different physical execution strategies. In particular we identify iterative and recursive strategies for the traversal of semantic overlay networks as two important alternatives. At the logical layer we support semantic interoperability through schema inheritance and Semantic Gossiping. Thus our system provides a complete solution to the implementation of semantic overlay networks supporting both scalability and interoperability.</p></td></tr><tr><td>593</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_46">Creating and Using Organisational Semantic Webs in Large Networked Organisations</a></td></tr><tr><td colspan=3><p>Modern knowledge management is based on the orchestration of dynamic communities that acquire and share knowledge according to customized schemas. However, while independence of ontological views is favoured, these communities must also be able to share their knowledge with the rest of the organization. In this paper we introduce K-Forms and K-Search, a suite of Semantic Web tools for supporting distributed and networked knowledge acquisition, capturing, retrieval and sharing. They enable communities of users to define their own domain views in an intuitive way (automatically translated into formal ontologies) and capture and share knowledge according to them. The tools favour reuse of existing ontologies; reuse creates as side effect a network of (partially) interconnected ontologies that form the basis for knowledge exchange among communities. The suite is under release to support knowledge capture, retrieval and sharing in a large jet engine company.</p></td></tr><tr><td>594</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_45">Deploying Semantic Web Technologies for Work Integrated Learning in Industry - A Comparison: SME vs. Large Sized Company</a></td></tr><tr><td colspan=3><p>Modern businesses operate in a rapidly changing environment. Continuous learning is an essential ingredient in order to stay competitive in such environments. The APOSDLE system utilizes semantic web technologies to create a generic system for supporting knowledge workers in different domains to learn work. Since APOSDLE relies on three interconnected semantic models to achieve this goal, the question on how to efficiently create high-quality semantic models has become one of the major research challenges. On the basis of two concrete examples-namely deployment of such a learning system at EADS, a large corporation, and deployment at ISN, a network of SMEs-we report in detail the issues a company has to face, when it wants to deploy a modern learning environment relying on semantic web technology.</p></td></tr><tr><td>595</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_44">Thesaurus-Based Search in Large Heterogeneous Collections</a></td></tr><tr><td colspan=3><p>In cultural heritage, large virtual collections are coming into existence. Such collections contain heterogeneous sets of metadata and vocabulary concepts, originating from multiple sources. In the context of the E-Culture demonstrator we have shown earlier that such virtual collections can be effectively explored with keyword search and semantic clustering. In this paper we describe the design rationale of ClioPatria, an open-source system which provides APIs for scalable semantic graph search. The use of ClioPatria’s search strategies is illustrated with a realistic use case: searching for ”Picasso”. We discuss details of scalable graph search, the required OWL reasoning functionalities and show why SPARQL queries are insufficient for solving the search problem.</p></td></tr><tr><td>596</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_11">Top-</a></td></tr><tr><td colspan=3><p>Increasing the number of peers in a peer-to-peer network usually increases the number of answers to a given query as well. While having more answers is nice in principle, users are not interested in arbitrarily large and unordered answer sets, but rather in a small set of “best” answers. Inspired by the success of ranking algorithms in Web search engine and top-</p></td></tr><tr><td>597</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_41">ELP: Tractable Rules for OWL 2</a></td></tr><tr><td colspan=3><p>We introduce </p></td></tr><tr><td>598</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_43">Semantic Relatedness Measure Using Object Properties in an Ontology</a></td></tr><tr><td colspan=3><p>This paper presents a new semantic relatedness measure on ontologies which considers especially the object properties between the concepts. Our approach relies on two hypotheses. Firstly, using only concept hierarchy and object properties, only a few paths can be considered as “semantically corrects” and these paths obey to a given set of rules. Secondly, following a given edge in a path has a cost (represented as a weight), which depends on its type (</p></td></tr><tr><td>599</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_42">Term Dependence on the Semantic Web</a></td></tr><tr><td colspan=3><p>A large amount of terms (classes and properties) have been published on the Semantic Web by various parties, to be shared for describing resources. Terms are defined based on other terms, and thus a directed dependence relation is formed. The study of term dependence is a foundation work and is important for many other tasks, such as ontology maintenance, integration, and distributed reasoning on the Web scale. In this paper, we analyze the complex network characteristics of the term dependence graph and the induced vocabulary dependence graph. The graphs analyzed in the experiments are constructed from a large data set that contains 1,278,233 terms in 3,039 vocabularies. The results characterize the current status of schemas on the Semantic Web in many aspects, including degree distributions, reachability, and connectivity.</p></td></tr><tr><td>600</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_40">Semantic Modelling of User Interests Based on Cross-Folksonomy Analysis</a></td></tr><tr><td colspan=3><p>The continued increase in Web usage, in particular participation in folksonomies, reveals a trend towards a more dynamic and interactive Web where individuals can organise and share resources. Tagging has emerged as the de-facto standard for the organisation of such resources, providing a versatile and reactive knowledge management mechanism that users find easy to use and understand. It is common nowadays for users to have multiple profiles in various folksonomies, thus distributing their tagging activities. In this paper, we present a method for the automatic consolidation of user profiles across two popular social networking sites, and subsequent semantic modelling of their interests utilising Wikipedia as a multi-domain model. We evaluate how much can be learned from such sites, and in which domains the knowledge acquired is focussed. Results show that far richer interest profiles can be generated for users when multiple tag-clouds are combined.</p></td></tr><tr><td>601</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_39">Semantic Grounding of Tag Relatedness in Social Bookmarking Systems</a></td></tr><tr><td colspan=3><p>Collaborative tagging systems have nowadays become important data sources for populating semantic web applications. For tasks like synonym detection and discovery of concept hierarchies, many researchers introduced measures of tag similarity. Even though most of these measures appear very natural, their design often seems to be rather ad hoc, and the underlying assumptions on the notion of similarity are not made explicit. A more systematic characterization and validation of tag similarity in terms of formal representations of knowledge is still lacking. Here we address this issue and analyze several measures of tag similarity: Each measure is computed on data from the social bookmarking system del.icio.us and a semantic grounding is provided by mapping pairs of similar tags in the folksonomy to pairs of synsets in Wordnet, where we use validated measures of semantic distance to characterize the semantic relation between the mapped tags. This exposes important features of the investigated similarity measures and indicates which ones are better suited in the context of a given semantic application.</p></td></tr><tr><td>602</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_38">Exploring Semantic Social Networks Using Virtual Reality</a></td></tr><tr><td colspan=3><p>We present Redgraph, the first generic virtual reality visualization program for Semantic Web data. Redgraph is capable of handling large data-sets, as we demonstrate on social network data from the U.S. Patent Trade Office. We develop a Semantic Web vocabulary of virtual reality terms compatible with GraphXML to map graph visualization into the Semantic Web itself. Our approach to visualizing Semantic Web data takes advantage of user-interaction in an immersive environment to bypass a number of difficult issues in 3-dimensional graph visualization layout by relying on users themselves to interactively extrude the nodes and links of a 2-dimensional graph into the third dimension. When users touch nodes in the virtual reality environment, they retrieve data formatted according to the data’s schema or ontology. We applied Redgraph to social network data constructed from patents, inventors, and institutions from the United States Patent and Trademark Office in order to explore networks of innovation in computing. Using this data-set, results of a user study comparing extrusion (3-D) vs. no-extrusion (2-D) are presented. The study showed the use of a 3-D interface by subjects led to significant improvement on answering of fine-grained questions about the data-set, but no significant difference was found for broad questions about the overall structure of the data. Furthermore, inference can be used to improve the visualization, as demonstrated with a data-set of biotechnology patents and researchers.</p></td></tr><tr><td>603</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_36">Formal Model for Semantic-Driven Service Execution</a></td></tr><tr><td colspan=3><p>Integration of heterogeneous services is often hard-wired in service or workflow implementations. In this paper we define an execution model operating on semantic descriptions of services allowing flexible integration of services with solving data and process conflicts where necessary. We implement the model using our WSMO technology and a case scenario from the B2B domain of the SWS Challenge.</p></td></tr><tr><td>604</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_35">Semantic Web Service Choreography: Contracting and Enactment</a></td></tr><tr><td colspan=3><p>The emerging paradigm of service-oriented computing requires novel techniques for various service-related tasks. Along with automated support for service discovery, selection, negotiation, and composition, support for automated service contracting and enactment is crucial for any large scale service environment, where large numbers of clients and service providers interact. Many problems in this area involve reasoning, and a number of logic-based methods to handle these problems have emerged in the field of Semantic Web Services. In this paper, we build upon our previous work where we used Concurrent Transaction Logic (CTR) to model and reason about service contracts. We significantly extend the modeling power of the previous work by allowing iterative processes in the specification of service contracts, and we extend the proof theory of CTR to enable reasoning about such contracts. With this extension, our logic-based approach is capable of modeling general services represented using languages such as WS-BPEL.</p></td></tr><tr><td>605</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_34">On the Semantics of Trust and Caching in the Semantic Web</a></td></tr><tr><td colspan=3><p>The Semantic Web is a distributed environment for knowledge representation and reasoning. The distributed nature brings with it failing data sources and inconsistencies between autonomous knowledge bases. To reduce problems resulting from unavailable sources and to improve performance, caching can be used. Caches, however, raise new problems of imprecise or outdated information. We propose to distinguish between certain and cached information when reasoning on the semantic web, by extending the well known </p></td></tr><tr><td>606</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_37">Efficient Semantic Web Service Discovery in Centralized and P2P Environments</a></td></tr><tr><td colspan=3><p>Efficient and scalable discovery mechanisms are critical for enabling service-oriented architectures on the Semantic Web. The majority of currently existing approaches focuses on centralized architectures, and deals with efficiency typically by pre-computing and storing the results of the semantic matcher for all possible query concepts. Such approaches, however, fail to scale with respect to the number of service advertisements and the size of the ontologies involved. On the other hand, this paper presents an efficient and scalable index-based method for Semantic Web service discovery that allows for fast selection of services at query time and is suitable for both centralized and P2P environments. We employ a novel encoding of the service descriptions, allowing the match between a request and an advertisement to be evaluated in constant time, and we index these representations to prune the search space, reducing the number of comparisons required. Given a desired ranking function, the search algorithm can retrieve the top-</p></td></tr><tr><td>607</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_32">RDFS Reasoning and Query Answering on Top of DHTs</a></td></tr><tr><td colspan=3><p>We study the problem of distributed RDFS reasoning and query answering on top of distributed hash tables. Scalable, distributed RDFS reasoning is an essential functionality for providing the scalability and performance that large-scale Semantic Web applications require. Our goal in this paper is to compare and evaluate two well-known approaches to RDFS reasoning, namely backward and forward chaining, on top of distributed hash tables. We show how to implement both algorithms on top of the distributed hash table Bamboo and prove their correctness. We also study the time-space trade-off exhibited by the algorithms analytically, and experimentally by evaluating our algorithms on PlanetLab.</p></td></tr><tr><td>608</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_33">An Interface-Based Ontology Modularization Framework for Knowledge Encapsulation</a></td></tr><tr><td colspan=3><p>In this paper, we present a framework for developing ontologies in a modular manner, which is based on the notions of interfaces and knowledge encapsulation. Within the context of this framework, an ontology can be defined and developed as a set of ontology modules that can access the knowledge bases of the others through their well-defined interfaces. An important implication of the proposed framework is that ontology modules can be developed completely independent of each others’ signature and language. Such modules are free to only utilize the required knowledge segments of the others. We describe the interface-based modular ontology formalism, which theoretically supports this framework and present its distinctive features compared to the exiting modular ontology formalisms. We also describe the real-world design and implementation of the framework for creating modular ontologies by extending OWL-DL and modifying the Swoop interfaces and reasoners.</p></td></tr><tr><td>609</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_31">Bringing the IPTC News Architecture into the Semantic Web</a></td></tr><tr><td colspan=3><p>For easing the exchange of news, the International Press Telecommunication Council (IPTC) has developed the NewsML Architecture (NAR), an XML-based model that is specialized into a number of languages such as NewsML G2 and EventsML G2. As part of this architecture, specific controlled vocabularies, such as the IPTC News Codes, are used to categorize news items together with other industry-standard thesauri. While news is still mainly in the form of text-based stories, these are often illustrated with graphics, images and videos. Media-specific metadata formats, such as EXIF, DIG35 and XMP, are used to describe the media. The use of different metadata formats in a single production process leads to interoperability problems within the news production chain itself. It also excludes linking to existing web knowledge resources and impedes the construction of uniform end-user interfaces for searching and browsing news content.</p></td></tr><tr><td>610</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_27">A Kernel Revision Operator for Terminologies — Algorithms and Evaluation</a></td></tr><tr><td colspan=3><p>Revision of a description logic-based ontology deals with the problem of incorporating newly received information consistently. In this paper, we propose a general operator for revising terminologies in description logic-based ontologies. Our revision operator relies on a reformulation of the kernel contraction operator in belief revision. We first define our revision operator for terminologies and show that it satisfies some desirable logical properties. Second, two algorithms are developed to instantiate the revision operator. Since in general, these two algorithms are computationally too hard, we propose a third algorithm as a more efficient alternative. We implemented the algorithms and provide evaluation results on their efficiency, effectiveness and meaningfulness in the context of two application scenarios: Incremental ontology learning and mapping revision.</p></td></tr><tr><td>611</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_29">RDF123: From Spreadsheets to RDF</a></td></tr><tr><td colspan=3><p>We describe RDF123, a highly flexible open-source tool for translating spreadsheet data to RDF. Existing spreadsheet-to-rdf tools typically map only to star-shaped RDF graphs, i.e. each spreadsheet row is an instance, with each column representing a property. RDF123, on the other hand, allows users to define mappings to arbitrary graphs, thus allowing much richer spreadsheet semantics to be expressed. Further, each row in the spreadsheet can be mapped with a fairly different RDF scheme. Two interfaces are available. The first is a graphical application that allows users to create their mapping in an intuitive manner. The second is a Web service that takes as input a URL to a Google spreadsheet or CSV file and an RDF123 map, and provides RDF as output.</p></td></tr><tr><td>612</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_7">Extending the RDFS Entailment Lemma</a></td></tr><tr><td colspan=3><p>We complement the RDF semantics specification of the W3C by proving decidability of RDFS entailment. Furthermore, we show completeness and decidability of entailment for RDFS extended with datatypes and a property-related subset of OWL.</p></td></tr><tr><td>613</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_6">A Model Theoretic Semantics for Ontology Versioning</a></td></tr><tr><td colspan=3><p>We show that the Semantic Web needs a formal semantics for the various kinds of links between ontologies and other documents. We provide a model theoretic semantics that takes into account ontology extension and ontology versioning. Since the Web is the product of a diverse community, as opposed to a single agent, this semantics accommodates different viewpoints by having different entailment relations for different ontology perspectives. We discuss how this theory can be practically applied to RDF and OWL and provide a theorem that shows how to compute perspective-based entailment using existing logical reasoners. We illustrate these concepts using examples and conclude with a discussion of future work.</p></td></tr><tr><td>614</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_30">Evaluating Long-Term Use of the Gnowsis Semantic Desktop for PIM</a></td></tr><tr><td colspan=3><p>The Semantic Desktop is a means to support users in Personal Information Management (PIM). Using the open source software prototype Gnowsis, we evaluated the approach in a two month case study in 2006 with eight participants. Two participants continued using the prototype and were interviewed after two years in 2008 to show their long-term usage patterns. This allows us to analyse how the system was used for PIM. Contextual interviews gave insights on behaviour, while questionnaires and event logging did not. We discovered that in the personal environment, simple has-Part and is-related relations are sufficient for users to file and re-find information, and that the personal semantic wiki was used creatively to note information.</p></td></tr><tr><td>615</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_4">Contexts for the Semantic Web</a></td></tr><tr><td colspan=3><p>A central theme of the Semantic Web is that programs should be able to easily aggregate data from different sources. Unfortunately, even if two sites provide their data using the same data model and vocabulary, subtle differences in their use of terms and in the assumptions they make pose challenges for aggregation. Experiences with the TAP project reveal some of the phenomena that pose obstacles to a simplistic model of aggregation. Similar experiences have been reported by AI projects such as Cyc, which has led to the development and use of various context mechanisms. In this paper we report on some of the problems with aggregating independently published data and propose a context mechanism to handle some of these problems. We briefly survey the context mechanisms developed in AI and contrast them with the requirements of a context mechanism for the Semantic Web. Finally, we present a context mechanism for the Semantic Web that is adequate to handle the aggregation tasks, yet simple from both computational and model theoretic perspectives.</p></td></tr><tr><td>616</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_28">Description Logic Reasoning with Decision Diagrams</a></td></tr><tr><td colspan=3><p>We propose a novel method for reasoning in the description logic </p></td></tr><tr><td>617</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_3">A Method for Converting Thesauri to RDF/OWL</a></td></tr><tr><td colspan=3><p>This paper describes a method for converting existing thesauri and related resources from their native format to RDF(S) and OWL. The method identifies four steps in the conversion process. In each step, decisions have to be taken with respect to the syntax or semantics of the resulting representation. Each step is supported through a number of guidelines. The method is illustrated through conversions of two large thesauri: MeSH and WordNet.</p></td></tr><tr><td>618</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_26">Scalable Grounded Conjunctive Query Evaluation over Large and Expressive Knowledge Bases</a></td></tr><tr><td colspan=3><p>Grounded conjunctive query answering over OWL-DL ontologies is intractable in the worst case, but we present novel techniques which allow for efficient querying of large expressive knowledge bases in secondary storage. In particular, we show that we can effectively answer grounded conjunctive queries without building a full completion forest for a large Abox (unlike state of the art tableau reasoners). Instead we rely on the completion forest of a dramatically reduced summary of the Abox. We demonstrate the effectiveness of this approach in Aboxes with up to 45 million assertions.</p></td></tr><tr><td>619</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_5">Bipartite Graphs as Intermediate Model for RDF</a></td></tr><tr><td colspan=3><p>RDF Graphs are sets of assertions in the form of subject-predicate-object triples of information resources. Although for simple examples they can be understood intuitively as directed labeled graphs, this representation does not scale well for more complex cases, particularly regarding the central notion of connectivity of resources.</p></td></tr><tr><td>620</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_2">Small Can Be Beautiful in the Semantic Web</a></td></tr><tr><td colspan=3><p>In 1984, Peter Patel-Schneider published a paper [1] entitled </p></td></tr><tr><td>621</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_24">Collecting Community-Based Mappings in an Ontology Repository</a></td></tr><tr><td colspan=3><p>Several ontology repositories provide access to the growing collection of ontologies on the Semantic Web. Some repositories collect ontologies automatically by crawling the Web; in other repositories, users submit ontologies themselves. In addition to providing search across multiple ontologies, the added value of ontology repositories lies in the metadata that they may contain. This metadata may include information provided by ontology authors, such as ontologies’ scope and intended use; feedback provided by users such as their experiences in using the ontologies or reviews of the content; and </p></td></tr><tr><td>622</td><td>2004</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-30475-3_1">How to Build Google2Google – An (Incomplete) Recipe –</a></td></tr><tr><td colspan=3><p>This talk explores aspects relevant for peer-to-peer search infrastructures, which we think are better suited to semantic web search than centralized approaches. It does so in the form of an (incomplete) cookbook recipe, listing necessary ingredients for putting together a distributed search infrastructure. The reader has to be aware, though, that many of these ingredients are research questions rather than solutions, and that it needs quite a few more research papers on these aspects before we can really cook and serve the final infrastructure. We’ll include appropriate references as examples for the aspects discussed (with some bias to our own work at L3S), though a complete literature overview would go well beyond cookbook recipe length limits.</p></td></tr><tr><td>623</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_23">Instanced-Based Mapping between Thesauri and Folksonomies</a></td></tr><tr><td colspan=3><p>The emergence of web based systems in which users can annotate items, raises the question of the semantic interoperability between vocabularies originating from collaborative annotation processes, often called folksonomies, and keywords assigned in a more traditional way. If collections are annotated according to two systems, e.g. with tags and keywords, the annotated data can be used for instance based mapping between the vocabularies. The basis for this kind of matching is an appropriate similarity measure between concepts, based on their distribution as annotations. In this paper we propose a new similarity measure that can take advantage of some special properties of user generated metadata. We have evaluated this measure with a set of articles from Wikipedia which are both classified according to the topic structure of Wikipedia and annotated by users of the bookmarking service del.icio.us. The results using the new measure are significantly better than those obtained using standard similarity measures proposed for this task in the literature, i.e., it correlates better with human judgments. We argue that the measure also has benefits for instance based mapping of more traditionally developed vocabularies.</p></td></tr><tr><td>624</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_22">Learning Concept Mappings from Instance Similarity</a></td></tr><tr><td colspan=3><p>Finding mappings between compatible ontologies is an important but difficult open problem. Instance-based methods for solving this problem have the advantage of focusing on the most active parts of the ontologies and reflect concept semantics as they are actually being used. However such methods have not at present been widely investigated in ontology mapping, compared to linguistic and structural techniques. Furthermore, previous instance-based mapping techniques were only applicable to cases where a substantial set of instances was available that was doubly annotated with both vocabularies. In this paper we approach the mapping problem as a classification problem based on the similarity between instances of concepts. This has the advantage that no doubly annotated instances are required, so that the method can be applied to any two corpora annotated with their own vocabularies. We evaluate the resulting classifiers on two real-world use cases, one with homogeneous and one with heterogeneous instances. The results illustrate the efficiency and generality of this method.</p></td></tr><tr><td>625</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_19">Improving an RCC-Derived Geospatial Approximation by OWL Axioms</a></td></tr><tr><td colspan=3><p>An approach to improve an RCC-derived geospatial approximation is presented which makes use of concept inclusion axioms in OWL. The algorithm used to control the approximation combines hypothesis testing with consistency checking provided by a knowledge representation system based on description logics. Propositions about the consistency of the refined ABox w.r.t. the associated TBox when compared to baseline ABox and TBox are made. Formal proves of the divergent consistency results when checking either of both are provided. The application of the approach to a geospatial setting results in a roughly tenfold improved approximation when using the refined ABox and TBox. Ways to further improve the approximation and to automate the detection of falsely calculated relations are discussed.</p></td></tr><tr><td>626</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_25">Algebras of Ontology Alignment Relations</a></td></tr><tr><td colspan=3><p>Correspondences in ontology alignments relate two ontology entities with a relation. Typical relations are equivalence or subsumption. However, different systems may need different kinds of relations. We propose to use the concepts of algebra of relations in order to express the relations between ontology entities in a general way. We show the benefits in doing so in expressing disjunctive relations, merging alignments in different ways, amalgamating alignments with relations of different granularity, and composing alignments.</p></td></tr><tr><td>627</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_21">Laconic and Precise Justifications in OWL</a></td></tr><tr><td colspan=3><p>A justification for an entailment in an OWL ontology is a minimal subset of the ontology that is sufficient for that entailment to hold. Since justifications respect the syntactic form of axioms in an ontology, they are usually neither </p></td></tr><tr><td>628</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_18">Combining a DL Reasoner and a Rule Engine for Improving Entailment-Based OWL Reasoning</a></td></tr><tr><td colspan=3><p>We introduce the notion of the mixed DL and entailment-based (DLE) OWL reasoning, defining a framework inspired from the hybrid and homogeneous paradigms for integration of rules and ontologies. The idea is to combine the TBox inferencing capabilities of the DL algorithms and the scalability of the rule paradigm over large ABoxes. Towards this end, we define a framework that uses a DL reasoner to reason over the TBox of the ontology (hybrid-like) and a rule engine to apply a domain-specific version of ABox-related entailments (homogeneous-like) that are generated by TBox queries to the DL reasoner. The DLE framework enhances the entailment-based OWL reasoning paradigm in two directions. Firstly, it disengages the manipulation of the TBox semantics from any incomplete entailment-based approach, using the efficient DL algorithms. Secondly, it achieves faster application of the ABox-related entailments and efficient memory usage, comparing it to the conventional entailment-based approaches, due to the low complexity and the domain-specific nature of the entailments.</p></td></tr><tr><td>629</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_20">OWL Datatypes: Design and Implementation</a></td></tr><tr><td colspan=3><p>We analyze the datatype system of OWL and OWL 2, and discuss certain nontrivial consequences of its definition, such as the extensibility of the set of supported datatypes and complexity of reasoning. We also argue that certain datatypes from the list of normative datatypes in the current OWL 2 Working Draft are inappropriate and should be replaced with different ones. Finally, we present an algorithm for datatype reasoning. Our algorithm is modular in the sense that it can handle any datatype that supports certain basic operations. We show how to implement these operations for number and string datatypes.</p></td></tr><tr><td>630</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_16">Comparison between Ontology Distances (Preliminary Results)</a></td></tr><tr><td colspan=3><p>There are many reasons for measuring a distance between ontologies. In particular, it is useful to know quickly if two ontologies are close or remote before deciding to match them. To that extent, a distance between ontologies must be quickly computable. We present constraints applying to such measures and several possible ontology distances. Then we evaluate experimentally some of them in order to assess their accuracy and speed.</p></td></tr><tr><td>631</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_15">Modeling Documents by Combining Semantic Concepts with Unsupervised Statistical Learning</a></td></tr><tr><td colspan=3><p>Human-defined concepts are fundamental building-blocks in constructing knowledge bases such as ontologies. Statistical learning techniques provide an alternative automated approach to concept definition, driven by data rather than prior knowledge. In this paper we propose a probabilistic modeling framework that combines both human-defined concepts and data-driven topics in a principled manner. The methodology we propose is based on applications of statistical topic models (also known as latent Dirichlet allocation models). We demonstrate the utility of this general framework in two ways. We first illustrate how the methodology can be used to automatically tag Web pages with concepts from a known set of concepts without any need for labeled documents. We then perform a series of experiments that quantify how combining human-defined semantic knowledge with data-driven techniques leads to better language models than can be obtained with either alone.</p></td></tr><tr><td>632</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_14">Optimization and Evaluation of Reasoning in Probabilistic Description Logic: Towards a Systematic Approach</a></td></tr><tr><td colspan=3><p>This paper describes the first steps towards developing a methodology for testing and evaluating the performance of reasoners for the probabilistic description logic P-</p></td></tr><tr><td>633</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_11">Enhancing Semantic Web Services with Inheritance</a></td></tr><tr><td colspan=3><p>Currently proposed Semantic Web Services technologies allow the creation of ontology-based semantic annotations of Web services so that software agents are able to discover, invoke, compose and monitor these services with a high degree of automation. The OWL Services (OWL-S) ontology is an upper ontology in OWL language, providing essential vocabularies to semantically describe Web services. Currently OWL-S services can only be developed independently; if one service is unavailable then finding a suitable alternative would require an expensive and difficult global search/match. It is desirable to have a new OWL-S construct that can systematically support substitution tracing as well as incremental development and reuse of services. Introducing inheritance relationship (IR) into OWL-S is a natural solution. However, OWL-S, as well as most of the other currently discussed formalisms for Semantic Web Services such as WSMO or SAWSDL, has yet to define a concrete and self-contained mechanism of establishing inheritance relationships among services, which we believe is very important for the automated annotation and discovery of Web services as well as human organization of services into a taxonomy-like structure. In this paper, we extend OWL-S with the ability to define and maintain inheritance relationships between services. Through the definition of an additional “inheritance profile”, inheritance relationships can be stated and reasoned about. Two types of IRs are allowed to grant service developers the choice to respect the “contract” between services or not. The proposed inheritance framework has also been implemented and the prototype will be briefly evaluated as well.</p></td></tr><tr><td>634</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_13">Statistical Learning for Inductive Query Answering on OWL Ontologies</a></td></tr><tr><td colspan=3><p>A novel family of parametric language-independent kernel functions defined for individuals within ontologies is presented. They are easily integrated with efficient statistical learning methods for inducing linear classifiers that offer an alternative way to perform classification w.r.t. deductive reasoning. A method for adapting the parameters of the kernel to the knowledge base through stochastic optimization is also proposed. This enables the exploitation of statistical learning in a variety of tasks where an inductive approach may bridge the gaps of the standard methods due the inherent incompleteness of the knowledge bases. In this work, a system integrating the kernels has been tested in experiments on approximate query answering with real ontologies collected from standard repositories.</p></td></tr><tr><td>635</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_10">Extracting Semantic Constraint from Description Text for Semantic Web Service Discovery</a></td></tr><tr><td colspan=3><p>Various semantic web service discovery techniques have been proposed, many of which perform the profile based service signature (I/O) matching. However, the service I/O concepts are not sufficient to discover web services accurately. This paper presents a new method to enhance the semantic description of semantic web service by using the semantic constraints of service I/O concepts in specific context. The semantic constraints described in a constraint graph are extracted automatically from the parsing results of the service description text by a set of heuristic rules. The corresponding semantic web service matchmaker performs not only the profile’s semantic matching but also the matching of their semantic constraints with the help of a constraint graph based matchmaking algorithm. The experiment results are encouraging when applying the semantic constraint to discover semantic web services on the service retrieval test collection OWLS-TC v2.</p></td></tr><tr><td>636</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_8">The Expressive Power of SPARQL</a></td></tr><tr><td colspan=3><p>This paper studies the expressive power of SPARQL. The main result is that SPARQL and non-recursive safe Datalog with negation have equivalent expressive power, and hence, by classical results, SPARQL is equivalent from an expressiveness point of view to Relational Algebra. We present explicit generic rules of the transformations in both directions. Among other findings of the paper are the proof that negation can be simulated in SPARQL, that non-safe filters are superfluous, and that current SPARQL W3C semantics can be simplified to a standard compositional one.</p></td></tr><tr><td>637</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_7">Anytime Query Answering in RDF through Evolutionary Algorithms</a></td></tr><tr><td colspan=3><p>We present a technique for answering queries over RDF data through an evolutionary search algorithm, using fingerprinting and Bloom filters for rapid approximate evaluation of generated solutions. Our evolutionary approach has several advantages compared to traditional database-style query answering. First, the result quality increases monotonically and converges with each evolution, offering “anytime” behaviour with arbitrary trade-off between computation time and query results; in addition, the level of approximation can be tuned by varying the size of the Bloom filters. Secondly, through Bloom filter compression we can fit large graphs in main memory, reducing the need for disk I/O during query evaluation. Finally, since the individuals evolve independently, parallel execution is straightforward. We present our prototype that evaluates basic SPARQL queries over arbitrary RDF graphs and show initial results over large datasets.</p></td></tr><tr><td>638</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_6">An Experimental Comparison of RDF Data Management Approaches in a SPARQL Benchmark Scenario</a></td></tr><tr><td colspan=3><p>Efficient RDF data management is one of the cornerstones in realizing the Semantic Web vision. In the past, different RDF storage strategies have been proposed, ranging from simple triple stores to more advanced techniques like clustering or vertical partitioning on the predicates. We present an experimental comparison of existing storage strategies on top of the SP</p></td></tr><tr><td>639</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_12">Using Semantic Distances for Reasoning with Inconsistent Ontologies</a></td></tr><tr><td colspan=3><p>Re-using and combining multiple ontologies on the Web is bound to lead to inconsistencies between the combined vocabularies. Even many of the ontologies that are in use today turn out to be inconsistent once some of their implicit knowledge is made explicit. However, robust and efficient methods to deal with inconsistencies are lacking from current Semantic Web reasoning systems, which are typically based on classical logic. In earlier papers, we have proposed the use of </p></td></tr><tr><td>640</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_5">nSPARQL: A Navigational Language for RDF</a></td></tr><tr><td colspan=3><p>Navigational features have been largely recognized as fundamental for graph database query languages. This fact has motivated several authors to propose RDF query languages with navigational capabilities. In particular, we have argued in a previous paper that </p></td></tr><tr><td>641</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_4">RoundTrip Ontology Authoring</a></td></tr><tr><td colspan=3><p>Controlled Language (CL) for Ontology Editing tools offer an attractive alternative for naive users wishing to create ontologies, but they are still required to spend time learning the correct syntactic structures and vocabulary in order to use the Controlled Language properly. This paper extends previous work (CLOnE) which uses standard NLP tools to process the language and manipulate an ontology. Here we also generate text in the CL from an existing ontology using template-based (or shallow) Natural Language Generation (NLG). The text generator and the CLOnE authoring process combine to form a RoundTrip Ontology Authoring environment: one can start with an existing imported ontology or one originally produced using CLOnE, (re)produce the Controlled Language, modify or edit the text as required and then turn the text back into the ontology in the CLOnE environment. Building on previous methodology we undertook an evaluation, comparing the RoundTrip Ontology Authoring process with a well-known ontology editor; where previous work required a CL reference manual with several examples in order to use the controlled language, the use of NLG reduces this learning curve for users and improves on existing results for basic ontology editing tasks.</p></td></tr><tr><td>642</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_3">Identifying Potentially Important Concepts and Relations in an Ontology</a></td></tr><tr><td colspan=3><p>More and more ontologies have been published and used widely on the web. In order to make good use of an ontology, especially a new and complex ontology, we need methods to help understand it first. Identifying potentially important concepts and relations in an ontology is an intuitive but challenging method. In this paper, we first define four features for potentially important concepts and relation from the ontological structural point of view. Then a simple yet effective Concept-And-Relation-Ranking (</p></td></tr><tr><td>643</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_2">Supporting Collaborative Ontology Development in Protégé</a></td></tr><tr><td colspan=3><p>Ontologies are becoming so large in their coverage that no single person or a small group of people can develop them effectively and ontology development becomes a community-based enterprise. In this paper, we discuss requirements for supporting collaborative ontology development and present Collaborative Protégé—a tool that supports many of these requirements, such as discussions integrated with ontology-editing process, chats, and annotations of changes and ontology components. We have evaluated Collaborative Protégé in the context of ontology development in an ongoing large-scale biomedical project that actively uses ontologies at the VA Palo Alto Healthcare System. Users have found the new tool effective as an environment for carrying out discussions and for recording references for the information sources and design rationale.</p></td></tr><tr><td>644</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_17">Folksonomy-Based Collabulary Learning</a></td></tr><tr><td colspan=3><p>The growing popularity of social tagging systems promises to alleviate the knowledge bottleneck that slows down the full materialization of the Semantic Web since these systems allow ordinary users to create and share knowledge in a simple, cheap, and scalable representation, usually known as folksonomy. However, for the sake of knowledge workflow, one needs to find a compromise between the uncontrolled nature of folksonomies and the controlled and more systematic vocabulary of domain experts. In this paper we propose to address this concern by devising a method that automatically enriches a folksonomy with domain expert knowledge and by introducing a novel algorithm based on frequent itemset mining techniques to efficiently learn an ontology over the enriched folksonomy. In order to quantitatively assess our method, we propose a new benchmark for task-based ontology evaluation where the quality of the ontologies is measured based on how helpful they are for the task of personalized information finding. We conduct experiments on real data and empirically show the effectiveness of our approach.</p></td></tr><tr><td>645</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_49">FrameNet Meets the Semantic Web: Lexical Semantics for the Web</a></td></tr><tr><td colspan=3><p>This paper describes FrameNet [9,1,3], an online lexical resource for English based on the principles of frame semantics [5,7,2]. We provide a data category specification for frame semantics and FrameNet annotations in an RDF-based language. More specifically, we provide an RDF markup for </p></td></tr><tr><td>646</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_47">Haystack: A Platform for Authoring End User Semantic Web Applications</a></td></tr><tr><td colspan=3><p>The Semantic Web promises to open innumerable opportunities for automation and information retrieval by standardizing the protocols for metadata exchange. However, just as the success of the World Wide Web can be attributed to the ease of use and ubiquity of Web browsers, we believe that the unfolding of the Semantic Web vision depends on users getting powerful but easy-to-use tools for managing their information. But unlike HTML, which can be easily edited in any text editor, RDF is more complicated to author and does not have an obvious presentation mechanism. Previous work has concentrated on the ideas of generic RDF graph visualization and RDF Schema-based form generation. In this paper, we present a comprehensive platform for constructing end user applications that create, manipulate, and visualize arbitrary RDF-encoded information, adding another layer to the abstraction cake. We discuss a programming environment specifically designed for manipulating RDF and introduce user interface concepts on top that allow the developer to quickly assemble applications that are based on RDF data models. Also, because user interface specifications and program logic are themselves describable in RDF, applications built upon our framework enjoy properties such as network updatability, extensibility, and end user customizability – all desirable characteristics in the spirit of the Semantic Web.</p></td></tr><tr><td>647</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_45">Ontology-Based Resource Matching in the Grid – The Grid Meets the Semantic Web</a></td></tr><tr><td colspan=3><p>The Grid is an emerging technology for enabling resource sharing and coordinated problem solving in dynamic multi-institutional virtual organizations. In the Grid environment, shared resources and users typically span different organizations. The resource matching problem in the Grid involves assigning resources to tasks in order to satisfy task requirements and resource policies. These requirements and policies are often expressed in disjoint application and resource models, forcing a resource selector to perform semantic matching between the two. In this paper, we propose a flexible and extensible approach for solving resource matching in the Grid using semantic web technologies. We have designed and prototyped an ontology-based resource selector that exploits ontologies, background knowledge, and rules for solving resource matching in the Grid.</p></td></tr><tr><td>648</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_46">A </a></td></tr><tr><td colspan=3><p>Semantic Web supports a fire-new infrastructure for solving the problem of semantic information interoperability, and it promises to support an intelligent and automatic information-processing platform for multi-agent system whose ultimate objective is to provide better services for end-users, for example, interoperable information query. Therefore, except agent-to-agent interaction in multi-agent system, there is human-to-agent interaction. To unify the two kinds of interaction, this paper introduces </p></td></tr><tr><td>649</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_9">Integrating Object-Oriented and Ontological Representations: A Case Study in Java and OWL</a></td></tr><tr><td colspan=3><p>The Web Ontology Language (OWL) provides a modelling paradigm that is especially well suited for developing models of large, structurally complex domains such as those found in Health Care and the Life Sciences. OWL’s declarative nature combined with powerful reasoning tools has effectively supported the development of very large and complex anatomy, disease, and clinical ontologies. OWL, however, is not a programming language, so using these models in applications necessitates both a </p></td></tr><tr><td>650</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-88564-1_1">Involving Domain Experts in Authoring OWL Ontologies</a></td></tr><tr><td colspan=3><p>The process of authoring ontologies requires the active involvement of domain experts who should lead the process, as well as providing the relevant conceptual knowledge. However, most domain experts lack knowledge modelling skills and find it hard to follow logical notations in OWL. This paper presents </p></td></tr><tr><td>651</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_43">WebScripter: Grass-Roots Ontology Alignment via End-User Report Creation</a></td></tr><tr><td colspan=3><p>Ontologies define hierarchies of classes and attributes; they are meta-data: data about data. In the “traditional approach” to ontology engineering, experts add new data by carefully analyzing others’ ontologies and fitting their new concepts into the existing hierarchy. In the emerging “Semantic Web approach”, ordinary users may not look at anyone’s ontology before creating theirs – instead, they may simply define a new local schema from scratch that addresses their immediate needs, without worrying if and how their data may some day integrate with others’ data. This paper describes WebScripter, a tool for translating between the countless mini-ontologies that the “Semantic Web approach” yields. In our approach, ordinary users graphically align data from multiple sources in a simple spreadsheet-like view without having to know anything about ontologies. The resulting web of equivalency statements is then mined by WebScripter to help users find related ontologies and data, and to automatically align the related data with their own.</p></td></tr><tr><td>652</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_44">Magpie – Towards a Semantic Web Browser</a></td></tr><tr><td colspan=3><p>Web browsing involves two tasks: finding the right web page and then making sense of its content. So far, research has focused on supporting the task of finding web resources through ‘standard’ information retrieval mechanisms, or semantics-enhanced search. Much less attention has been paid to the second problem. In this paper we describe Magpie, a tool which supports the interpretation of web pages. Magpie offers complementary knowledge sources, which a reader can call upon to quickly gain access to any background knowledge relevant to a web resource. Magpie automatically associates an ontology-based semantic layer to web resources, allowing relevant services to be invoked within a standard web browser. Hence, Magpie may be seen as a step towards a semantic web browser. The functionality of Magpie is illustrated using examples of how it has been integrated with our lab’s web resources.</p></td></tr><tr><td>653</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_48">Mangrove: Enticing Ordinary People onto the Semantic Web via Instant Gratification</a></td></tr><tr><td colspan=3><p>Despite numerous efforts, the </p></td></tr><tr><td>654</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_42">Cooking the Semantic Web with the OWL API</a></td></tr><tr><td colspan=3><p>This paper discusses issues that surround the provision of application support using OWL ontologies. It presents the OWL API, a high-level programmatic interface for accessing and manipulating OWL ontologies. We discuss the underlying design issues and illustrate possible solutions to technical issues occurring in systems that intend to support the OWL standard. Although the context of our solutions is that of a particular implementation, the issues discussed are largely independent of this and should be of interest to a wider community.</p></td></tr><tr><td>655</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_41">Prolog-Based Infrastructure for RDF: Scalability and Performance</a></td></tr><tr><td colspan=3><p>The semantic web is a promising application-area for the Prolog programming language for its non-determinism and pattern-matching. In this paper we outline an infrastructure for loading and saving RDF/XML, storing triples, elementary reasoning with triples and visualization. A predecessor of the infrastructure described here has been used in various applications for ontology-based annotation of multimedia objects using semantic web languages. Our library aims at fast parsing, fast access and scalability for fairly large but not unbounded applications upto 40 million triples.</p></td></tr><tr><td>656</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_40">DAMLJessKB: A Tool for Reasoning with the Semantic Web</a></td></tr><tr><td colspan=3><p>We describe DAMLJessKB, a tool for reasoning with the DARPA Agent Markup Language (DAML) and performing inference on the Semantic Web. DAMLJessKB maps DAML’s semantics into facts and rules for use in a production system, such as the Java Expert System Shell (Jess). This article presents our underlying methodology and provides a detailed example of how DAMLJessKB can be used to make decisions about DAML-encoded engineering design knowledge. We believe that tools like DAMLJessKB are needed to help realize the full potential of the Semantic Web and DAML.</p></td></tr><tr><td>657</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_37">SCULPTEUR: Towards a New Paradigm for Multimedia Museum Information Handling</a></td></tr><tr><td colspan=3><p>This paper describes the design and prototype implementation of a novel architecture for integrated concept, metadata and content based browsing and retrieval of museum information. The work is part of a European project involving several major galleries and the aim is to provide more versatile access to digital collections of museum artefacts, including 2-D images, 3-D models and other multimedia representations. An ontology for the museum domain, based on the CIDOC Conceptual Reference Model, is being developed as a semantic layer with references to the digital collection as instance information. A graphical concept browser is an integral component in the user interface, allowing navigation through the semantic layer, display of thumbnails, or full representations of artefacts and textual information in appropriate viewers and the invocation of conventional content based searching or combined querying. Semantic Web technologies are used in system integration to describe how tools for analysis and visualisation can be applied to different data types and sources. This supports flexible and managed formulation, execution and interpretation of the results of distributed multimedia queries. Combined searches using concepts, content and metadata can be initiated from a single user interface.</p></td></tr><tr><td>658</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_39">Benchmarking DAML+OIL Repositories</a></td></tr><tr><td colspan=3><p>We present a benchmark that facilitates the evaluation of DAML+OIL repositories in a standard and systematic way. This benchmark is intended to evaluate the performance of DAML+OIL repositories with respect to extensional queries over a large data set that commits to a single realistic ontology. It consists of the ontology, customizable synthetic data, a set of test queries, and several performance metrics. Main features of the benchmark include a plausible ontology for the university domain, a repeatable data set that can be scaled to an arbitrary size, and an approach for measuring the degree to which a repository returns complete query answers. We also show a benchmark experiment for the evaluation of DLDB, a DAML+OIL repository that extends a relational database management system with description logic inference capabilities.</p></td></tr><tr><td>659</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_38">Towards Ontology-Driven Discourse: From Semantic Graphs to Multimedia Presentations</a></td></tr><tr><td colspan=3><p>Traditionally, research in applying Semantic Web technology to multimedia information systems has focused on using annotations and ontologies to improve the retrieval process. This paper concentrates on improving the presentation of the retrieval results.</p></td></tr><tr><td>660</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_36">Integrating Structure and Semantics into Audio-visual Documents</a></td></tr><tr><td colspan=3><p>Describing audio-visual documents amounts to consider documentary aspects (the structure) as well as conceptual aspects (the content). In this paper, we propose an architecture which describes formally the content of the videos and which constrains the structure of their descriptions. This work is based on languages and technologies underlying the Semantic Web and in particular ontologies. Therefore, we propose to combine emerging Web standards, namely MPEG-7/XML Schema for the structural part and OWL/RDF for the knowledge part of the description. Finally, our work offers reasoning support on both aspects when querying a database of videos.</p></td></tr><tr><td>661</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_35">Semi-automatic Semantic Annotation of Images Using Machine Learning Techniques</a></td></tr><tr><td colspan=3><p>The success of the Semantic Web hinges on being able to produce semantic markups on Web pages and their components, in a way that is cost-effective and consistent with adopted schemas and ontologies. Since images are an essential component of the Web, this work focuses on an intelligent approach to semantic annotation of images. We propose a three-layer architecture, in which the bottom layer organizes visual information extracted from the raw image contents, which are mapped to semantically meaningful keywords in the middle layer, which are then connected to schemas and ontologies on the top layer. Our key contribution is the use of machine learning algorithms for user-assisted, semi-automatic image annotation, in such a way that the knowledge of previously annotated images – both at metadata and visual levels – is used to speed up the annotation of subsequent images within the same domain (ontology) as well as to improve future query and retrieval of annotated images.</p></td></tr><tr><td>662</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_34">Automatic Annotation of Content-Rich HTML Documents: Structural and Semantic Analysis</a></td></tr><tr><td colspan=3><p>Although RDF/XML has been widely recognized as the standard vehicle for representing semantic information on the Web, an enormous amount of semantic data is still being encoded in HTML documents that are designed primarily for human consumption and not directly amenable to machine processing. This paper seeks to bridge this semantic gap by addressing the fundamental problem of </p></td></tr><tr><td>663</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_33">Querying Semantic Web Resources Using TRIPLE Views</a></td></tr><tr><td colspan=3><p>Resources on the Semantic Web are described by metadata based on some formal or informal ontology. It is a common situation that casual users are not familiar with a domain ontology in detail. This makes it difficult for such users (or their user tools) to formulate queries to find the relevant resources. Users consider the resources in their specific context, so the most straightforward solution is to formulate queries in an ontology that corresponds to a user-specific view. We present an approach based on multiple views expressed in ontologies simpler than the domain ontology. This allows users to query heterogeneous data repositories in terms of multiple, relatively simple, view ontologies. Ontology developers can define such view ontologies and the corresponding mapping rules. These ontologies are represented in Semantic Web ontology languages such as RDFS, DAML+OIL, or OWL. We present our approach with examples from the e-learning domain using the Semantic Web query and transformation language TRIPLE.</p></td></tr><tr><td>664</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_32">An Approach for the Ranking of Query Results in the Semantic Web</a></td></tr><tr><td colspan=3><p>One of the vital problems in the searching for information is the ranking of the retrieved results, because users make typically very short queries (2-3 terms) and tend to consider only the first ten results. In traditional IR approaches the relevance of the results is determined only by analysing the underlying information repository (content and hyperlink structure), which leads to the weak relevance model. On the other hand, in the Semantic Web the querying process is supported by an ontology such that other important sources for determining the relevance of results can be considered: the structure of the underlying domain and the characteristics of the searching process.</p></td></tr><tr><td>665</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_31">Semantic Annotation, Indexing, and Retrieval</a></td></tr><tr><td colspan=3><p>The Semantic Web realization depends on the availability of critical mass of metadata for the web content, linked to formal knowledge about the world. This paper presents our vision about a holistic system allowing annotation, indexing, and retrieval of documents with respect to real-world entities. A system (called KIM), partially implementing this concept is shortly presented and used for evaluation and demonstration.</p></td></tr><tr><td>666</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_30">Beyond Ontology Construction; Ontology Services as Online Knowledge Sharing Communities</a></td></tr><tr><td colspan=3><p>There are a number of different ontology server implementations available, Their functionality focuses on editing, browsing and storing ontologies. In some cases the ontology server also provides an inference engine that allows statements about the relationships between entities in different ontologies to be tested or retrieved. These functions are certainly required for a practical deployment of an ontology service in an organization but when an ontology service is to be deployed in an open environment, such as the Semantic Web, a number of other considerations become apparent. This paper describes ACOS (Agent Cities Ontology Service), during the development of ACOS we have explored some aspects of creating a collaborative, community-oriented ontology server, in an open environment.</p></td></tr><tr><td>667</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_29">An Environment for Distributed Ontology Development Based on Dependency Management</a></td></tr><tr><td colspan=3><p>This paper describes a system for supporting development of ontology in a distributed manner. By a distributed manner, we mean ontology is divided into several component ontologies, which are developed by different developers in a distributed environment. The target ontology is obtained by compiling the component ontologies. These component ontologies are identified according to their conceptual level or domain characteristics. The distributed development of ontologies applies to many situations such as cooperative development, reusing ontologies and so on. To support such a way of ontology development, we investigate the dependency between component ontologies and design some functions for management of these ontologies based on their dependencies. We next consider the influence of a change of one ontology to others through its dependencies and design a function to suggest a few candidate modifications of the influenced ontology for keeping the consistency. We also present some examples of how the system works.</p></td></tr><tr><td>668</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_26">A Policy Based Approach to Security for the Semantic Web</a></td></tr><tr><td colspan=3><p>Along with developing specifications for the description of meta-data and the extraction of information for the Semantic Web, it is important to maximize security in this environment, which is fundamentally dynamic, open and devoid of many of the clues human societies have relied on for security assessment. Our research investigates the marking up of web entities with a semantic policy language and the use of distributed policy management as an alternative to traditional authentication and access control schemes. The policy language allows policies to be described in terms of deontic concepts and models speech acts, which allows the dynamic modification of existing policies, decentralized security control and less exhaustive policies. We present a security framework, based on this policy language, which addresses security issues for web resources, agents and services in the Semantic Web.</p></td></tr><tr><td>669</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_25">A Semantic E-Wallet to Reconcile Privacy and Context Awareness</a></td></tr><tr><td colspan=3><p>Increasingly, application developers are looking for ways to provide users with higher levels of personalization that capture different elements of a user’s operating context, such as her location, the task that she is currently engaged in, who her colleagues are, etc. While there are many sources of contextual information, they tend to vary from one user to another and also over time. Different users may rely on different location tracking functionality provided by different cell phone operators; they may use different calendar systems, etc. In this paper, we describe work on a Semantic e-Wallet aimed at supporting automated discovery and access of personal resources, each represented as a Semantic Web Service. A key objective is to provide a Semantic Web environment for open access to a user’s contextual resources, thereby reducing the costs associated with the development and maintenance of context-aware applications. A second objective is, through Semantic Web technologies, to empower users to selectively control who has access to their contextual information and under which conditions. This work has been carried out in the context of myCampus, a context-aware environment aimed at enhancing everyday campus life. Empirical results obtained on Carnegie Mellon’s campus are encouraging.</p></td></tr><tr><td>670</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_22">Security for DAML Web Services: Annotation and Matchmaking</a></td></tr><tr><td colspan=3><p>In the next generation of the Internet semantic annotations will enable software agents to extract and interpret web content more quickly than it is possible with current techniques. The focus of this paper is to develop security annotations for web services that are represented in DAML-S and used by agents. We propose several security-related ontologies that are designed to represent well-known security concepts. These ontologies are used to describe the security requirements and capabilities of web services providers and requesting agents. A reasoning engine decides whether agents and web service have comparable security characteristics. Our prototypical implementation uses the Java Theorem Prover from Stanford for deciding the degree to which the requirements and capabilities match based on our matching algorithm. The security reasoner is integrated with the Semantic Matchmaker from CMU giving it the ability to provide security brokering between agents and services.</p></td></tr><tr><td>671</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_27">Semantic Web Languages for Policy Representation and Reasoning: A Comparison of KAoS, Rei, and Ponder</a></td></tr><tr><td colspan=3><p>Policies are being increasingly used for automated system management and controlling the behavior of complex systems. The use of policies allows administrators to modify system behavior without changing source code or requiring the consent or cooperation of the components being governed. Early approaches to policy representation have been restrictive in many ways. However semantically-rich policy representations can reduce human error, simplify policy analysis, reduce policy conflicts, and facilitate interoperability. In this paper, we compare three approaches to policy representation, reasoning, and enforcement. We highlight similarities and differences between Ponder, KAoS, and Rei, and sketch out some general criteria and properties for more adequate approaches to policy semantics in the future.</p></td></tr><tr><td>672</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_20">IRS–II: A Framework and Infrastructure for Semantic Web Services</a></td></tr><tr><td colspan=3><p>In this paper we describe IRS–II (Internet Reasoning Service) a framework and implemented infrastructure, whose main goal is to support the publication, location, composition and execution of heterogeneous web services, augmented with semantic descriptions of their functionalities. IRS–II has three main classes of features which distinguish it from other work on semantic web services. Firstly, it supports one-click publishing of standalone software: IRS–II automatically creates the appropriate wrappers, given pointers to the standalone code. Secondly, it explicitly distinguishes between tasks (what to do) and methods (how to achieve tasks) and as a result supports capability-driven service invocation; flexible mappings between services and problem specifications; and dynamic, knowledge-based service selection. Finally, IRS–II services are web service compatible – standard web services can be trivially published through the IRS–II and any IRS–II service automatically appears as a standard web service to other web service infrastructures. In the paper we illustrate the main functionalities of IRS–II through a scenario involving a distributed application in the healthcare domain.</p></td></tr><tr><td>673</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_19">The DAML-S Virtual Machine</a></td></tr><tr><td colspan=3><p>This paper introduces the DAML-S Virtual Machine (DS-VM): an embedded component that uses the DAML-S Process Model to control the interaction between Web services. We provide a proof of the validity of the implementation of the DAML-S Virtual Machine by proving a mapping from the rules used by the DS-VM to the DAML-S Operational Semantics. Finally, we provide an example of use of the DS-VM with a DAML-Sized version of Amazon.com’s Web service, and we conclude with an empirical evaluation that shows that the overhead required by the DS-VM during the interaction with Amazon is only a small fraction of the time required by a query to Amazon. The DS-VM provides crucial evidence that DAML-S can be effectively used to manage the interaction between Web Services.</p></td></tr><tr><td>674</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_23">Trust Management for the Semantic Web</a></td></tr><tr><td colspan=3><p>Though research on the Semantic Web has progressed at a steady pace, its promise has yet to be realized. One major difficulty is that, by its very nature, the Semantic Web is a large, uncensored system to which anyone may contribute. This raises the question of how much credence to give each source. We cannot expect each user to know the trustworthiness of each source, nor would we want to assign top-down or global credibility values due to the subjective nature of trust. We tackle this problem by employing a web of trust, in which each user maintains trusts in a small number of other users. We then compose these trusts into trust values for all other users. The result of our computation is not an agglomerate “trustworthiness" of each user. Instead, each user receives a personalized set of trusts, which may vary widely from person to person. We define properties for combination functions which merge such trusts, and define a class of functions for which merging may be done locally while maintaining these properties. We give examples of specific functions and apply them to data from Epinions and our BibServ bibliography server. Experiments confirm that the methods are robust to noise, and do not put unreasonable expectations on users. We hope that these methods will help move the Semantic Web closer to fulfilling its promise.</p></td></tr><tr><td>675</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_21">Towards a Knowledge-Based Approach to Semantic Service Composition</a></td></tr><tr><td colspan=3><p>The successful application of Grid and Web Service technologies to real-world problems, such as e-Science [1], requires not only the development of a common vocabulary and meta-data framework as the basis for inter-agent communication and service integration but also the access and use of a rich repository of domain-specific knowledge for problem solving. Both requirements are met by the respective outcomes of ontological and knowledge engineering initiatives. In this paper we discuss a novel, knowledge-based approach to resource synthesis (service composition), which draws on the functionality of semantic web services to represent and expose available resources. The approach we use exploits domain knowledge to guide the service composition process and provide advice on service selection and instantiation. The approach has been implemented in a prototype workflow construction environment that supports the runtime recommendation of a service solution, service discovery via semantic service descriptions, and knowledge-based configuration of selected services. The use of knowledge provides a basis for full automation of service composition via conventional planning algorithms. Workflows produced by this system can be executed through a domain-specific direct mapping mechanism or via a more fluid approach such as WSDL-based service grounding. The approach and prototype have been used to demonstrate practical benefits in the context of the Geodise initiative [2].</p></td></tr><tr><td>676</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_24">Signing RDF Graphs</a></td></tr><tr><td colspan=3><p>Assuming P < GI < NP, the creation and verification of a digital signature of an arbitrary RDF graph cannot be done in polynomial time. However, it is possible to define a large class of canonicalizable RDF graphs, such that digital signatures for graphs in this class can be created and verified in O(</p></td></tr><tr><td>677</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_18">Semantic Markup for Semantic Web Tools: A DAML-S Description of an RDF-Store</a></td></tr><tr><td colspan=3><p>Easy integration of available tools will be a key success factor for the future of the Semantic Web. We envision that semantic descriptions of the tools themselves will play an important part in addressing this issue. Motivated by this vision we used DAML-S (a key initiative to support automated management of Web services) to describe Sesame (an RDF(S) storage and query engine). This paper discusses the major problems that we encountered as well as suggested solutions. This work is relevant to other Semantic Web research groups who wish to annotate their tools or to use annotated tools. Also, we hope to offer helpful input to the DAML-S coalition in the further development of their standard.</p></td></tr><tr><td>678</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_15">Adapting BPEL4WS for the Semantic Web: The Bottom-Up Approach to Web Service Interoperation</a></td></tr><tr><td colspan=3><p>Towards the ultimate goal of seamless interaction among networked programs and devices, industry has developed orchestration and process modeling languages such as XLANG, WSFL, and recently BPEL4WS. Unfortunately, these efforts leave us a long way from seamless interoperation. Researchers in the Semantic Web community have taken up this challenge proposing top-down approaches to achieve aspects of Web Service interoperation. Unfortunately, many of these efforts have been disconnected from emerging industry standards, particularly in process modeling. In this paper we take a bottom-up approach to integrating Semantic Web technology into Web services. Building on BPEL4WS, we present integrated Semantic Web technology for automating customized, dynamic binding of Web services together with interoperation through semantic translation. We discuss the value of semantically enriched service interoperation and demonstrate how our framework accounts for user-defined constraints while gaining potentially successful execution pathways in a practically motivated example. Finally, we provide an analysis of the forward-looking limitations of frameworks like BPEL4WS, and suggest how such specifications might embrace semantic technology at a fundamental level to work towards fully automated Web service interoperation.</p></td></tr><tr><td>679</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_16">Request Rewriting-Based Web Service Discovery</a></td></tr><tr><td colspan=3><p>One of the challenging problems that Web service technology faces is the ability to effectively discover services based on their capabilities. We present an approach to tackle this problem in the context of DAML-S ontologies of services. The proposed approach enables to select the combinations of Web services that </p></td></tr><tr><td>680</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_14">Surfing the Service Web</a></td></tr><tr><td colspan=3><p>The way that web services are currently being developed places them beside rather than within the existing World Wide Web. In this paper we present an approach that combines the strength of the World Wide Web, viz. interlinked HTML pages for presentation and human consumption, with the strength of semantic web services, viz. support for semi-automatic composition and invocation of web services that have semantically heterogeneous descriptions. The objective we aim at eventually is that a human user can seamlessly surf the existing World Wide Web and the emerging web services and that he can easily compose and invoke Web services on the fly without being a software engineer. This paper presents our framework, OntoMat-Service , which trades off between having a reasonably easy to use interface for web services and the complexity of web service workflows. It is not our objective that everybody can produce arbitrarily complex workflows of web services with our tool, the OntoMat-Service-Surfer. However, OntoMat-Service aims at a service web, where simple service flows are easily possible – even for the persons with not much technical background, while still allowing for difficult flows for the expert engineer.</p></td></tr><tr><td>681</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_13">Automating DAML-S Web Services Composition Using SHOP2</a></td></tr><tr><td colspan=3><p>The DAML-S Process Model is designed to support the application of AI planning techniques to the automated composition of Web services. SHOP2 is an Hierarchical Task Network (HTN) planner well-suited for working with the Process Model. We have proven the correspondence between the semantics of SHOP2 and the situation calculus semantics of the Process Model. We have also implemented a system which soundly and completely plans over sets of DAML-S descriptions using a SHOP2 planner, and then executes the resulting plans over the Web. We discuss the challenges and difficulties of using SHOP2 in the information-rich and human-oriented context of Web services.</p></td></tr><tr><td>682</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_17">Learning to Attach Semantic Metadata to Web Services</a></td></tr><tr><td colspan=3><p>Emerging Web standards promise a network of heterogeneous yet interoperable Web Services. Web Services would greatly simplify the development of many kinds of data integration and knowledge management applications. Unfortunately, this vision requires that services describe themselves with large amounts of semantic metadata “glue”. We explore a variety of machine learning techniques to semi-automatically create such metadata.</p></td></tr><tr><td>683</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_28">An Agent Framework for Inter-personal Information Sharing with an RDF-Based Repository</a></td></tr><tr><td colspan=3><p>In this paper, we propose a framework for personal agents that supports inter-personal information exchange and sharing. The framework offers a repository-centered architecture in which a user can store information resources and annotations. On this framework, applications work collaboratively with the help of the event notification mechanism.</p></td></tr><tr><td>684</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_11">C-OWL: Contextualizing Ontologies</a></td></tr><tr><td colspan=3><p>Ontologies are </p></td></tr><tr><td>685</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_10">Interoperability on XML Data</a></td></tr><tr><td colspan=3><p>We study the problem of interoperability among XML data sources. We propose a lightweight infrastructure for this purpose that derives its inspiration from the recent semantic web initiative. Our approach is based on enriching local sources with semantic declarations so as to enable interoperability. These declarations expose the semantics of the information content of sources by mapping the concepts present therein to a common (application specific) vocabulary, in the spirit of RDF. In addition to this infrastructure, we discuss tools that may assist in generating semantic declarations, and formulation of global queries and address some interesting issues in query processing and optimization.</p></td></tr><tr><td>686</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_12">Web Ontology Language Requirements w.r.t Expressiveness of Taxonomy and Axioms in Medicine</a></td></tr><tr><td colspan=3><p>An important issue is to know whether Web ontology languages, meet the expected requirements of expressiveness and reasoning. This paper aims at contributing to this question in evaluating and comparing several languages. After describing the needs of a Semantic Web in medicine, it analyses Protégé and DAML+OIL primitives on a concrete medical ontology, the brain cortex anatomy ontology. It draws conclusions about the requirements that a Web ontology language should meet for the representation of medical taxonomy and axioms. The expressiveness of DAML+OIL or OWL DL seems suited to describe the complex taxonomic knowledge. But rules are required for representing the deductive knowledge (dependencies between relations) and to support several tasks (ontology construction, maintenance, verification, query of heterogeneous distributed information sources). Finally, the paper evaluates the features of the next standard OWL and of an hybrid language CARIN-</p></td></tr><tr><td>687</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_8">Infrastructure for Web Explanations</a></td></tr><tr><td colspan=3><p>The Semantic Web lacks support for explaining knowledge provenance. When web applications return answers, many users do not know what information sources were used, when they were updated, how reliable the source was, or what information was looked up versus derived. The Semantic Web also lacks support for explaining reasoning paths used to derive answers. The Inference Web (IW) aims to take opaque query answers and make the answers more transparent by providing explanations. The explanations include information concerning where answers came from and how they were derived (or retrieved). In this paper we describe an infrastructure for IW explanations. The infrastructure includes: an extensible web-based registry containing details on information sources, reasoners, languages, and rewrite rules; a portable proof specification; and a proof and explanation browser. Source information in the IW registry is used to convey knowledge provenance. Representation and reasoning language axioms and rewrite rules in the IW registry are used to support proofs, proof combination, and semantic web agent interoperability. The IW browser is used to support navigation and presentations of proofs and their explanations. The Inference Web is in use by two Semantic Web agents using an embedded reasoning engine fully registered in the IW. Additional reasoning engine registration is underway in order to help provide input for evaluation of the adequacy, breadth, and scalability of our approach.</p></td></tr><tr><td>688</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_7">Viewing the Semantic Web through RVL Lenses</a></td></tr><tr><td colspan=3><p>Personalized access and content syndication involving diverse conceptual representations of information resources are two of the key challenges of real-scale Semantic Web (SW) applications, such as e-Commerce, e-Learning or e-Science portals. RDF/S represents nowadays the core SW language for creating and exchanging resource descriptions worldwide. Unfortunately, full-fledged view definition languages for the RDF/S data model are still missing. We propose </p></td></tr><tr><td>689</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_4">Web Ontology Reasoning with Datatype Groups</a></td></tr><tr><td colspan=3><p>When providing reasoning services for ontology languages such as DAML+OIL and OWL, it is necessary for description logics to deal with “concrete” datatypes (strings, integers, etc.) as well as “abstract” concepts and relationships. In this paper, we present a new approach, the datatype group approach, to integrating DLs with multiple datatypes. We discuss the advantages of such approach over the existing ones and show how a tableaux algorithm for the description logic </p></td></tr><tr><td>690</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_5">Merging Topics in Well-Formed XML Topic Maps</a></td></tr><tr><td colspan=3><p>Topic Maps are a standardized modelling approach for the semantic annotation and description of WWW resources. They enable an improved search and navigational access on information objects stored in semi-structured information spaces like the WWW. However, the according standards ISO 13250 and XTM (XML Topic Maps) lack formal semantics, several questions concerning e.g. subclassing, inheritance or merging of topics are left open. The proposed TMUML meta model, directly derived from the well known UML meta model, is a meta model for Topic Maps which enables semantic constraints to be formulated in OCL (object constraint language) in order to answer such open questions and overcome possible inconsistencies in Topic Map repositories. We will examine the XTM merging conditions and show, in several examples, how the TMUML meta model enables semantic constraints for Topic Map merging to be formulated in OCL. Finally, we will show how the TM validation process, i.e., checking if a Topic Map is well formed, includes our merging conditions.</p></td></tr><tr><td>691</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_3">RDFS(FA) and RDF MT: Two Semantics for RDFS</a></td></tr><tr><td colspan=3><p>RDF Schema (RDFS) has a non-standard metamodeling architecture, which makes some elements in the model have dual roles in the RDFS specification. As a result, this can be confusing and difficult to understand and, more importantly, the specification of its semantics requires a non-standard model theory. This leads to semantic problems when trying to layer conventional first order languages, like DAML+OIL, on top of RDFS. In this paper we will first demonstrate how this problem with RDFS can be solved in a sub-language of RDFS – RDFS(FA), which introduces a Fixed layer metamodeling Architecture to RDFS, based on a (relatively) standard model-theoretic semantics. Logical layer Semantic Web languages such as DAML+OIL and OWL can, therefore, be built on top of both the syntax and semantics of RDFS(FA). We will also compare this approach with the existing RDF Model Theory and discuss the advantages and disadvantages of the two approaches.</p></td></tr><tr><td>692</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_1">Representing the UMLS® Semantic Network Using OWL</a></td></tr><tr><td colspan=3><p>The Semantic Network, a component of the Unified Medical Language System® (UMLS), describes core biomedical knowledge consisting of semantic types and relationships. It is a well established, semi-formal ontology in widespread use for over a decade. We expected to “publish” this ontology on the Semantic Web, using OWL, with relatively little effort. However, we ran into a number of problems concerning alternative interpretations of the SN notation and the inability to express some of the interpretations in OWL. We detail these problems, as a cautionary tale to others planning to publish pre-existing ontologies on the Semantic Web, as a list of issues to consider when describing formally concepts in any ontology, and as a collection of criteria for evaluating alternative representations, which could form part of a methodology of ontology development.</p></td></tr><tr><td>693</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_38">Business and Enterprise Ontology Management with SymOntoX</a></td></tr><tr><td colspan=3><p>Ontologies are emerging as a key solution for knowledge sharing in co-operative business environment. From the technology point of view, the growth of Internet use has favoured the development of environments devoted to collaborative and distributed work, allowing different communities to increase flexibility and effectiveness in their work. From the representation point of view, an ontology management system represents a powerful tool to create common and shareable knowledge repositories. The goal of this work is to present SymOntoX, a web-based ontology management system. It is an open source environment supporting collaborative and distributed ontology construction and maintenance.</p></td></tr><tr><td>694</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_2">Reducing OWL Entailment to Description Logic Satisfiability</a></td></tr><tr><td colspan=3><p>We show how to reduce ontology entailment for the OWL DL and OWL Lite ontology languages to knowledge base satisfiability in (respectively) the </p></td></tr><tr><td>695</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_37">ClaiMaker: Weaving a Semantic Web of Research Papers</a></td></tr><tr><td colspan=3><p>The usability of research papers on the Web would be enhanced by a system that explicitly modelled the rhetorical relations between claims in related papers. We describe ClaiMaker, a system for modelling readers’ interpretations of the core content of papers. ClaiMaker provides tools to build a Semantic Web representation of the claims in research papers using an ontology of relations. We demonstrate how the system can be used to make inter-document queries.</p></td></tr><tr><td>696</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_35">Four Steps Towards the Widespread Adoption of a Semantic Web</a></td></tr><tr><td colspan=3><p>This paper suggests four steps towards the realization of a semantic web. Promotion of the idea should be based on practical application. There is need for the immediate development of practical demonstration applications. Simplicity and tolerance of error should be prime targets of research and development. An Open Source project to develop and populate a framework of tools and applications should be started.</p></td></tr><tr><td>697</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_34">Preemptive Reification</a></td></tr><tr><td colspan=3><p>It is useful to express the location of a node in a semantic graph in terms of a sequence of knowledge-bearing arcs that lead to the node from a node that is used as a point of reference. If the arcs on which such “graph-based addressing expressions” depend disappear, the expressions become invalid.</p></td></tr><tr><td>698</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_36">Three Implementations of SquishQL, a Simple RDF Query Language</a></td></tr><tr><td colspan=3><p>RDF provides a basic way to represent data for the Semantic Web. We have been experimenting with the query paradigm for working with RDF data in semantic web applications. Query of RDF data provides a declarative access mechanism that is suitable for application usage and remote access. We describe work on a conceptual model for querying RDF data that refines ideas first presented in at the W3C workshop on Query Languages [</p></td></tr><tr><td>699</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_33">SWAD-Europe: Semantic Web Advanced Development in Europe A Position Paper</a></td></tr><tr><td colspan=3><p>For the Web to reach its full potential, it must evolve into a Semantic Web, providing a universally accessible platform that allows data to be shared and processed by automated tools as well as by people. The ‘Semantic Web’ is a recent initiative of the World Wide Web Consortium (W3C), with the goal of extending the current Web to facilitate Web automation, universally accessible content, and the ‘Web of Trust’. However, if the semantic web is going to be adopted and assimilated a clear migration path from present technologies to new ones is required. The SWAD-Europe project aims to support the W3C’s Semantic Web initiative in Europe, providing targeted research, demonstrations and outreach to ensure Semantic Web technologies move into the mainstream of networked computing. The project aims to support the development and deployment of W3C Semantic Web specifications through implementation, research and testing activities.</p></td></tr><tr><td>700</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_32">A Mini-experiment in Semantic Annotation</a></td></tr><tr><td colspan=3><p>This paper describes a mini-experiment in using a tool for semantic annotation to index photographs of Windsor chairs, a type of antique furniture. The annotation tool makes use of an ontology based on art standards. We report on the experiences of subjects using the tool. The results suggest that a certain level of domain expertise is needed for semantic annotations, but also that an annotation tool has a clear added value for indexers.</p></td></tr><tr><td>701</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_9">Semantic Coordination: A New Approach and an Application</a></td></tr><tr><td colspan=3><p>Semantic coordination, namely the problem of finding an agreement on the meaning of heterogeneous semantic models, is one of the key issues in the development of the Semantic Web. In this paper, we propose a new algorithm for discovering semantic mappings across hierarchical classifications based on a new approach to semantic coordination. This approach shifts the problem of semantic coordination from the problem of computing linguistic or structural similarities (what most other proposed approaches do) to the problem of deducing relations between sets of logical formulae that represent the meaning of concepts belonging to different models. We show how to apply the approach and the algorithm to an interesting family of semantic models, namely hierarchical classifications, and present the results of preliminary tests on two types of hierarchical classifications, web directories and catalogs. Finally, we argue why this is a significant improvement on previous approaches.</p></td></tr><tr><td>702</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_30">Nonmonotonic Rule Systems on Top of Ontology Layers</a></td></tr><tr><td colspan=3><p>The development of the Semantic Web proceeds in layers. Currently the most advanced layer that has reached maturity is the ontology layer, in the from of the DAML+OIL language which corresponds to a rich description logic. The next step will be the the realization of logical rule systems on top of the ontology layer.</p></td></tr><tr><td>703</td><td>2003</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-39718-2_6">Semantic Processing of the Semantic Web</a></td></tr><tr><td colspan=3><p>We develop a semantics based approach to process information on the semantic web. We show how Horn logic can be used to denotationally capture the semantics of mark-up languages designed for describing resources on the semantic web (such as RDF). The same approach can also be used to specify the semantics of query languages for the semantic web. The semantics of both the resource description languages and the query languages are executable and when put together can be used to compute answers to semantic web queries. The main advantage of this semantic-based approach to processing the semantic web is that these executable semantics can be developed extremely quickly. Thus, as the semantic web mark-up languages evolve rapidly, their implementations can be developed at the same pace. In this paper, we present our approach based on denotational semantics and Horn logic. Our approach is quite general, and applicable to any description format (XML, RDF, DAML, etc.), though in this paper we illustrate it via RDF (Resource Description Framework).</p></td></tr><tr><td>704</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_31">An RDF NetAPI</a></td></tr><tr><td colspan=3><p>This paper describes some initial work on a NetAPI for accessing and updating RDF data over the web. The NetAPI includes actions for conditional extraction or update of RDF data, actions for model upload and download and also the ability to enquire about the capabilities of a hosting server. An initial experimental system is described which partially implements these ideas within the Jena toolkit.</p></td></tr><tr><td>705</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_27">DAML-S: Web Service Description for the Semantic Web</a></td></tr><tr><td colspan=3><p>In this paper we present DAML-S, a DAML+OIL ontology for describing the properties and capabilities of Web Services. Web Services - Web-accessible programs and devices - are garnering a great deal of interest from industry, and standards are emerging for low-level descriptions of Web Services. DAML-S complements this effort by providing Web Service descriptions at the application layer, describing </p></td></tr><tr><td>706</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_29">A Data Integration Framework for e-Commerce Product Classification</a></td></tr><tr><td colspan=3><p>A marketplace is the place in which the demand and supply of buyers and vendors participating in a business process may meet. Therefore, electronic marketplaces are virtual communities in which buyers may meet proposals of several suppliers and make the best choice. In the electronic commerce world, the comparison between different products is blocked due to the lack of standards (on the contrary, the proliferation of standards) describing and classifying them. Therefore, the need for B2B and B2C marketplaces is to reclassify products and goods according to different standardization models. This paper aims to face this problem by suggesting the use of a semi-automatic methodology, supported by a tool (SI-Designer), to define the mapping among different e-commerce product classification standards. This methodology was developed for the MOMIS system within the Intelligent Integration of Information research area. We describe our extension to the methodology that makes it applyable in general to product classification standard, by selecting a fragment of ECCMA/UNSPSC and ecl@ss standard.</p></td></tr><tr><td>707</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_26">Semantic Matching of Web Services Capabilities</a></td></tr><tr><td colspan=3><p>The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that location of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL.</p></td></tr><tr><td>708</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_24">Taking the RDF Model Theory Out for a Spin</a></td></tr><tr><td colspan=3><p>Entailment, as defined by RDF’s model-theoretical semantics, is a basic requirement for processing RDF, and represents the kind of “semantic interoperability” that RDF-based systems have been anticipated to have to realize the vision of the “Semantic Web”. In this paper we give some results in our investigation of a practical implementation of the entailment rules, based on the graph-walking query mechanism of the Wi</p></td></tr><tr><td>709</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_28">TRIPLE—A Query, Inference, and Transformation Language for the Semantic Web</a></td></tr><tr><td colspan=3><p>This paper presents TRIPLE, a layered and modular rule language for the Semantic Web [</p></td></tr><tr><td>710</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_22">Bringing Together Semantic Web and Web Services</a></td></tr><tr><td colspan=3><p>There are two major ongoing efforts to advance the World Wide Web. On one side there is the Semantic Web research, on the other side is the Web Service research. Both activities aim to make content on the web accessible and usable not only for humans but also for machines in order to create a foundation for intelligent automated services and business processes. These two efforts are highly complementary, and there is work in progress towards a unification of them. This paper contributes to this process of unification by presenting a method of connecting Web Services descriptions with Semantic Web ontologies.</p></td></tr><tr><td>711</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_25">Concurrent Execution Semantics of DAML-S with Subtypes</a></td></tr><tr><td colspan=3><p>The DARPA Agent Markup Language ontology for Services (DAML-S) enables the description of Web-based services, such that they can be discovered, accessed and composed dynamically by intelligent software agents and other Web services, thereby facilitating the coordination between distributed, heterogeneous systems on the Web. We propose a formalised syntax and an initial reference semantics for DAML-S, which incorporates subtype polymorphism. The semantics we describe is derived from the semantics for Erlang and Concurrent Haskell. We contrast our semantics with an alternate semantics proposed for DAML-S, based on the situation calculus and Petri nets.</p></td></tr><tr><td>712</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_23">Global vs. Community Metadata Standards: Empowering Users for Knowledge Exchange</a></td></tr><tr><td colspan=3><p>The idea of knowledge sharing has strong roots in the education process. With the current development of the technology and moving learning material into the web environment it acquired a new dimension. Learning objects are the chunks of knowledge shared by e-learning community. Organizations and individuals are building repositories of learning objects and annotate them with metadata to describe their educational values and standardization efforts are on the way to provide a franca lingua for the educators. In this paper we describe the peer-to-peer infrastructure for sharing learning object we are building in Canada. The POOL projects builds on the three types of nodes: SPLASH is an freely downloadable application which allows individuals to create metadata and maintain their collection of learning objects, PONDs are bigger repositories of learning objects connected to the peer-to-peer network and POOL centrals increase the speed and breadth of the searches in the peer-to-peer network. The POOL project uses CanCore - a subset of the IMS metadata protocol - to describe learning objects. In the second part of the paper we discuss the future direction of this initiative based on the maturing learning objects community and lessons learned in the deployment of POOL network. We argue that the standardization effort, although very important, currently provides solutions that are too complex. We see the communities where the knowledge is shared to be the main force in the creation of the metadata standards which would support the growth of semantic web. The implications of moving the responsibility for schemas and metadata creation on communities poses new requirements on interoperability and tools. We describe those requirements and we outline approach we are developing to address them.</p></td></tr><tr><td>713</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_19">Towards a Modification Exchange Language for Distributed RDF Repositories</a></td></tr><tr><td colspan=3><p>Many RDF repositories have already been implemented with various access languages and mechanisms. The aim of the EDUTELLA framework is to allow communication between different RDF repository implementations. Part of EDUTELLA is a Query Exchange Language (QEL) which can be used as lingua franca to retrieve information from RDF repositories. This work shows why we also need standardization of distributed modification capabilities. We describe use case scenarios for annotation and replication services and use them as guideline for our approach towards a Modification Exchange Language (MEL) for distributed RDF repositories.</p></td></tr><tr><td>714</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_18">OntoEdit: Collaborative Ontology Development for the Semantic Web</a></td></tr><tr><td colspan=3><p>Ontologies now play an important role for enabling the semantic web. They provide a source of precisely defined terms e.g. for knowledge-intensive applications. The terms are used for concise communication across people and applications. Typically the development of ontologies involves collaborative efforts of multiple persons. OntoEdit is an ontology editor that integrates numerous aspects of ontology engineering. This paper focuses on collaborative development of ontologies with OntoEdit which is guided by a comprehensive methodology.</p></td></tr><tr><td>715</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_16">Semantic Configuration Web Services in the CAWICOMS Project</a></td></tr><tr><td colspan=3><p>Product configuration is a key technology in today’s highly specialized economy. Within the scope of state-of-the-art B2B frameworks and eProcurement solutions, various initiatives take into account the provision of configuration services. However, they all are based on the idea of defining quasi-standards for many-to-many relationships between customers and vendors. When moving towards networked markets, where suppliers dynamically form supply-side consortia, more flexible approaches to B2B integration become necessary. The emerging paradigm of Web services has therefore a huge potential in business application integration. This paper presents an application scenario for configuration Web services, that is currently under development in the research project CAWICOMS</p></td></tr><tr><td>716</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_20">Representing Disjunction and Quantifiers in RDF</a></td></tr><tr><td colspan=3><p>The advantage of the RDF/DAML+OIL family of languages over ordinary XML is that it is topic-neutral and composable. However, its expressivity is severely limited. This limitation is well known, and the usual remedy is </p></td></tr><tr><td>717</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_17">Integrating Vocabularies: Discovering and Representing Vocabulary Maps</a></td></tr><tr><td colspan=3><p>The Semantic Web would enable new ways of doing business on the Web that require development of advanced business document integration technologies performing intelligent document transformation. The documents use different vocabularies that consist of large hierarchies of terms. Accordingly, vocabulary mapping and transformation becomes an important task in the whole business document transformation process. It includes several subtasks: map discovery, map representation, and map execution that must be seamlessly integrated into the document integration process. In this paper we discuss the process of discovering the maps between two vocabularies assuming availability of two sets of documents, each using one of the vocabularies. We take the vocabularies of product classification codes as a playground and propose a reusable map discovery technique based on Bayesian text classification approach. We show how the discovered maps can be integrated into the document transformation process.</p></td></tr><tr><td>718</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_15">Querying the Semantic Web: A Formal Approach</a></td></tr><tr><td colspan=3><p>Ontologies are set to play a key role in the Semantic Web, and several web ontology languages, like DAML+OIL, are based on DLs. These not only provide a clear semantics to the ontology languages, but allows them to exploit DL systems in order to provide correct and complete reasoning services.</p></td></tr><tr><td>719</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_21">Towards Semantic Web Mining</a></td></tr><tr><td colspan=3><p>Semantic Web Mining aims at combining the two fast-developing research areas Semantic Web and Web Mining. The idea is to improve, on the one hand, the results of Web Mining by exploiting the new semantic structures in the Web; and to make use of Web Mining, on the other hand, for building up the Semantic Web. This paper gives an overview of where the two areas meet today, and sketches ways of how a closer integration could be profitable.</p></td></tr><tr><td>720</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_13">Building the Semantic Web on XML</a></td></tr><tr><td colspan=3><p>The semantic discontinuity between World-Wide Web languages, e.g., XML, XML Schema, and XPath, and Semantic Web languages, e.g., RDF, RDFS, and DAML+OIL, forms a serious barrier for the stated goals of the Semantic Web. This discontinuity results from a difference in modeling foundations between XML and logics. We propose to eliminate that discontinuity by creating a common semantic foundation for both the World-Wide Web and the Semantic Web, taking ideas from both. The common foundation results in essentially no change to XML, and only minor changes to RDF. But it allows the Semantic Web to get closer to its goal of describing the semantics of the World Wide Web. Other Semantic Web languages (including RDFS and DAML+OIL) are considerably changed because of this common foundation.</p></td></tr><tr><td>721</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_14">Trusting Information Sources One Citizen at a Time</a></td></tr><tr><td colspan=3><p>This paper describes an approach to derive assessments about information sources based on individual feedback about the sources. We describe TRELLIS, a system that helps users annotate their analysis of alternative information sources that can be contradictory and incomplete. As the user makes a decision on which sources to dismiss and which to believe in making a final decision, TRELLIS captures the derivation of the decision in a semantic markup. TRELLIS then uses these annotations to derive an assessment of the source based on the annotations of many individuals. Our work builds on the Semantic Web and presents a tool that helps users create annotations that are in a mix of formal and human language, and exploits the formal representations to derive measures of trust in the content of Web resources and their original source.</p></td></tr><tr><td>722</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_12">Benchmarking RDF Schemas for the Semantic Web</a></td></tr><tr><td colspan=3><p>Describing web resources using formal knowledge (i.e., creating metadata according to a formal representation of a domain of discourse) is the essence of the next evolution step of the Web, termed the Semantic Web. The W3C’s RDF/S (Resource Description Framework/Schema Language) enables the creation and exchange of resource metadata as normal web data. In this paper, we investigate the use of RDFS schemas as a means of knowledge representation and exchange in diverse application domains. In order to reason about the quality of existing RDF schemas, a benchmark serves as the basis of a statistical analysis performed with the aid of VRP, the Validating RDF Parser. The statistical data extracted lead to corollaries about the size and the morphology of RDF/S schemas. Furthermore, the study of the collected schemas draws useful conclusions about the actual use of RDF modeling constructs and frequent misuses of RDF/S syntax and/or semantics.</p></td></tr><tr><td>723</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_11">Ontology-Based Integration of XML Web Resources</a></td></tr><tr><td colspan=3><p>This paper deals with some modeling aspects that have to be addressed in the context of the integration of heterogeneous and autonomous XML resources. We propose an integration system, but the emphasis of this paper is neither on its algorithmic aspects nor on its technical details. Instead, we focus on the significance of offering appropriate high-level primitives and mechanisms for representing the </p></td></tr><tr><td>724</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_10">Automatic Generation of Java/SQL Based Inference Engines from RDF Schema and RuleML</a></td></tr><tr><td colspan=3><p>This paper describes two approaches for automatically converting RDF Schema and RuleML sources into an inference engine and storage repository. Rather than using traditional inference systems, our solution bases on mainstream technologies like Java and relational database systems. While this necessarily imposes some restrictions, the ease of integration into an existing IT landscape is a major advantage. We present the conversion tools and their limitations. Furthermore, an extension to RuleML is proposed, that allows Java-enabled reaction rules, where calls to Java libraries can be performed upon a rule firing. This requires hosts to be Java-enabled when rules and code are moved across the web. However, the solution allows for great engineering flexibility.</p></td></tr><tr><td>725</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_8">A Formal Model for Topic Maps</a></td></tr><tr><td colspan=3><p>Topic maps have been developed in order to represent the structures of relationships between subjects, independently of resources documenting them, and to allow standard representation and interoperability of such structures. The ISO 13250 XTM specification [</p></td></tr><tr><td>726</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_3">Matching RDF Graphs</a></td></tr><tr><td colspan=3><p>The Resource Description Framework (RDF) describes graphs of statements about resources. RDF is a fundamental lower layer of the semantic web. This paper explores the equality of two RDF graphs in light of the graph isomorphism literature. We consider anonymous resources as unlabelled vertices in a graph, and show that the standard graph isomorphism algorithms, developed in the 1970’s, can be used effectively for comparing RDF graphs. The techniques presented are useful for testing RDF software.</p></td></tr><tr><td>727</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_4">Layering the Semantic Web: Problems and Directions</a></td></tr><tr><td colspan=3><p>The Resource Description Framework and the Resource Description Framework Schema Specification are supposed to be the foundations of the Semantic Web, in that all other Semantic Web languages are to be layered on top of them. It turns out that such a layering cannot be achieved in a straightforward way. This paper describes the problem with the straightforward layering and lays out several alternative layering possibilities. The benefits and drawbacks of each of these possibilities are presented and analyzed.</p></td></tr><tr><td>728</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_5">Notions of Indistinguishability for Semantic Web Languages</a></td></tr><tr><td colspan=3><p>The paper reviews the notions of expressiveness of description logics from (N. Kurtonina and M. de Rijke. Expressiveness of concept expressions in first-order description logics. </p></td></tr><tr><td>729</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_2">The Grid, Grid Services and the Semantic Web: Technologies and Opportunities</a></td></tr><tr><td colspan=3><p>Grids are an emerging computational infrastructure that enables resource sharing and coordinated problem solving across dynamic, distributed collaborations that have come to be known as virtual organizations. Unlike the web, which primarily focuses on the sharing of information, the Grid provides a range of fundamental mechanisms for sharing diverse types of resource, such as computers, storage, data, software, and scientific instruments. In this talk, I will introduce the Grid concept and illustrate it with application examples from a range of scientific disciplines. It is likely that technology that is being developed for the Semantic Web will have important roles to play in Grid Services; I will explore some of these potential areas of Semantic Web technologies, identifying those that I think offer the most potential.</p></td></tr><tr><td>730</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_1">Semantic Web Enabled Web Services</a></td></tr><tr><td colspan=3><p>Web Services will transform the web from a collection of information into a distributed device of computation. In order to employ their full potential, appropriate description means for web services need to be developed. For this purpose we define a full-fledged </p></td></tr><tr><td>731</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_7">Sesame: A Generic Architecture for Storing and Querying RDF and RDF Schema</a></td></tr><tr><td colspan=3><p>RDF and RDF Schema are two W3C standards aimed at enriching the Web with machine-processable semantic data.</p></td></tr><tr><td>732</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_9">Towards High-Precision Service Retrieval</a></td></tr><tr><td colspan=3><p>The ability to rapidly locate useful on-line services (e.g. software applications, software components, process models, or service organizations), as opposed to simply useful documents, is becoming increasingly critical in many domains. Current service retrieval technology is, however, notoriously prone to low precision. This paper describes a novel service retrieval approached based on the sophisticated use of process ontologies. Our preliminary evaluations suggest that this approach offers qualitatively higher retrieval precision than existing (keyword and table-based) approaches without sacrificing recall and computational tractability/scalability.</p></td></tr><tr><td>733</td><td>2002</td><td><a href="https://link.springer.com/chapter/10.1007/3-540-48005-6_6">The Usable Ontology: An Environment for Building and Assessing a Domain Ontology</a></td></tr><tr><td colspan=3><p>Experience shows that the quality of the stored knowledge determines the success (therefore the effective usage) of an ontology. In fact, an ontology where relevant concepts are absent, or are not conformant to a domain view of a given community, will be scarcely used, or even disregarded. In this paper we present a method and a set of software tools aimed at supporting domain experts in populating a domain ontology and obtaining a shared consensus on its content. “Consensus” is achieved in an implicit and explicit way: implicitly, since candidate concepts are selected among the terms that are frequently and consistently referred in the documents produced by the virtual community of users; explicitly, through the use of a web-based groupware aimed at consensus building.</p></td></tr><tr><td>734</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_37">Measuring Semantic Coherence of a Conversation</a></td></tr><tr><td colspan=3><p>Conversational systems have become increasingly popular as a way for humans to interact with computers. To be able to provide intelligent responses, conversational systems must correctly model the structure and semantics of a conversation. We introduce the task of measuring semantic (in)coherence in a conversation with respect to background knowledge, which relies on the identification of semantic relations between concepts introduced during a conversation. We propose and evaluate graph-based and machine learning-based approaches for measuring semantic coherence using knowledge graphs, their vector space embeddings and word embedding models, as sources of background knowledge. We demonstrate how these approaches are able to uncover different coherence patterns in conversations on the Ubuntu Dialogue Corpus.</p></td></tr><tr><td>735</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_38">Combining Truth Discovery and RDF Knowledge Bases to Their Mutual Advantage</a></td></tr><tr><td colspan=3><p>This study exploits knowledge expressed in RDF Knowledge Bases (KBs) to enhance Truth Discovery (TD) performances. TD aims to identify </p></td></tr><tr><td>736</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_36">Cross-Lingual Classification of Crisis Data</a></td></tr><tr><td colspan=3><p>Many citizens nowadays flock to social media during crises to share or acquire the latest information about the event. Due to the sheer volume of data typically circulated during such events, it is necessary to be able to efficiently filter out irrelevant posts, thus focusing attention on the posts that are truly relevant to the crisis. Current methods for classifying the relevance of posts to a crisis or set of crises typically struggle to deal with posts in different languages, and it is not viable during rapidly evolving crisis situations to train new models for each language. In this paper we test statistical and semantic classification approaches on cross-lingual datasets from 30 crisis events, consisting of posts written mainly in English, Spanish, and Italian. We experiment with scenarios where the model is trained on one language and tested on another, and where the data is translated to a single language. We show that the addition of semantic features extracted from external knowledge bases improve accuracy over a purely statistical model.</p></td></tr><tr><td>737</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_34">WORQ: Workload-Driven RDF Query Processing</a></td></tr><tr><td colspan=3><p>Cloud-based systems provide a rich platform for managing large-scale RDF data. However, the distributed nature of these systems introduces several performance challenges, </p></td></tr><tr><td>738</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_35">Canonicalisation of Monotone SPARQL Queries</a></td></tr><tr><td colspan=3><p>Caching in the context of expressive query languages such as SPARQL is complicated by the difficulty of detecting equivalent queries: deciding if two conjunctive queries are equivalent is NP-complete, where adding further query features makes the problem undecidable. Despite this complexity, in this paper we propose an algorithm that performs syntactic canonicalisation of SPARQL queries such that the answers for the canonicalised query will not change versus the original. We can guarantee that the canonicalisation of two queries within a core fragment of SPARQL (monotone queries with select, project, join and union) is equal if and only if the two queries are equivalent; we also support other SPARQL features but with a weaker soundness guarantee: that the (partially) canonicalised query is equivalent to the input query. Despite the fact that canonicalisation must be harder than the equivalence problem, we show the algorithm to be practical for real-world queries taken from SPARQL endpoint logs, and further show that it detects more equivalent queries than when compared with purely syntactic methods. We also present the results of experiments over synthetic queries designed to stress-test the canonicalisation method, highlighting difficult cases.</p></td></tr><tr><td>739</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_30">Towards Empty Answers in SPARQL: Approximating Querying with RDF Embedding</a></td></tr><tr><td colspan=3><p>The LOD cloud offers a plethora of RDF data sources where users discover items of interest by issuing SPARQL queries. A common query problem for users is to face with empty answers: given a SPARQL query that returns nothing, how to refine the query to obtain a non-empty set? In this paper, we propose an RDF graph embedding based framework to solve the SPARQL empty-answer problem in terms of a continuous vector space. We first project the RDF graph into a continuous vector space by an entity context preserving translational embedding model which is specially designed for SPARQL queries. Then, given a SPARQL query that returns an empty set, we partition it into several parts and compute approximate answers by leveraging RDF embeddings and the translation mechanism. We also generate alternative queries for returned answers, which helps users recognize their expectations and refine the original query finally. To validate the effectiveness and efficiency of our framework, we conduct extensive experiments on the real-world RDF dataset. The results show that our framework can significantly improve the quality of approximate answers and speed up the generation of alternative queries.</p></td></tr><tr><td>740</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_31">Query-Based Linked Data Anonymization</a></td></tr><tr><td colspan=3><p>We introduce and develop a declarative framework for privacy-preserving Linked Data publishing in which privacy and utility policies are specified as SPARQL queries. Our approach is data-independent and leads to inspect only the privacy and utility policies in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies. We prove the soundness of our algorithms and gauge their performance through experiments.</p></td></tr><tr><td>741</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_29">Pragmatic Ontology Evolution: Reconciling User Requirements and Application Performance</a></td></tr><tr><td colspan=3><p>Increasingly, organizations are adopting ontologies to describe their large catalogues of items. These ontologies need to evolve regularly in response to changes in the domain and the emergence of new requirements. An important step of this process is the selection of candidate concepts to include in the new version of the ontology. This operation needs to take into account a variety of factors and in particular reconcile user requirements and application performance. Current ontology evolution methods focus either on ranking concepts according to their relevance or on preserving compatibility with existing applications. However, they do not take in consideration the impact of the ontology evolution process on the performance of computational tasks – e.g., in this work we focus on instance tagging, similarity computation, generation of recommendations, and data clustering. In this paper, we propose the </p></td></tr><tr><td>742</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_32">Answering Provenance-Aware Queries on RDF Data Cubes Under Memory Budgets</a></td></tr><tr><td colspan=3><p>The steadily-growing popularity of semantic data on the Web and the support for aggregation queries in SPARQL 1.1 have propelled the interest in Online Analytical Processing (OLAP) and data cubes in RDF. Query processing in such settings is challenging because SPARQL OLAP queries usually contain many triple patterns with grouping and aggregation. Moreover, one important factor of query answering on Web data is its provenance, i.e., metadata about its origin. Some applications in data analytics and access control require to augment the data with provenance metadata and run queries that impose constraints on this provenance. This task is called provenance-aware query answering. In this paper, we investigate the benefit of caching some parts of an RDF cube augmented with provenance information when answering provenance-aware SPARQL queries. We propose provenance-aware caching (PAC), a caching approach based on a provenance-aware partitioning of RDF graphs, and a benefit model for RDF cubes and SPARQL queries with aggregation. Our results on real and synthetic data show that PAC outperforms significantly the LRU strategy (least recently used) and the Jena TDB native caching in terms of hit-rate and response time.</p></td></tr><tr><td>743</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_28">Practical Ontology Pattern Instantiation, Discovery, and Maintenance with Reasonable Ontology Templates</a></td></tr><tr><td colspan=3><p>Reasonable Ontology Templates (</p></td></tr><tr><td>744</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_26">Mapping Diverse Data to RDF in Practice</a></td></tr><tr><td colspan=3><p>Converting data from diverse data sources to custom RDF datasets often faces several practical challenges related with the need to restructure and transform the source data. Existing RDF mapping languages assume that the resulting datasets mostly preserve the structure of the original data. In this paper, we present real cases that highlight the limitations of existing languages, and describe D2RML, a transformation-oriented RDF mapping language which addresses such practical needs by incorporating a programming flavor in the mapping process.</p></td></tr><tr><td>745</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_33">Bash Datalog: Answering Datalog Queries with Unix Shell Commands</a></td></tr><tr><td colspan=3><p>Dealing with large tabular datasets often requires extensive preprocessing. This preprocessing happens only once, so that loading and indexing the data in a database or triple store may be an overkill. In this paper, we present an approach that allows preprocessing large tabular data in Datalog – without indexing the data. The Datalog query is translated to Unix Bash and can be executed in a shell. Our experiments show that, for the use case of data preprocessing, our approach is competitive with state-of-the-art systems in terms of scalability and speed, while at the same time requiring only a Bash shell on a Unix system.</p></td></tr><tr><td>746</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_25">Specifying, Monitoring, and Executing Workflows in Linked Data Environments</a></td></tr><tr><td colspan=3><p>We present an ontology for representing workflows over components with Read-Write Linked Data interfaces and give an operational semantics to the ontology via a rule language. Workflow languages have been successfully applied for modelling behaviour in enterprise information systems, in which the data is often managed in a relational database. Linked Data interfaces have been widely deployed on the web to support data integration in very diverse domains, increasingly also in scenarios involving the Internet of Things, in which application behaviour is often specified using imperative programming languages. With our work we aim to combine workflow languages, which allow for the high-level specification of application behaviour by non-expert users, with Linked Data, which allows for decentralised data publication and integrated data access. We show that our ontology is expressive enough to cover the basic workflow patterns and demonstrate the applicability of our approach with a prototype system that observes pilots carrying out tasks in a virtual reality aircraft cockpit. On a synthetic benchmark from the building automation domain, the runtime scales linearly with the size of the number of Internet of Things devices.</p></td></tr><tr><td>747</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_27">A Novel Approach and Practical Algorithms for Ontology Integration</a></td></tr><tr><td colspan=3><p>Today a wealth of knowledge and data are distributed using Semantic Web standards. Especially in the (bio)medical domain several sources like SNOMED, NCI, FMA, and more are distributed in the form of OWL ontologies. These can be matched and integrated in order to create one large medical Knowledge Base. However, an important issue is that the structure of these ontologies may be profoundly different hence using the mappings as initially computed can lead to incoherences or changes in their original structure which may affect applications. In this paper we present a framework and novel approach for integrating independently developed ontologies. Starting from an initial seed ontology which may already be in use by an application, new sources are used to iteratively enrich and extend the seed one. To deal with structural incompatibilities we present a novel fine-grained approach which is based on mapping repair and alignment conservativity, formalise it and provide an exact as well as approximate but practical algorithms. Our framework has already been used to integrate a number of medical ontologies and support real-world healthcare services provided by Babylon Health. Finally, we also perform an experimental evaluation and compare with state-of-the-art ontology integration systems that take into account the structure and coherency of the integrated ontologies obtaining encouraging results.</p></td></tr><tr><td>748</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_23">Detecting Erroneous Identity Links on the Web Using Network Metrics</a></td></tr><tr><td colspan=3><p>In the absence of a central naming authority on the Semantic Web, it is common for different datasets to refer to the same thing by different IRIs. Whenever multiple names are used to denote the same thing, </p></td></tr><tr><td>749</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_24">: A Benchmark Generator for Spatial Link Discovery Tools</a></td></tr><tr><td colspan=3><p>A number of real and synthetic benchmarks have been proposed for evaluating the performance of link discovery systems. So far, only a limited number of </p></td></tr><tr><td>750</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_22">Representativeness of Knowledge Bases with the Generalized Benford’s Law</a></td></tr><tr><td colspan=3><p>Knowledge bases (KBs) such as DBpedia, Wikidata, and YAGO contain a huge number of entities and facts. Several recent works induce rules or calculate statistics on these KBs. Most of these methods are based on the assumption that the data is a representative sample of the studied universe. Unfortunately, KBs are biased because they are built from crowdsourcing and opportunistic agglomeration of available databases. This paper aims at approximating the representativeness of a relation within a knowledge base. For this, we use the generalized Benford’s law, which indicates the distribution expected by the facts of a relation. We then compute the minimum number of facts that have to be added in order to make the KB representative of the real world. Experiments show that our unsupervised method applies to a large number of relations. For numerical relations where ground truths exist, the estimated representativeness proves to be a reliable indicator.</p></td></tr><tr><td>751</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_20">Certain Answers for SPARQL with Blank Nodes</a></td></tr><tr><td colspan=3><p>Blank nodes in RDF graphs can be used to represent values known to exist but whose identity remains unknown. A prominent example of such usage can be found in the Wikidata dataset where, e.g., the author of Beowulf is given as a blank node. However, while SPARQL considers blank nodes in a query as existentials, it treats blank nodes in RDF data more like constants. Running SPARQL queries over datasets with unknown values may thus lead to counter-intuitive results, which may make the standard SPARQL semantics unsuitable for datasets with existential blank nodes. We thus explore the feasibility of an alternative SPARQL semantics based on certain answers. In order to estimate the performance costs that would be associated with such a change in semantics for current implementations, we adapt and evaluate approximation techniques proposed in a relational database setting for a core fragment of SPARQL. To further understand the impact that such a change in semantics may have on query solutions, we analyse how this new semantics would affect the results of user queries over Wikidata.</p></td></tr><tr><td>752</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_18">GraFa: Scalable Faceted Browsing for RDF Graphs</a></td></tr><tr><td colspan=3><p>Faceted browsing has become a popular paradigm for user interfaces on the Web and has also been investigated in the context of RDF graphs. However, current faceted browsers for RDF graphs encounter performance issues when faced with two challenges: scale, where large datasets generate many results, and heterogeneity, where large numbers of properties and classes generate many facets. To address these challenges, we propose </p></td></tr><tr><td>753</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_17"> Question Answering Over CodeOntology</a></td></tr><tr><td colspan=3><p>We present an unsupervised approach to process natural language questions that cannot be answered by factual question answering nor advanced data querying, requiring instead ad-hoc code generation and execution. To address this challenging task, our system, </p></td></tr><tr><td>754</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_19">Semantics and Validation of Recursive SHACL</a></td></tr><tr><td colspan=3><p>With the popularity of RDF as an independent data model came the need for specifying constraints on RDF graphs, and for mechanisms to detect violations of such constraints. One of the most promising schema languages for RDF is SHACL, a recent W3C recommendation. Unfortunately, the specification of SHACL leaves open the problem of validation against recursive constraints. This omission is important because SHACL by design favors constraints that reference other ones, which in practice may easily yield reference cycles.</p></td></tr><tr><td>755</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_21">Efficient Handling of SPARQL OPTIONAL for OBDA</a></td></tr><tr><td colspan=3><p> is a key feature in SPARQL for dealing with missing information. While this operator is used extensively, it is also known for its complexity, which can make efficient evaluation of queries with </p></td></tr><tr><td>756</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_15">That’s Interesting, Tell Me More! Finding Descriptive Support Passages for Knowledge Graph Relationships</a></td></tr><tr><td colspan=3><p>We address the problem of finding descriptive explanations of facts stored in a knowledge graph. This is important in high-risk domains such as healthcare, intelligence, etc. where users need additional information for decision making and is especially crucial for applications that rely on automatically constructed knowledge graphs where machine-learned systems extract facts from an input corpus and working of the extractors is opaque to the end-user. We follow an approach inspired from information retrieval and propose a simple, yet effective and efficient solution that takes into account passage level as well as document level properties to produce a ranked list of passages describing a given input relation. We test our approach using Wikidata as the knowledge base and Wikipedia as the source corpus and report results of user studies conducted to study the effectiveness of our proposed model.</p></td></tr><tr><td>757</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_16">Exploring RDFS KBs Using Summaries</a></td></tr><tr><td colspan=3><p>Ontology summarization aspires to produce an abridged version of the original data source highlighting its most important concepts. However, in an ideal scenario, the user should not be limited only to static summaries. Starting from the summary, s/he should be able to further explore the data source requesting more detailed information for a particular part of it. In this paper, we present a new approach enabling the dynamic exploration of summaries through two novel operations </p></td></tr><tr><td>758</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_14">Structured Event Entity Resolution in Humanitarian Domains</a></td></tr><tr><td colspan=3><p>In domains such as humanitarian assistance and disaster relief (HADR), events, rather than named entities, are the primary focus of analysts and aid officials. An important problem that must be solved to provide situational awareness to aid providers is automatic clustering of sub-events that refer to the same underlying event. An effective solution to the problem requires judicious use of both domain-specific and semantic information, as well as statistical methods like deep neural embeddings. In this paper, we present an approach, AugSEER (Augmented feature sets for Structured Event Entity Resolution), that combines advances in deep neural embeddings both on text and graph data with minimally supervised inputs from domain experts. AugSEER can operate in both online and batch scenarios. On five real-world HADR datasets, AugSEER is found, on average, to outperform the next best baseline result by almost 15% on the cluster purity metric and by 3% on the F1-Measure metric. In contrast, text-based approaches are found to perform poorly, demonstrating the importance of semantic information in devising a good solution. We also use sub-event clustering visualizations to illustrate the qualitative potential of AugSEER.</p></td></tr><tr><td>759</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_13">Constructing a Recipe Web from Historical Newspapers</a></td></tr><tr><td colspan=3><p>Historical newspapers provide a lens on customs and habits of the past. For example, recipes published in newspapers highlight what and how we ate and thought about food. The challenge here is that newspaper data is often unstructured and highly varied. Digitised historical newspapers add an additional challenge, namely that of fluctuations in OCR quality. Therefore, it is difficult to locate and extract recipes from them. We present our approach based on distant supervision and automatically extracted lexicons to identify recipes in digitised historical newspapers, to generate recipe tags, and to extract ingredient information. We provide OCR quality indicators and their impact on the extraction process. We enrich the recipes with links to information on the ingredients. Our research shows how natural language processing, machine learning, and semantic web can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture.</p></td></tr><tr><td>760</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_12">QA4IE: A Question Answering Based Framework for Information Extraction</a></td></tr><tr><td colspan=3><p>Information Extraction (IE) refers to automatically extracting structured relation tuples from unstructured texts. Common IE solutions, including Relation Extraction (RE) and open IE systems, can hardly handle cross-sentence tuples, and are severely restricted by limited relation types as well as informal relation specifications (e.g., free-text based relation tuples). In order to overcome these weaknesses, we propose a novel IE framework named QA4IE, which leverages the flexible question answering (QA) approaches to produce high quality relation triples across sentences. Based on the framework, we develop a large IE benchmark with high quality human evaluation. This benchmark contains 293K documents, 2M golden relation triples, and 636 relation types. We compare our system with some IE baselines on our benchmark and the results show that our system achieves great improvements.</p></td></tr><tr><td>761</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_11">Enriching Knowledge Bases with Counting Quantifiers</a></td></tr><tr><td colspan=3><p>Information extraction traditionally focuses on extracting relations between identifiable entities, such as </p></td></tr><tr><td>762</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_7">EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Many question answering systems over knowledge graphs rely on entity and relation linking components in order to connect the natural language input to the underlying knowledge graph. Traditionally, entity linking and relation linking have been performed either as dependent sequential tasks or as independent parallel tasks. In this paper, we propose a framework called EARL, which performs entity linking and relation linking as a joint task. EARL implements two different solution strategies for which we provide a comparative analysis in this paper: The first strategy is a formalisation of the joint entity and relation linking tasks as an instance of the Generalised Travelling Salesman Problem (GTSP). In order to be computationally feasible, we employ approximate GTSP solvers. The second strategy uses machine learning in order to exploit the connection density between nodes in the knowledge graph. It relies on three base features and re-ranking steps in order to predict entities and relations. We compare the strategies and evaluate them on a dataset with 5000 questions. Both strategies significantly outperform the current state-of-the-art approaches for entity and relation linking.</p></td></tr><tr><td>763</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_9">An Ontology-Driven Probabilistic Soft Logic Approach to Improve NLP Entity Annotations</a></td></tr><tr><td colspan=3><p>Many approaches for Knowledge Extraction and Ontology Population rely on well-known Natural Language Processing (NLP) tasks, such as Named Entity Recognition and Classification (NERC) and Entity Linking (EL), to identify and semantically characterize the entities mentioned in natural language text. Despite being intrinsically related, the analyses performed by these tasks differ, and combining their output may result in NLP annotations that are implausible or even conflicting considering common world knowledge about entities. In this paper we present a Probabilistic Soft Logic (PSL) model that leverages ontological entity classes to relate NLP annotations from different tasks insisting on the same entity mentions. The intuition behind the model is that an annotation likely implies some ontological classes on the entity identified by the mention, and annotations from different tasks on the same mention have to share more or less the same implied entity classes. In a setting with various NLP tools returning multiple, confidence-weighted, candidate annotations on a single mention, the model can be operationally applied to compare the different annotation combinations, and to possibly revise the tools’ best annotation choice. We experimented applying the model with the candidate annotations produced by two state-of-the-art tools for NERC and EL, on three different datasets. The results show that the joint “a posteriori” annotation revision suggested by our PSL model consistently improves the original scores of the two tools.</p></td></tr><tr><td>764</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_10">Ontology Driven Extraction of Research Processes</a></td></tr><tr><td colspan=3><p>We address the automatic extraction from publications of two key concepts for representing research processes: the concept of research activity and the sequence relation between successive activities. These representations are driven by the Scholarly Ontology, specifically conceived for documenting research processes. Unlike usual named entity recognition and relation extraction tasks, we are facing textual descriptions of activities of widely variable length, while pairs of successive activities often span multiple sentences. We developed and experimented with several sliding window classifiers using Logistic Regression, SVMs, and Random Forests, as well as a two-stage pipeline classifier. Our classifiers employ task-specific features, as well as word, part-of-speech and dependency embeddings, engineered to exploit distinctive traits of research publications written in English. The extracted activities and sequences are associated with other relevant information from publication metadata and stored as RDF triples in a knowledge base. Evaluation on datasets from three disciplines, Digital Humanities, Bioinformatics, and Medicine, shows very promising performance.</p></td></tr><tr><td>765</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_8">TSE-NER: An Iterative Approach for Long-Tail Entity Extraction in Scientific Publications</a></td></tr><tr><td colspan=3><p>Named Entity Recognition and Typing (NER/NET) is a challenging task, especially with long-tail entities such as the ones found in scientific publications. These entities (e.g. “WebKB”,“StatSnowball”) are rare, often relevant only in specific knowledge domains, yet important for retrieval and exploration purposes. State-of-the-art NER approaches employ supervised machine learning models, trained on expensive type-labeled data laboriously produced by human annotators. A common workaround is the generation of labeled training data from knowledge bases; this approach is not suitable for long-tail entity types that are, by definition, scarcely represented in KBs. This paper presents an iterative approach for training NER and NET classifiers in scientific publications that relies on minimal human input, namely a small seed set of instances for the targeted entity type. We introduce different strategies for training data extraction, semantic expansion, and result entity filtering. We evaluate our approach on scientific publications, focusing on the long-tail entities types </p></td></tr><tr><td>766</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_4">Towards Encoding Time in Text-Based Entity Embeddings</a></td></tr><tr><td colspan=3><p>Knowledge Graphs (KG) are widely used abstractions to represent entity-centric knowledge. Approaches to embed entities, entity types and relations represented in the graph into vector spaces - often referred to as KG embeddings - have become increasingly popular for their ability to capture the similarity between entities and support other reasoning tasks. However, representation of time has received little attention in these approaches. In this work, we make a first step to encode time into vector-based entity representations using a text-based KG embedding model named Typed Entity Embeddings (TEEs). In TEEs, each entity is represented by a vector that represents the entity and its type, which is learned from entity mentions found in a text corpus. Inspired by evidence from cognitive sciences and application-oriented concerns, we propose an approach to encode representations of years into TEEs by aggregating the representations of the entities that occur in event-based descriptions of the years. These representations are used to define two time-aware similarity measures to control the implicit effect of time on entity similarity. Experimental results show that the linear order of years obtained using our model is highly correlated with natural time flow and the effectiveness of the time-aware similarity measure proposed to flatten the time effect on entity similarity.</p></td></tr><tr><td>767</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_6">A Novel Ensemble Method for Named Entity Recognition and Disambiguation Based on Neural Network</a></td></tr><tr><td colspan=3><p>Named entity recognition (NER) and disambiguation (NED) are subtasks of information extraction that aim to recognize named entities mentioned in text, to assign them pre-defined types, and to link them with their matching entities in a knowledge base. Many approaches, often exposed as web APIs, have been proposed to solve these tasks during the last years. These APIs classify entities using different taxonomies and disambiguate them with different knowledge bases. In this paper, we describe Ensemble Nerd, a framework that collects numerous extractors responses, normalizes them and combines them in order to produce a final entity list according to the pattern (surface form, type, link). The presented approach is based on representing the extractors responses as real-value vectors and on using them as input samples for two Deep Learning networks: ENNTR (Ensemble Neural Network for Type Recognition) and ENND (Ensemble Neural Network for Disambiguation). We train these networks using specific gold standards. We show that the models produced outperform each single extractor responses in terms of micro and macro F1 measures computed by the GERBIL framework.</p></td></tr><tr><td>768</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_2">Aligning Knowledge Base and Document Embedding Models Using Regularized Multi-Task Learning</a></td></tr><tr><td colspan=3><p>Knowledge Bases (KBs) and textual documents contain rich and complementary information about real-world objects, as well as relations among them. While text documents describe entities in freeform, KBs organizes such information in a structured way. This makes these two information representation forms hard to compare and integrate, limiting the possibility to use them jointly to improve predictive and analytical tasks. In this article, we study this problem, and we propose KADE, a solution based on a regularized multi-task learning of KB and document embeddings. KADE can potentially incorporate any KB and document embedding learning method. Our experiments on multiple datasets and methods show that KADE effectively aligns document and entities embeddings, while maintaining the characteristics of the embedding models.</p></td></tr><tr><td>769</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_5">Rule Learning from Knowledge Graphs Guided by Embedding Models</a></td></tr><tr><td colspan=3><p>Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as confidence reflect rule quality well when the KG is reasonably complete; however, these measures might be misleading otherwise. So it is difficult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules could be generated. Therefore, the ranking and pruning of candidate rules are major problems. To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.</p></td></tr><tr><td>770</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_3">Inducing Implicit Relations from Text Using Distantly Supervised Deep Nets</a></td></tr><tr><td colspan=3><p>Knowledge Base Population (KBP) is an important problem in Semantic Web research and a key requirement for successful adoption of semantic technologies in many applications. In this paper we present Socrates, a deep learning based solution for Automated Knowledge Base Population from Text. Socrates does not require manual annotations which would make the solution hard to adapt to a new domain. Instead, it exploits a partially populated knowledge base and a large corpus of text documents to train a set of deep neural network models. As a result of the training process, the system learns how to identify implicit relations between entities across a highly heterogeneous set of documents from various sources, making it suitable for large-scale knowledge extraction from Web documents. Main contributions of this paper include (a) a novel approach based on composite contexts to acquire implicit relations from Title Oriented Documents, and (b) an architecture for unifying relation extraction using binary, unary, and composite contexts. We provide an extensive evaluation of the system across three different benchmarks with different characteristics, showing that our unified framework can consistently outperform state of the art solutions. Remarkably, Socrates ranked first in both the knowledge base population and attribute validation track at the Semantic Web Challenge at ISWC 2017.</p></td></tr><tr><td>771</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_1">Fine-Grained Evaluation of Rule- and Embedding-Based Systems for Knowledge Graph Completion</a></td></tr><tr><td colspan=3><p>Over the recent years, embedding methods have attracted increasing focus as a means for knowledge graph completion. Similarly, rule-based systems have been studied for this task in the past. What is missing so far is a common evaluation that includes more than one type of method. We close this gap by comparing representatives of both types of systems in a frequently used evaluation protocol. Leveraging the explanatory qualities of rule-based systems, we present a fine-grained evaluation that gives insight into characteristics of the most popular datasets and points out the different strengths and shortcomings of the examined approaches. Our results show that models such as TransE, RESCAL or HolE have problems in solving certain types of completion tasks that can be solved by a rule-based approach with high precision. At the same time, there are other completion tasks that are difficult for rule-based systems. Motivated by these insights, we combine both families of approaches via ensemble learning. The results support our assumption that the two methods complement each other in a beneficial way.</p></td></tr><tr><td>772</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_12">A Semantic Wiki Based Light-Weight Web Application Model</a></td></tr><tr><td colspan=3><p>Wiki is a well-known Web 2.0 content management platform. The recent advance of semantic wikis enriches the conventional wikis by allowing users to edit and query structured semantic annotations (e.g., categories and typed links) beyond plain wiki text. This new feature provided by semantic wikis, as shown in this paper, enables a novel, transparent, and light-weight social Web application model. This model let developers collectively build Web applications using semantic wikis, including for data modeling, data management, data processing and data presentation. The source scripts and data of such applications are transparent to Web users. Beyond a generic description for the Web application model, we show two proof-of-concept prototypes, namely RPI Map and CNL (Controlled Natural Language) Wiki, both of which are based on Semantic MediaWiki (SMW).</p></td></tr><tr><td>773</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_14">Semantic-Linguistic Feature Vectors for Search: Unsupervised Construction and Experimental Validation</a></td></tr><tr><td colspan=3><p>In this paper, we elaborate on an approach to construction of semantic-linguistic feature vectors (FV) that are used in search. These FVs are built based on domain semantics encoded in an ontology and enhanced by a relevant terminology from Web documents. The value of this approach is twofold. First, it captures relevant semantics from an ontology, and second, it accounts for statistically significant collocations of other terms and phrases in relation to the ontology entities. The contribution of this paper is the FV construction process and its evaluation. Recommendations and lessons learnt are laid down.</p></td></tr><tr><td>774</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_13">Guidelines for the Specification and Design of Large-Scale Semantic Applications</a></td></tr><tr><td colspan=3><p>This paper presents a set of guidelines to help software engineers with the specification and design of large-scale semantic applications by defining new processes for </p></td></tr><tr><td>775</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_11">LODE: Linking Open Descriptions of Events</a></td></tr><tr><td colspan=3><p>People conventionally refer to an action or occurrence taking place at a certain time at a specific location as an </p></td></tr><tr><td>776</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_10">Merging and Ranking Answers in the Semantic Web: The Wisdom of Crowds</a></td></tr><tr><td colspan=3><p>In this paper we propose algorithms for combining and ranking answers from distributed heterogeneous data sources in the context of a multi-ontology Question Answering task. Our proposal includes a merging algorithm that aggregates, combines and filters ontology-based search results and three different ranking algorithms that sort the final answers according to different criteria such as popularity, confidence and semantic interpretation of results. An experimental evaluation on a large scale corpus indicates improvements in the quality of the search results with respect to a scenario where the merging and ranking algorithms were not applied. These collective methods for merging and ranking allow to answer questions that are distributed across ontologies, while at the same time, they can filter irrelevant answers, fuse similar answers together, and elicit the most accurate answer(s) to a question.</p></td></tr><tr><td>777</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_9">Deciding Query Entailment for Fuzzy </a></td></tr><tr><td colspan=3><p>Significant research efforts in the Semantic Web community are recently directed toward the representation and reasoning with fuzzy ontologies. As the theoretical counterpart of fuzzy ontology languages, fuzzy Description Logics (DLs) have attracted a wide range of concerns. With the emergence of a great number of large-scale domain ontologies, the basic reasoning services cannot meet the need of dealing with complex queries (mainly conjunctive queries), which are indispensable in data-intensive applications. Conjunctive queries (CQs), originated from relational databases, play an important role as an expressive reasoning service for ontologies. Since, however, the negation of a role atom in a CQ is not expressible as a part of a knowledge base, existing tableau algorithms cannot be used directly to deal with the issue. In this paper, we thus present a tableau-based algorithm for deciding query entailment of fuzzy conjunctive queries w.r.t. fuzzy </p></td></tr><tr><td>778</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_36">Understanding Semantic Web Applications</a></td></tr><tr><td colspan=3><p>Ten years have passed since the concept of the semantic web was proposed by Tim Berners-Lee. For these years, basic technologies for them such as RDF(S) and OWL were published. As a result, many systems using semantic technologies have been developed. Some of them are not prototype systems for researches but real systems for practical use. The authors analyzed semantic web applications published in the semantic web conferences (ISWC, ESWC, ASWC) and classified them based on ontological engineering. This paper is a review of application papers published in Semantic Web conferences. We discuss a trend and the future view of them using the results.</p></td></tr><tr><td>779</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_37">A Formal Model for Classifying Trusted Semantic Web Services</a></td></tr><tr><td colspan=3><p>Semantic Web Services (SWS) aim to alleviate Web service limitations, by combining Web service technologies with the potential of Semantic Web. Several open issues have to be tackled yet, in order to enable a safe and efficient Web services selection. One of them is represented by trust. In this paper, we introduce a trust definition and formalize a model for managing trust in SWS. The model approaches the selection of trusted Web services as a classification problem, and it is realized by an ontology, which extends WSMO. A prototype is deployed, in order to give a proof of concept of our approach.</p></td></tr><tr><td>780</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_31">Shining Light on Complex RDF Data through Advanced Data Visualization</a></td></tr><tr><td colspan=3><p>This Demonstration paper discusses ongoing work at Tom Sawyer Software in the area of advanced visualization and analysis of very large data sets, including RDF data. There is a growing imperative to explore large data sets as part of opportunity and threat analysis in areas such as national defense, financial risk analysis, market intelligence, and disease epidemiology. An increasing volume of this type of information is represented as RDF graphs. By visualizing and visually analyzing data, it is possible to see patterns, trends, and outliers in complex RDF graphs that would otherwise be difficult or even impossible to discover. Since RDF graphs are by nature difficult for humans to read, Tom Sawyer Software has been developing an innovative, graphical approach to defining the schema of RDF data, visualizing salient parts of the RDF graph, and integrating social network analysis into the visualization process to provide intuitive visual navigation, query, and understanding of information. This Demonstration paper discusses the underlying technology and its realization in sophisticated software for building advanced data visualization and analysis applications for making sense of large RDF graphs.</p></td></tr><tr><td>781</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_32">OntoRevision: A Plug-in System for Ontology Revision in Protégé</a></td></tr><tr><td colspan=3><p>Ontologies have been widely used in advanced information systems. However, it has been a challenging issue in ontology engineering to efficiently revise ontologies as new information becomes available. A novel method of revising ontologies has been proposed recently by Wang et al. However, related algorithms have not been implemented yet. In this article we describe an implementation of these algorithms called OntoRevision and report some experimental results. Our system is a plug-in for revising general ontologies in Protégé and thus can be used by Protégé users to revise ontologies automatically.</p></td></tr><tr><td>782</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_33">An Efficient Approach to Debugging Ontologies Based on Patterns</a></td></tr><tr><td colspan=3><p>Ontology debugging helps users to understand the unsatisfiability of a concept in an ontology by finding minimal unsatisfiability-preserving sub-ontologies (MUPS) of the ontology for the concept. Although existing approaches have shown good performance for some real life ontologies, they are still inefficient to handle ontologies that have many MUPS for an unsatisfiable concept. In this paper, we propose an efficient approach to debugging ontologies based on a set of patterns. Patterns provide general information to explain unsatisfiability but are not dependent on a specific ontology. In this approach, we make use of a set of heuristic strategies and construct a directed graph w.r.t. the hierarchies where the depth-first search strategy can be used to search paths. The experiments show that our approach has gained a significant improvement over the state of the art and can find considerable number of MUPS.</p></td></tr><tr><td>783</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_30">An Ontological Approach to Oracle BPM</a></td></tr><tr><td colspan=3><p>A modern business process management (BPM) operates using common tenants of an underlying Service Oriented Architecture (SOA) runtime infrastructure based on the Service Component Architecture (SCA) and supports the BPMN 2.0 OMG</p></td></tr><tr><td>784</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_29">An Abductive CQA Based Matchmaking System for Finding Renting Houses</a></td></tr><tr><td colspan=3><p>A matchmaking system for finding renting houses is required as the housing problem becomes serious in China and many people resort to rent a house. A semantic approach based on abductive conjunctive query answering (CQA) in Description Logic ontologies is exploited to provide more matches for a request about renting houses. Moreover, a matchmaking system based on this approach is developed. This demo will guide users to find suitable renting houses using this matchmaking system and show the advantages of the system.</p></td></tr><tr><td>785</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_29">Implementing LOD Surfer as a Search System for the Annotation of Multiple Protein Sequence Alignment</a></td></tr><tr><td colspan=3><p>Many life science databases have been provided as Linked Open Data (LOD). To promote the utilization of these databases, we had developed a method that can be referred to as LOD Surfer, that employed federated query search along a path of class–class relationships. In this study, we developed a specified version of the LOD Surfer for the annotation of multiple protein sequence alignment. The system comprised a web application programming interface (API) and a client system for the API. The web API provides a list of classes, and a list of paths between the classes that are specified by a user. The client presents the list of classes and the list of paths obtained from the API and assists a user in selecting classes and paths to acquire the required annotation of proteins. Additionally, the client system generates SPARQL queries to execute a federated query search for a selected path. During the development of the system, we can observe that (1) the client system should display some instances with human readable information because class selection is not an easy task for biological researchers, and (2) it is preferable that the client system stores paths that are selected by a user for reuse by other users because path selection may be time consuming at times and because the selected paths may be valuable for other researchers.</p></td></tr><tr><td>786</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_28">Integrated Semantic Model for Complex Disease Network</a></td></tr><tr><td colspan=3><p>To understand biological phenomena, biologists have identified the interactions between biological molecules in vivo. Until recently, all of the unique and interactive information of such molecules has been built into a database and made available online. Among them, there was an effort to understand the relationship of molecules based on biological pathways, and a standard model called BioPAX was made to enable interchange and operation of data. In particular, Pathway Commons integrates other biological data besides biological pathways using BioPAX. We are interested in identifying the molecular mechanisms of disease and recommending drugs for treatment. In addition to data provided by Pathway Commons, additional disease and drug data was added to be used in various analysis. We extended the model to express the data that BioPAX could not cover and converted all the data to RDF based on the model. We integrate and present diverse biological data using semantic technologies from the perspective of representing disease networks. We hope that this information will aid in a deeper understanding of disease and drug recommendations.</p></td></tr><tr><td>787</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_12">Estimation of Spatio-Temporal Missing Data for Expanding Urban LOD</a></td></tr><tr><td colspan=3><p>The illegal parking of bicycles has been an urban problem in Tokyo and other urban areas. We have sustainably built a Linked Open Data (LOD) relating to the illegal parking of bicycles (IPBLOD) to support the problem solving by raising social awareness. Also, we have estimated and complemented the temporally missing data to enrich the IPBLOD, which consisted of intermittent social-sensor data. However, there are also spatial missing data where a bicycle might be illegally parked, and it is necessary to estimate those data in order to expand the areas. Thus, we propose and evaluate a method for estimating spatially missing data. Specifically, we find stagnation points using computational fluid dynamics (CFD), and we filter the stagnation points based on popularity stakes that are calculated using Linked Data. As a result, a significant difference in between the baseline and our approach was represented using the chi-square test.</p></td></tr><tr><td>788</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_40">Ontolex JeuxDeMots and Its Alignment to the Linguistic Linked Open Data Cloud</a></td></tr><tr><td colspan=3><p>JeuxDeMots (JdM) is a rich collaborative lexical network in French, built on a crowdsourcing principle as a game with a purpose, represented in an ad-hoc tabular format. In the interest of reuse and interoperability, we propose a conversion algorithm for JdM following the Ontolex model, along with a word sense alignment algorithm, called JdMBabelizer, that anchors JdM sense-refinements to synsets in the </p></td></tr><tr><td>789</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_44">Cross-Lingual Infobox Alignment in Wikipedia Using Entity-Attribute Factor Graph</a></td></tr><tr><td colspan=3><p>Wikipedia infoboxes contain information about article entities in the form of attribute-value pairs, and are thus a very rich source of structured knowledge. However, as the different language versions of Wikipedia evolve independently, it is a promising but challenging problem to find correspondences between infobox attributes in different language editions. In this paper, we propose 8 effective features for cross lingual infobox attribute matching containing categories, templates, attribute labels and values. We propose entity-attribute factor graph to consider not only individual features but also the correlations among attribute pairs. Experiments on the two Wikipedia data sets of English-Chinese and English-French show that proposed approach can achieve high F1-measure: 85.5% and 85.4% respectively on the two data sets. Our proposed approach finds 23,923 new infobox attribute mappings between English and Chinese Wikipedia, and 31,576 between English and French based on no more than six thousand existing matched infobox attributes. We conduct an infobox completion experiment on English-Chinese Wikipedia and complement 76,498 (more than 30% of EN-ZH Wikipedia existing cross-lingual links) pairs of corresponding articles with more than one attribute-value pairs.</p></td></tr><tr><td>790</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_41">Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics</a></td></tr><tr><td colspan=3><p>Knowledge Graphs (KGs) effectively capture explicit relational knowledge about individual entities. However, visual attributes of those entities, like their shape and color and pragmatic aspects concerning their usage in natural language are not covered. Recent approaches encode such knowledge by learning latent representations (‘embeddings’) separately: In computer vision, visual object features are learned from large image collections and in computational linguistics, word embeddings are extracted from huge text corpora which capture their distributional semantics. We investigate the potential of complementing the relational knowledge captured in KG embeddings with knowledge from text documents and images by learning a shared latent representation that integrates information across those modalities. Our empirical results show that a joined concept representation provides measurable benefits for (i) semantic similarity benchmarks, since it shows a higher correlation with the human notion of similarity than uni- or bi-modal representations, and (ii) entity-type prediction tasks, since it clearly outperforms plain KG embeddings. These findings encourage further research towards capturing types of knowledge that go beyond today’s KGs.</p></td></tr><tr><td>791</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_43">Encoding Category Correlations into Bilingual Topic Modeling for Cross-Lingual Taxonomy Alignment</a></td></tr><tr><td colspan=3><p>Cross-lingual taxonomy alignment (CLTA) refers to mapping each category in the source taxonomy of one language onto a ranked list of most relevant categories in the target taxonomy of another language. Recently, vector similarities depending on bilingual topic models have achieved the state-of-the-art performance on CLTA. However, these models only model the textual context of categories, but ignore explicit category correlations, such as correlations between the categories and their co-occurring words in text or correlations among the categories of ancestor-descendant relationships in a taxonomy. In this paper, we propose a unified solution to encode category correlations into bilingual topic modeling for CLTA, which brings two novel category correlation based bilingual topic models, called </p></td></tr><tr><td>792</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_42">An Extension of SPARQL for Expressing Qualitative Preferences</a></td></tr><tr><td colspan=3><p>In this paper we present SPREFQL, an extension of the SPARQL language that allows appending a </p></td></tr><tr><td>793</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_38">Blockchain Enabled Privacy Audit Logs</a></td></tr><tr><td colspan=3><p>Privacy audit logs are used to capture the actions of participants in a data sharing environment in order for auditors to check compliance with privacy policies. However, collusion may occur between the auditors and participants to obfuscate actions that should be recorded in the audit logs. In this paper, we propose a Linked Data based method of utilizing blockchain technology to create tamper-proof audit logs that provide proof of log manipulation and non-repudiation. We also provide experimental validation of the scalability of our solution using an existing Linked Data privacy audit log model.</p></td></tr><tr><td>794</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_37">Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding</a></td></tr><tr><td colspan=3><p>Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.</p></td></tr><tr><td>795</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_39">VICKEY: Mining Conditional Keys on Knowledge Bases</a></td></tr><tr><td colspan=3><p>A conditional key is a key constraint that is valid in only a part of the data. In this paper, we show how such keys can be mined automatically on large knowledge bases (KBs). For this, we combine techniques from key mining with techniques from rule mining. We show that our method can scale to KBs of millions of facts. We also show that the conditional keys we mine can improve the quality of entity linking by up to 47% points.</p></td></tr><tr><td>796</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_33">Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine</a></td></tr><tr><td colspan=3><p>Real-time processing of data streams emanating from sensors is becoming a common task in Internet of Things scenarios. The key implementation goal consists in efficiently handling massive incoming data streams and supporting advanced data analytics services like anomaly detection. In an on-going, industrial project, a 24 / 7 available stream processing engine usually faces dynamically changing data and workload characteristics. These changes impact the engine’s performance and reliability. We propose Strider, a hybrid adaptive distributed RDF Stream Processing engine that optimizes logical query plan according to the state of data streams. Strider has been designed to guarantee important industrial properties such as scalability, high availability, fault tolerance, high throughput and acceptable latency. These guarantees are obtained by designing the engine’s architecture with state-of-the-art Apache components such as Spark and Kafka. We highlight the efficiency (</p></td></tr><tr><td>797</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_36">Investigating Learnability, User Performance, and Preferences of the Path Query Language SemwidgQL Compared to SPARQL</a></td></tr><tr><td colspan=3><p>In this paper, we present an empirical comparison of user performance and perceived usability for </p></td></tr><tr><td>798</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_35">Semantic Faceted Search with Aggregation and Recursion</a></td></tr><tr><td colspan=3><p>Faceted search is the de facto approach for exploration of data in e-commerce: it allows users to construct queries in an intuitive way without a prior knowledge of formal query languages. This approach has been recently adapted to the context of RDF. Existing faceted search systems however do not allow users to construct queries with aggregation and recursion which poses limitations in practice. In this work we extend faceted search over RDF with these functionalities and study the corresponding query language. In particular, we investigate complexity of the query answering and query containment problems.</p></td></tr><tr><td>799</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_31">Entity Comparison in RDF Graphs</a></td></tr><tr><td colspan=3><p>In many applications, there is an increasing need for the new types of RDF data analysis that are not covered by standard reasoning tasks such as SPARQL query answering. One such important analysis task is entity comparison, i.e., determining what are similarities and differences between two given entities in an RDF graph. For instance, in an RDF graph about drugs, we may want to compare Metamizole and Ibuprofen and automatically find out that they are similar in that they are both analgesics but, in contrast to Metamizole, Ibuprofen also has a considerable anti-inflammatory effect. Entity comparison is a widely used functionality available in many information systems, such as universities or product comparison websites. However, comparison is typically domain-specific and depends on a fixed set of aspects to compare. In this paper, we propose a formal framework for domain-independent entity comparison over RDF graphs. We model similarities and differences between entities as SPARQL queries satisfying certain additional properties, and propose algorithms for computing them.</p></td></tr><tr><td>800</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_32">Provenance Information in a Collaborative Knowledge Graph: An Evaluation of Wikidata External References</a></td></tr><tr><td colspan=3><p>Wikidata is a collaboratively-edited knowledge graph; it expresses knowledge in the form of subject-property-value triples, which can be enhanced with references to add provenance information. Understanding the quality of Wikidata is key to its widespread adoption as a knowledge resource. We analyse one aspect of Wikidata quality, provenance, in terms of relevance and authoritativeness of its external references. We follow a two-staged approach. First, we perform a crowdsourced evaluation of references. Second, we use the judgements collected in the first stage to train a machine learning model to predict reference quality on a large-scale. The features chosen for the models were related to reference editing and the semantics of the triples they referred to. </p></td></tr><tr><td>801</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_34">Mining Hypotheses from Data in OWL: Advanced Evaluation and Complete Construction</a></td></tr><tr><td colspan=3><p>Automated acquisition (learning) of ontologies from data has attracted research interest because it can complement manual, expensive construction of ontologies. We investigate the problem of General Terminology Induction in OWL, i.e. acquiring general, expressive TBox axioms (hypotheses) from an ABox (data). We define novel measures designed to rigorously evaluate the quality of hypotheses while respecting the standard semantics of OWL. We propose an informed, data-driven algorithm that constructs class expressions for hypotheses in OWL and guarantees completeness. We empirically evaluate the quality measures on two corpora of ontologies and run a case study with a domain expert to gain insight into applicability of the measures and acquired hypotheses. The results show that the measures capture different quality aspects and not only correct hypotheses can be interesting.</p></td></tr><tr><td>802</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_30">Completeness-Aware Rule Learning from Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts. They are widely used in entity recognition, structured search, question answering, and other important tasks. Rule mining is commonly applied to discover patterns in KGs. However, unlike in traditional association rule mining, KGs provide a setting with a high degree of </p></td></tr><tr><td>803</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_28">The </a></td></tr><tr><td colspan=3><p>Answering queries over a federation of SPARQL endpoints requires combining data from more than one data source. Optimizing queries in such scenarios is particularly challenging not only because of (i) the large variety of possible query execution plans that correctly answer the query but also because (ii) there is only limited access to statistics about schema and instance data of remote sources. To overcome these challenges, most federated query engines rely on heuristics to reduce the space of possible query execution plans or on dynamic programming strategies to produce optimal plans. Nevertheless, these plans may still exhibit a high number of intermediate results or high execution times because of heuristics and inaccurate cost estimations. In this paper, we present </p></td></tr><tr><td>804</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_29">Automated Fine-Grained Trust Assessment in Federated Knowledge Bases</a></td></tr><tr><td colspan=3><p>The federation of different data sources gained increasing attention due to the continuously growing amount of data. But the more data are available from heterogeneous sources, the higher the risk is of inconsistency. To tackle this challenge in federated knowledge bases we propose a fully automated approach for computing trust values at different levels of granularity. Gathering both the conflict graph and statistical evidence generated by inconsistency detection and resolution, we create a Markov network to facilitate the application of Gibbs sampling to compute a probability for each conflicting assertion. Based on which, trust values for each integrated data source and its respective signature elements are computed. We evaluate our approach on a large distributed dataset from the domain of library science.</p></td></tr><tr><td>805</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_27">Cost-Driven Ontology-Based Data Access</a></td></tr><tr><td colspan=3><p>SPARQL query answering in ontology-based data access (OBDA) is carried out by translating into SQL queries over the data source. Standard translation techniques try to transform the user query into a union of conjunctive queries (UCQ), following the heuristic argument that UCQs can be efficiently evaluated by modern relational database engines. In this work, we show that translating to UCQs is not always the best choice, and that, under certain conditions on the interplay between the ontology, the mappings, and the statistics of the data, alternative translations can be evaluated much more efficiently. To find the best translation, we devise a cost model together with a novel cardinality estimation that takes into account all such OBDA components. Our experiments confirm that </p></td></tr><tr><td>806</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_24">Alignment Cubes: Towards Interactive Visual Exploration and Evaluation of Multiple Ontology Alignments</a></td></tr><tr><td colspan=3><p>Ontology alignment is an area of active research where many algorithms and approaches are being developed. Their performance is usually evaluated by comparing the produced alignments to a reference alignment in terms of precision, recall and F-measure. These measures, however, only provide an overall assessment of the quality of the alignments, but do not reveal differences and commonalities between alignments at a finer-grained level such as, e.g., regions or individual mappings. Furthermore, reference alignments are often unavailable, which makes the comparative exploration of alignments at different levels of granularity even more important. Making such comparisons efficient calls for a “human-in-the-loop” approach, best supported through interactive visual representations of alignments. Our approach extends a recent tool, Matrix Cubes, used for visualizing dense dynamic networks. We first identify use cases for ontology alignment evaluation that can benefit from interactive visualization, and then detail how our </p></td></tr><tr><td>807</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_26">Reliable Granular References to Changing Linked Data</a></td></tr><tr><td colspan=3><p>Nanopublications are a concept to represent Linked Data in a granular and provenance-aware manner, which has been successfully applied to a number of scientific datasets. We demonstrated in previous work how we can establish reliable and verifiable identifiers for nanopublications and sets thereof. Further adoption of these techniques, however, was probably hindered by the fact that nanopublications can lead to an explosion in the number of triples due to auxiliary information about the structure of each nanopublication and repetitive provenance and metadata. We demonstrate here that this significant overhead disappears once we take the version history of nanopublication datasets into account, calculate incremental updates, and allow users to deal with the specific subsets they need. We show that the total size and overhead of evolving scientific datasets is reduced, and typical subsets that researchers use for their analyses can be referenced and retrieved efficiently with optimized precision, persistence, and reliability.</p></td></tr><tr><td>808</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_23">Language-Agnostic Relation Extraction from Wikipedia Abstracts</a></td></tr><tr><td colspan=3><p>Large-scale knowledge graphs, such as DBpedia, Wikidata, or YAGO, can be enhanced by relation extraction from text, using the data in the knowledge graph as training data, i.e., using </p></td></tr><tr><td>809</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_22">A Formal Framework for Comparing Linked Data Fragments</a></td></tr><tr><td colspan=3><p>The Linked Data Fragment (LDF) framework has been proposed as a uniform view to explore the trade-offs of consuming Linked Data when servers provide (possibly many) different interfaces to access their data. Every such interface has its own particular properties regarding performance, bandwidth needs, caching, etc. Several practical challenges arise. For example, before exposing a new type of LDFs in some server, can we formally say something about how this new LDF interface compares to other interfaces previously implemented in the same server? From the client side, given a client with some restricted capabilities in terms of time constraints, network connection, or computational power, which is the best type of LDFs to complete a given task? Today there are only a few formal theoretical tools to help answer these and other practical questions, and researchers have embarked in solving them mainly by experimentation.</p></td></tr><tr><td>810</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_25">Attributed Description Logics: Ontologies for Knowledge Graphs</a></td></tr><tr><td colspan=3><p>In modelling real-world knowledge, there often arises a need to represent and reason with meta-knowledge. To equip description logics (DLs) for dealing with such ontologies, we enrich DL concepts and roles with finite sets of attribute–value pairs, called annotations, and allow concept inclusions to express constraints on annotations. We show that this may lead to increased complexity or even undecidability, and we identify cases where this increased expressivity can be achieved without incurring increased complexity of reasoning. In particular, we describe a tractable fragment based on the lightweight description logic </p></td></tr><tr><td>811</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_20">AMUSE: Multilingual Semantic Parsing for Question Answering over Linked Data</a></td></tr><tr><td colspan=3><p>The task of answering natural language questions over RDF data has received wide interest in recent years, in particular in the context of the series of QALD benchmarks. The task consists of mapping a natural language question to an executable form, e.g. SPARQL, so that answers from a given KB can be extracted. So far, most systems proposed are (i) monolingual and (ii) rely on a set of hard-coded rules to interpret questions and map them into a SPARQL query. We present the first multilingual QALD pipeline that induces a model from training data for mapping a natural language question into logical form as probabilistic inference. In particular, our approach learns to map universal syntactic dependency representations to a language-independent logical form based on DUDES (Dependency-based Underspecified Discourse Representation Structures) that are then mapped to a SPARQL query as a deterministic second step. Our model builds on factor graphs that rely on features extracted from the dependency graph and corresponding semantic representations. We rely on approximate inference techniques, Markov Chain Monte Carlo methods in particular, as well as Sample Rank to update parameters using a ranking objective. Our focus lies on developing methods that overcome the lexical gap and present a novel combination of machine translation and word embedding approaches for this purpose. As a proof of concept for our approach, we evaluate our approach on the QALD-6 datasets for English, German & Spanish.</p></td></tr><tr><td>812</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_18">Meta Structures in Knowledge Graphs</a></td></tr><tr><td colspan=3><p>This paper investigates meta structures, schema-level graphs that abstract connectivity information among a set of entities in a knowledge graph. Meta structures are useful in a variety of knowledge discovery tasks ranging from relatedness explanation to data retrieval. We formalize the meta structure computation problem and devise efficient automata-based algorithms. We introduce a meta structure-based relevance measure, which can retrieve entities related to those in input. We implemented our machineries in a visual tool called MEKoNG. We report on an extensive experimental evaluation, which confirms the suitability of our proposal from both the efficiency and effectiveness point of view.</p></td></tr><tr><td>813</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_21">Computing FO-Rewritings in </a></td></tr><tr><td colspan=3><p>A prominent approach to implementing ontology-mediated queries (OMQs) is to rewrite into a first-order query, which is then executed using a conventional SQL database system. We consider the case where the ontology is formulated in the description logic </p></td></tr><tr><td>814</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_19">Challenges of Source Selection in the WoD</a></td></tr><tr><td colspan=3><p>Federated querying, the idea to execute queries over several distributed knowledge bases, lies at the core of the semantic web vision. To accommodate this vision, SPARQL provides the SERVICE keyword that allows one to allocate sub-queries to servers. In many cases, however, data may be available from multiple sources resulting in a combinatorially growing number of alternative allocations of subqueries to sources. Running a federated query on all possible sources might not be very lucrative from a user’s point of view if extensive execution times or fees are involved in accessing the sources’ data. To address this shortcoming, federated join-cardinality approximation techniques have been proposed to narrow down the number of possible allocations to a few most promising (or results-yielding) ones.</p></td></tr><tr><td>815</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_16">Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings</a></td></tr><tr><td colspan=3><p>Web tables constitute valuable sources of information for various applications, ranging from Web search to Knowledge Base (KB) augmentation. An underlying common requirement is to annotate the rows of Web tables with semantically rich descriptions of entities published in Web KBs. In this paper, we evaluate three unsupervised annotation methods: (a) a </p></td></tr><tr><td>816</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_13">LDScript: A Linked Data Script Language</a></td></tr><tr><td colspan=3><p>In addition to the existing standards dedicated to representation or querying, Semantic Web programmers could really benefit from a dedicated programming language enabling them to directly define functions on RDF terms, RDF graphs or SPARQL results. This is especially the case, for instance, when defining SPARQL extension functions. The ability to capitalize complex SPARQL filter expressions into extension functions or to define and reuse dedicated aggregates are real cases where a dedicated language can support modularity and maintenance of the code. Other families of use cases include the definition of </p></td></tr><tr><td>817</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_14">Practical Update Management in Ontology-Based Data Access</a></td></tr><tr><td colspan=3><p>Ontology-based Data Access (OBDA) is gaining importance both scientifically and practically. However, little attention has been paid so far to the problem of updating OBDA systems. This is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level, while maintaining the independence of the data sources. In this paper, we propose mechanisms to properly handle updates in this context. We show that updating data both at the ontology and source level is first-order rewritable. We also provide a practical implementation of such updating mechanisms based on non-recursive Datalog.</p></td></tr><tr><td>818</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_9">Semantic Wide and Deep Learning for Detecting Crisis-Information Categories on Social Media</a></td></tr><tr><td colspan=3><p>When crises hit, many flog to social media to share or consume information related to the event. Social media posts during crises tend to provide valuable reports on affected people, donation offers, help requests, advice provision, etc. Automatically identifying the category of information (e.g., </p></td></tr><tr><td>819</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_15">Computing Authoring Tests from Competency Questions: Experimental Validation</a></td></tr><tr><td colspan=3><p>This paper explores whether Authoring Tests derived from Competency Questions accurately represent the expectations of ontology authors. In earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given Competency Question (CQ) is able to be answered by the ontology at a given stage of its construction, an approach known as CQ-driven Ontology Authoring (CQOA). The experiments presented in the present paper suggest that CQOA’s understanding of CQs matches users’ understanding quite well, especially for inexperienced ontology authors.</p></td></tr><tr><td>820</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_17">Learning Commonalities in SPARQL</a></td></tr><tr><td colspan=3><p>Finding the commonalities between descriptions of data or knowledge is a foundational reasoning problem of Machine Learning. It was formalized in the early 70’s as computing a </p></td></tr><tr><td>821</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_12">Global RDF Vector Space Embeddings</a></td></tr><tr><td colspan=3><p>Vector space embeddings have been shown to perform well when using RDF data in data mining and machine learning tasks. Existing approaches, such as RDF2Vec, use </p></td></tr><tr><td>822</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_11">Zooming in on Ontologies: Minimal Modules and Best Excerpts</a></td></tr><tr><td colspan=3><p>Ensuring access to the most relevant knowledge contained in large ontologies has been identified as an important challenge. To this end, minimal modules (sub-ontologies that preserve all entailments over a given vocabulary) and excerpts (certain, small number of axioms that best capture the knowledge regarding the vocabulary by allowing for a degree of semantic loss) have been proposed. In this paper, we introduce the notion of subsumption justification as an extension of justification (a minimal set of axioms needed to preserve a logical consequence) to capture the subsumption knowledge between a term and all other terms in the vocabulary. We present algorithms for computing subsumption justifications based on a simulation notion developed for the problem of deciding the logical difference between ontologies. We show how subsumption justifications can be used to obtain minimal modules and to compute best excerpts by additionally employing a partial Max-SAT solver. This yields two state-of-the-art methods for computing all minimal modules and all best excerpts, which we evaluate over large biomedical ontologies.</p></td></tr><tr><td>823</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_7">Semantics and Validation of Shapes Schemas for RDF</a></td></tr><tr><td colspan=3><p>We present a formal semantics and proof of soundness for shapes schemas, an expressive schema language for RDF graphs that is the foundation of Shape Expressions Language 2.0. It can be used to describe the vocabulary and the structure of an RDF graph, and to constrain the admissible properties and values for nodes in that graph. The language defines a typing mechanism called shapes against which nodes of the graph can be checked. It includes an algebraic grouping operator, a choice operator and cardinality constraints for the number of allowed occurrences of a property. Shapes can be combined using Boolean operators, and can use possibly recursive references to other shapes.</p></td></tr><tr><td>824</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_10">Tractable Query Answering for Expressive Ontologies and Existential Rules</a></td></tr><tr><td colspan=3><p>The disjunctive skolem chase is a sound and complete (albeit non-terminating) algorithm that can be used to solve conjunctive query answering over DL ontologies and programs with disjunctive existential rules. Even though acyclicity notions can be used to ensure chase termination for a large subset of real-world knowledge bases, the complexity of reasoning over acyclic theories still remains high. Hence, we study several restrictions which not only guarantee chase termination but also ensure polynomiality. We include an evaluation that shows that almost all acyclic DL ontologies do indeed satisfy these general restrictions.</p></td></tr><tr><td>825</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_6">Expressive Stream Reasoning with Laser</a></td></tr><tr><td colspan=3><p>An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases.</p></td></tr><tr><td>826</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_4">Improving Visual Relationship Detection Using Semantic Modeling of Scene Descriptions</a></td></tr><tr><td colspan=3><p>Structured scene descriptions of images are useful for the automatic processing and querying of large image databases. We show how the combination of a statistical semantic model and a visual model can improve on the task of mapping images to their associated scene description. In this paper we consider scene descriptions which are represented as a set of triples (</p></td></tr><tr><td>827</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_5">An Empirical Study on How the Distribution of Ontologies Affects Reasoning on the Web</a></td></tr><tr><td colspan=3><p>The Web of Data is an inherently distributed environment where ontologies are located in (physically) remote locations and are subject to constant changes. Reasoning is affected by these changes, but the extent and significance of this dependency is not well-studied yet. To address this problem, this paper presents an empirical study on how the distribution of ontological data on the Web affects the outcome of reasoning. We study (1) to what degree datasets depend on external ontologies and (2) to what extent the inclusion of additional ontological information via IRI de-referencing and the </p></td></tr><tr><td>828</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_8">Temporal Query Answering in DL-Lite over Inconsistent Data</a></td></tr><tr><td colspan=3><p>In ontology-based systems that process data stemming from different sources and that is received over time, as in context-aware systems, reasoning needs to cope with the temporal dimension and should be resilient against inconsistencies in the data. Motivated by such settings, this paper addresses the problem of handling inconsistent data in a temporal version of ontology-based query answering. We consider a recently proposed temporal query language that combines conjunctive queries with operators of propositional linear temporal logic and extend to this setting three inconsistency-tolerant semantics that have been introduced for querying inconsistent description logic knowledge bases. We investigate their complexity for DL-Lite</p></td></tr><tr><td>829</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_10">WC3: Analyzing the Style of Metadata Annotation Among Wikipedia Articles by Using Wikipedia Category and the DBpedia Metadata Database</a></td></tr><tr><td colspan=3><p>WC3 (Wikipedia Category Consistency Checker) is a system that supports the analysis of the metadata-annotation style in Wikipedia articles belonging to a particular Wikipedia category (the subcategory of “Categories by parameter”) by using the DBpedia metadata database. This system aims to construct an appropriate SPARQL query to represent the category and compares the retrieved results and articles that belong to the category. In this paper, we introduce WC3 and extend the algorithm to analyze efficiently additional varieties of Wikipedia category. We also discuss the metadata-annotation quality of the Wikipedia by using WC3. URL of WC3 is </p></td></tr><tr><td>830</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_3">A Decidable Very Expressive Description Logic for Databases</a></td></tr><tr><td colspan=3><p>We introduce </p></td></tr><tr><td>831</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_2">The Efficacy of OWL and DL on User Understanding of Axioms and Their Entailments</a></td></tr><tr><td colspan=3><p>OWL is recognized as the de facto standard notation for ontology engineering. The Manchester OWL Syntax (MOS) was developed as an alternative to symbolic description logic (DL) and it is believed to be more effective for users. This paper sets out to test that belief from two perspectives by evaluating how accurately and quickly people </p></td></tr><tr><td>832</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_1">Multi-label Based Learning for Better Multi-criteria Ranking of Ontology Reasoners</a></td></tr><tr><td colspan=3><p>A growing number of highly optimized reasoning algorithms have been developed to allow inference tasks on expressive ontology languages such as OWL(DL). Nevertheless, there is broad agreement that a reasoner could be optimized for some, but not all the ontologies. This particular fact makes it hard to select the best performing reasoner to handle a given ontology, especially for novice users. In this paper, we present a novel method to support the selection ontology reasoners. Our method generates a recommendation in the form of reasoner ranking. The efficiency as well as the correctness are our main ranking criteria. Our solution combines and adjusts multi-label classification and multi-target regression techniques. A large collection of ontologies and 10 well-known reasoners are studied. The experimental results show that the proposed method performs significantly better than several state-of-the-art ranking solutions. Furthermore, it proves that our introduced ranking method could effectively be evolved to a competitive meta-reasoner.</p></td></tr><tr><td>833</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_8">Chainable and Extendable Knowledge Integration Web Services</a></td></tr><tr><td colspan=3><p>This paper introduces the current state of the FREME framework. The paper puts FREME into the context of linguistic linked data and related approaches of multilingual and semantic processing. In addition, we focus on two specific aspects of FREME: the FREME NER e-Service, and chaining of FREME e-Services. We believe that the flexible and distributed combination of e-Services bears a potential for their mutual improvement. The FREME framework is an open source software available for free download (</p></td></tr><tr><td>834</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_7">Extracting Process Graphs from Medical Text Data</a></td></tr><tr><td colspan=3><p>In this paper a natural language processing workflow to extract sequential activities from large collections of medical text documents is developed. A graph-based data structure is introduced to merge extracted sequences which contain similar activities in order to build a global graph on procedures which are described in documents on similar topics or tasks. The method describes an information extraction process which will, in the future, enrich or create knowledge bases for process models or activity sequences for the medical domain.</p></td></tr><tr><td>835</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_9">Entity Typing Using Distributional Semantics and DBpedia</a></td></tr><tr><td colspan=3><p>Recognising entities in a text and linking them to an external resource is a vital step in creating a structured resource (e.g. a knowledge base) from text. This allows semantic querying over a dataset, for example selecting all politicians or football players. However, traditional named entity recognition systems only distinguish a limited number of entity types (such as Person, Organisation and Location) and entity linking has the limitation that often not all entities found in a text can be linked to a knowledge base. This creates a gap in coverage between what is in the text and what can be annotated with fine grained types.</p></td></tr><tr><td>836</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_6">Identifying Poorly-Defined Concepts in WordNet with Graph Metrics</a></td></tr><tr><td colspan=3><p>Princeton WordNet is the most widely-used lexical resource in natural language processing and continues to provide a gold standard model of semantics. However, there are still significant quality issues with the resource and these affect the performance of all NLP systems built on this resource. One major issue is that many nodes are insufficiently defined and new links need to be added to increase performance in NLP. We combine the use of graph-based metrics with measures of ambiguity in order to predict which synsets are difficult for word sense disambiguation, a major NLP task, which is dependent on good lexical information. We show that this method allows use to find poorly defined nodes with a 89.9% precision, which would assist manual annotators to focus on improving the most in-need parts of the WordNet graph.</p></td></tr><tr><td>837</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_4">Wikipedia and DBpedia for Media - Managing Audiovisual Resources in Their Semantic Context</a></td></tr><tr><td colspan=3><p>
The EBU, NRK and VRT are three European media companies. The EBU is the largest association of broadcasters. The NRK and VRT are the national public broadcasters in Norway and in Belgium (Flemish). The EBU, NRK and VRT are known in the media community for striving innovation. They have developed recognised expertise in engineering solutions and standards around the management of information for the audiovisual industry in a multi-lingual environment. They promote the use of semantic technologies for the production and distribution of content across a variety of media and platforms. In this context, Wikipedia, DBpedia, automatic metadata extraction and other tools are important information sources. This is not an academic paper but a report on the operational use of such information (access, usability, long term availability of information, editorial quality) and needs by the media industry. For broadcasters, minimizing cost and complexity is of the essence!
</p></td></tr><tr><td>838</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_5">Identifying Global Representative Classes of DBpedia Ontology Through Multilingual Analysis: A Rank Aggregation Approach</a></td></tr><tr><td colspan=3><p>Identifying the global representative parts from the multilingual pivotal ontology is important for integrating local language resources into Linked Data. We present a novel method of identifying global representative classes of DBpedia ontology based on the collective popularity, calculated by the aggregation of ranking orders from Wikipedia’s local language editions. We publish the contents of this paper on </p></td></tr><tr><td>839</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_1">Knowledge Graphs: Venturing Out into the Wild</a></td></tr><tr><td colspan=3><p>While we now have vast collections of knowledge at our disposal, it appears that our systems often need further kinds of knowledge that are still missing in most knowledge graphs. This paper argues that we need keep moving further beyond simple collections of encyclopedic facts. Three key directions are (1) aiming at more tightly integrated knowledge, (2) distilling knowledge from text and other unstructured data, and (3) moving towards cognitive and neural approaches to better exploit the available knowledge in intelligent applications.</p></td></tr><tr><td>840</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_3">Statistical Induction of Coupled Domain/Range Restrictions from RDF Knowledge Bases</a></td></tr><tr><td colspan=3><p>Statistical Schema Induction can be applied on an RDF dataset to induce domain and range restrictions. We extend an existing approach that derives independent domain and range restrictions to derive coupled domain/range restrictions, which may be beneficial in the context of Natural Language Processing tasks such as Semantic Parsing and Entity Classification. We provide results from an experiment on the DBpedia graph. An evaluation shows that high precision can be achieved. Code and data are available at </p></td></tr><tr><td>841</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_27">Semantic Navigation of Disease-Specific Pathways: The Case of Non-small Cell Lung Cancer (NSCLC)</a></td></tr><tr><td colspan=3><p>By studying the cancer genome, scientists can discover what base changes are causing a cell to become a cancer cell. In addition, cancers and diseases are affected by a series of complex interactions between a multitude of entities such as genes and proteins. Biological pathway analysis became necessary to understand these entities within diverse contexts. In this paper, we propose a framework for researchers to navigate disease-specific pathways. The basic structure of analysis data is BioPAX which is described in RDF and is produced by the Reactome database (biological pathway database). For this framework, we utilize a large scale of biological sources such as Pathway Commons, clinical data, dbSNP, and ClinVar. Especially, we choose non-small cell lung cancer (NSCLC) for case study to demonstrate components of semantic navigation. Furthermore, we generate and analyze non-small cell lung cancer (NSCLC) specific pathways. Our proposed system will help researchers find a point at which they begin their interests. For instance, it can help discover which protein or gene most affect a specific disease or it can aid in integrating different sources of biological information. Moreover, plenty of biological data extended by our system suggests a new perspective for scientists to find a direction of research.</p></td></tr><tr><td>842</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_2">Information Extraction from the Web by Matching Visual Presentation Patterns</a></td></tr><tr><td colspan=3><p>The documents available in the World Wide Web contain large amounts of information presented in tables, lists or other visually regular structures. The published information is however usually not annotated explicitly or implicitly and its interpretation is left on a human reader. This makes the information extraction from web documents a challenging problem. Most existing approaches are based on a top-down approach that proceeds from the larger page regions to individual data records, which depends on different heuristics. We present an opposite bottom-up approach. We roughly identify the smallest data fields in the document and later, we refine this approximation by matching the discovered visual presentation patterns with the expected semantic structure of the extracted information. This approach allows to efficiently extract structured data from heterogeneous documents without any kind of additional annotations as we demonstrate experimentally on various application domains.</p></td></tr><tr><td>843</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_26">Inference of Functions, Roles, and Applications of Chemicals Using Linked Open Data and Ontologies</a></td></tr><tr><td colspan=3><p>A simple method to efficiently collect reliable chemical information was studied for developing an ontological foundation. Even ChEBI, a major chemical ontology, which consists of approximately 90,000 chemicals and information about 1,000 biological and chemical roles, and applications, lacks information regarding the roles of most of the chemicals. NikkajiRDF, linked open data which provide information of approximately 3.5 million chemicals and 694 application examples, is also being developed. NikkajiRDF was integrated with Interlinking Ontology for Biological Concepts (IOBC), which includes 80,000 concepts, including information on a number of diseases and drugs. As a result, it was possible to infer new information on at least one of the 432 biological and chemical functions, applications and involvements with biological phenomena, including diseases to 5,038 chemicals using IOBC’s ontological structure. Furthermore, seven chemicals and drugs, which would be involved in 16 diseases, were discovered using knowledge graphs that were developed from IOBC.</p></td></tr><tr><td>844</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_23">IMI: A Common Vocabulary Framework for Open Government Data</a></td></tr><tr><td colspan=3><p>Making meanings of terms used in government systems exchangeable is important to enhance semantic interoperability between systems and promote use of open government data. IMI is an interoperability framework for digital government and open government data in Japan. The IMI common vocabulary framework which is a part of IMI since 2013 aims to provide a mechanism for sharing meanings of terms and relationships between terms to enhance interoperability. This paper describes the current status of the IMI common vocabulary framework to share our experiences and knowledge from the development in five years. At first we illustrate the IMI common vocabulary and its core vocabulary which includes a basic set of terms used in data or referred from existing data to share meanings. Then we describe specifications of components in the common vocabulary framework like data exchange formats, notations and a package system. As the IMI common vocabulary has been already used in various areas, we also introduce its deployment support and real use cases of the common vocabulary framework.</p></td></tr><tr><td>845</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_24">A Conceptual Framework for Linking Open Government Data Based-On Geolocation: A Case of Thailand</a></td></tr><tr><td colspan=3><p>Over the past decade, most governments have steadily progressed towards a policy for more openness, more accountability and more transparency. Such a strategy to publish open data, which are meaningful and valuable, has made available open government data (OGD) that are publicly accessible to everyone. To promote OGD usage, most OGD datasets are published in a tabular form or a CSV spreadsheet format, which can be easily browsed and downloaded by a human user. However, applications of OGD often require data from different datasets to be integrated. This is a challenging and cumbersome task which usually demand huge human effort, especially if metadata as well as data representation and encoding standards are not well defined. With a thorough analysis into Thailand’s OGD (ThOGD) having over thousand datasets, we found that OGD datasets often involve data related to geolocation, places or administrative division. Therefore, using such geodata as potential linking nodes is very attractive. However, this is not an easy task due to data heterogeneity issues. For example, a location might be represented using a geographic coordination system (e.g., latitude and longitude) or an administrative division which could be in a different level from highest to lowest division such as regions, provinces, districts, municipalities, etc.). Moreover, in Thailand geographical regions can be divided differently by different division schemes depending on the application domains, e.g., meteorology, tourism and statistics. To tackle this challenge, in this paper, we propose a conceptual framework for mapping and linking OGD datasets using geolocation data which could increase OGD usage and promote the development of new services or applications.</p></td></tr><tr><td>846</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_22">Building the Core Vocabulary of Crop Names to Integrate the Vocabularies by Multiple Government Agencies</a></td></tr><tr><td colspan=3><p>Since agriculture is the oldest industry in our society and the basis of our life and economics, knowledge of crops as product of agriculture is also old and spread all over the society. As a result, the names of crops are messy and sometimes inconsistent. It is problematic in the digital and Internet era since interoperability is not assured. In this paper, we proposes Crop Vocabulary (CVO) as the core vocabulary of crop names to solve interoperability and machine-readability on crop names. There are many vocabularies about crops by various food chain stakeholders. Here we picked up three vocabularies issued by Japanese government with respect to food security, namely the Agricultural chemical use reference, the Agricultural chemical residue reference, and the Food composition database, since food security is the primary concern of all food chain stakeholders including farmers and consumers. As the result of comparative analysis of these vocabularies, we defined the concept of crop as the botanical information such as the scientific name of species with additional information such as edible parts, cultivation methods and usage. According to the definition, we investigated these three vocabularies and identified 1,249 crops with unique names. The element of CVO contains the information about the crop itself such as synonym, English name and scientific name as well as links to names in the above-mentioned vocabularies and other external vocabularies such as Wikipedia, AGROVOC and NCBI Taxonomy. We develop web-based API for CVO and an application for the farm management as an example. The value of CVO as a core vocabulary in the field of agriculture is identified by compatibility with existing vocabularies and its usefulness is demonstrated by web services applications developed based on CVO.</p></td></tr><tr><td>847</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_21">Publication of Statistical Linked Open Data in Japan</a></td></tr><tr><td colspan=3><p>The Japanese Statistics Center began publishing a statistical linked open data (LOD) site in 2016. The data currently consists of approximately 1.3 billion triples. The publication of statistical data as LOD enables datasets and categorizations to be clarified. This allows users not only to search objective data easily, but also to combine the data with other domestic or international data. This paper first introduces a design policy for LOD and a method for representing geographic areas. Then, it explains the method used to query the LOD by using SPARQL or GeoSPARQL, and provides one example application.</p></td></tr><tr><td>848</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_20">Making Complex Ontologies End User Accessible via Ontology Projections</a></td></tr><tr><td colspan=3><p>Ontologies are a powerful mechanism to structure domains of interest. They have successfully been applied in medical domain, industry and other important areas. Despite the simplicity of ontological vocabularies that consist of classes and properties, ontologies can relate elements of the vocabulary with the help of axioms in a very non-trivial way. Thus, the relationship between classes and properties can become hardly accessible by end users thus affecting the practical value of ontologies. Indeed, it is essential for end users to be able to navigate or browse through an ontology, to get a big picture of what classes there are and what they have in common in terms of other related classes and properties. This helps end users in effectively performing various knowledge engineering tasks such as querying and domain exploration. To this end, in this short paper, we describe an approach to project OWL 2 ontologies into graphs and show how to leverage this approach in practical systems for visual query formulation and faceted search that we tested in various scenarios.</p></td></tr><tr><td>849</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_25">Construction and Reuse of Linked Agriculture Data: An Experience of Taiwan Government Open Data</a></td></tr><tr><td colspan=3><p>This paper describes our experiences on dealing with the transformation from Traceable Agriculture Product (TAP) records to Linked Open Government Data. By using existing ontologies and vocabularies, TAP Ontology is developed for clarifying the semantics of TAP. To increase the reusability of TAP, the crops and operational processing details of TAP are mapped to Common Agricultural Vocabulary (CAVOC). There are four SPARQL endpoints developed for supporting queries to TAP. To demonstrate the reuse of Linked TAP, we develop a Chrome extension LinkedFood to offer TAP information via reading ingredients in recipe websites.</p></td></tr><tr><td>850</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_18">Automatic Ontology Development from Semi-structured Data in Web-Portal: Towards Ontology of Thai Rice Knowledge</a></td></tr><tr><td colspan=3><p>Heavyweight ontology is difficult to develop even for experienced ontology engineer, but it is required for semantic based computer software as core knowledge. Most of existing automated ontology development methods however focuses on lightweight ontology, taxonomy-instance extraction. This work presents a method to automatically construct relation-heavy ontology from semi-structured web content providing deep knowledge in specific domain. Classes, instances and hierarchical relation are derived from the category content from the web. Relations are extracted based on frequent expression details. Templates of relation and its range are extracted from common content with partial difference. Similar contexts are grouped with similarity and form as relation to attach to ontology classes. The case study of this work is Thai rice knowledge including rice variety, disease, weed and pest provided in website from responsible government. The complete ontology is used as core knowledge for personalised web service. The service assists in filter content in summary that matched to users’ information. Courtesy to the generated relation-heavy ontology, it is able to recommend relevant chained concepts to users based on semantic relation. From evaluation from an expert, the generated ontology obtained about 97% accuracy.</p></td></tr><tr><td>851</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_19">Semantic Diagnostics of Smart Factories</a></td></tr><tr><td colspan=3><p>Smart factories are one of the biggest trends in modern manufacturing, also known as Industry 4.0. They reach a new level of process automation and make heavy use of sensors in manufactoring equipment, which brings new challenges to monitoring and diagnostics at smart factories. We propose to address the challenges with a novel rule-based monitoring and diagnostics language that relies on ontologies and reasoning and allows one to write diagnostic tasks at a high level of abstraction. We show that our approach speeds up the diagnostic routine of engineers at Siemens: they can formulate and deploy diagnostic tasks in factories faster than with existing Siemens data-driven solutions. Moreover we show that our diagnostic language, despite the built-in reasoning, allows for efficient execution of diagnostic tasks over large volumes of industrial data. Finally, we implemented our ideas in a prototypical diagnostic system for smart factories.</p></td></tr><tr><td>852</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_16">DeFind: A Protege Plugin for Computing Concept Definitions in </a></td></tr><tr><td colspan=3><p>We introduce an extension to the Protégé ontology editor, which allows for discovering concept definitions, which are not explicitly present in axioms, but are logically implied by an ontology. The plugin supports ontologies formulated in the Description Logic </p></td></tr><tr><td>853</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_15">A Quantitative Evaluation of Natural Language Question Interpretation for Question Answering Systems</a></td></tr><tr><td colspan=3><p>Systematic benchmark evaluation plays an important role in the process of improving technologies for Question Answering (QA) systems. While currently there are a number of existing evaluation methods for natural language (NL) QA systems, most of them consider only the final answers, limiting their utility within a black box style evaluation. Herein, we propose a subdivided evaluation approach to enable finer-grained evaluation of QA systems, and present an evaluation tool which targets the NL question (NLQ) interpretation step, an initial step of a QA pipeline. The results of experiments using two public benchmark datasets suggest that we can get a deeper insight about the performance of a QA system using the proposed approach, which should provide a better guidance for improving the systems, than using black box style approaches.</p></td></tr><tr><td>854</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_17">A Graph-Based Method for Interactive Mapping Revision</a></td></tr><tr><td colspan=3><p>Discovering semantic relations between heterogeneous ontologies is one of the key research topics in the Semantic Web. As the matching strategies adopted are largely heuristic, wrong mappings often exist in alignments generated by ontology matching systems. The mainstream methods for mapping revision deal with logical inconsistencies, so erroneous mappings not causing an inconsistency may be left out. Therefore, manual validations from domain experts are required. In this paper, we propose a graph-based method for interactive mapping revision with the purpose of reducing manual efforts as much as possible. Source ontologies are encoded into an integrated graph, where its mapping arcs are obtained by transforming mappings and will be evaluated by the expert. We specify the decision space for mapping revision and the corresponding operations that can be applied in the graph. After a human decision is made in each interaction, the mappings entailed by the manually confirmed ones are automatically approved. Conversely, those that would entail the rejected mappings or make the graph incoherent are declined. The whole update process modeled in the decision space can be done in polynomial time. Moreover, we define an impact function based on the integrated graph to identify the most influential mappings that will be displayed to the expert. In this way, the efforts of manual evaluation could be reduced further. The experiment on real-world ontology alignments shows that our method can save more decisions made by the expert than other revisions in most cases.</p></td></tr><tr><td>855</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_13">Leveraging Part-of-Speech Tagging for Sentiment Analysis in Short Texts and Regular Texts</a></td></tr><tr><td colspan=3><p>Sentiment analysis has been approached from a spectrum of methodologies, including statistical learning using labelled corpus and rule-based approach where rules may be constructed based on the observations on the lexicons as well as the output from natural language processing tools. In this paper, the experiments to transform labelled datasets by using NLP tools and subsequently performing sentiment analysis via statistical learning algorithms are detailed. In addition to the common data pre-processing prior to sentiment analysis, we represent the tokens in the datasets using Part-Of-Speech (POS) tags. The aim of the experiments is to investigate the impact of POS tags on sentiment analysis, particularly on both short texts and regular texts. The experimental results on short texts show that the combination of adjective and adverb predicts the sentiment of short texts the best. While noun is generally deemed to be neutral in sentiment polarity, the experimental results show that it helps to increase the accuracy of sentiment analysis on regular texts. Besides, the role of negation analysis in the datasets has also been investigated and reported based on the experimental results obtained.</p></td></tr><tr><td>856</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_11">Predicate Invention Based RDF Data Compression</a></td></tr><tr><td colspan=3><p>RDF is a data representation format for schema-free structured information that is gaining speed in the context of semantic web, life science, and vice versa. With the continuing proliferation of structured data, demand for RDF compression is becoming increasingly important. In this study, we introduce a novel lossless compression technique for RDF datasets (triples), called PIC (Predicate Invention based Compression). By generating informative predicates and constructing effective mapping to original predicates, PIC only needs to store dramatically reduced number of triples with the newly created predicates, and restoring the original triples efficiently using the mapping. These predicates are automatically generated by a decomposable forward-backward procedure, which consequently supports very fast parallel bit computation. As a semantic compression method for structured data, besides the reduction of syntactic verbosity and data redundancy, we also invoke semantics in the RDF datasets. Experiments on various datasets show competitive results in terms of compression ratio.</p></td></tr><tr><td>857</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_14">A Methodology for a Criminal Law and Procedure Ontology for Legal Question Answering</a></td></tr><tr><td colspan=3><p>The Internet and the development of the semantic web have created the opportunity to provide structured legal data on the web. However, most legal information is in text. It is difficult to automatically determine the right natural language answer about the law to a given natural language question. One approach is to develop systems of legal ontologies and rules. Our example ontology represents semantic information about USA criminal law and procedure as well as the applicable legal rules. The purpose of the ontology is to provide reasoning support to an legal question answering tool that determines entailment between a pair of texts, one known as the Background information (Bg) and the other Question statement (Q), whether Bg entails Q based on the application of the law. The key contribution of this paper is a clear and well-structured methodology that serves to develop such criminal law ontologies and rules (CLOR).</p></td></tr><tr><td>858</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_12">A Marker Passing Approach to Winograd Schemas</a></td></tr><tr><td colspan=3><p>This paper approaches a solution of Winograd Schemas with a marker passing algorithm which operates on an automatically generated semantic graph. The semantic graph contains common sense facts from data sources form the semantic web like domain ontologies e.g. from Linked Open Data (LOD), WordNet, Wikidata, and ConceptNet. Out of those facts, a semantic decomposition algorithm selects relevant facts for the concepts used in the Winograd Schema and adds them to the semantic graph. Markers are propagated through the graph and used to identify an answer to the Winograd Schema. Depending on the encoded knowledge in the graph (connectionist view of world knowledge) and the information encoded on the marker (for symbolic reasoning) our approach selects the answers. With this selection, the marker passing approach is able to beat the state-of-the-art approach by about 12%.</p></td></tr><tr><td>859</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_8">Unified Access to Heterogeneous Data Sources Using an Ontology</a></td></tr><tr><td colspan=3><p>The rise of cloud computing started a transition for software applications from local to remote infrastructures. This migration created an opportunity to aggregate and consolidate analogous data content. However, this data content usually come with very different data structures and data terminologies and is usually tightly coupled to one or more applications. With these disparities and restrictions, the analogous data ends up both centrally stored but spread over several disconnected heterogeneous data sources. In this article, we present an approach to aggregate data sources using live data consolidation. The approach preserves the original data sources; and by doing so, prevents associated applications from having to migrate to a new data source. The approach uses an ontology at its core to serve as a common semantic ground between data sources and leverage its stored knowledge to expand query capabilities.</p></td></tr><tr><td>860</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_10">SPARQ</a></td></tr><tr><td colspan=3><p>With more and more applications providing semantic data to improve interoperability, the amount of available RDF datasets is constantly increasing. The SPARQL query language is a W3C recommendation to provide query capabilities on such RDF datasets. Data integration from different RDF sources is up to now mostly task of RDF consuming clients. However, from a functional perspective, data integration boils down to a function application that consumes input data as parameters, and based on these, produces a new set of data as output. Following this notion, we introduce SPARQ</p></td></tr><tr><td>861</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_7">Ontology-Based Semantic Representation of Silk Road’s Caravanserais: Conceptualization of Multifaceted Links</a></td></tr><tr><td colspan=3><p>Knowledge representation and reasoning has gained relevance during the last years to improve historic architecture understanding and comparisons by developing innovative systems. This article presents research results about semantic representation of a sub set of Silk Road heritages, caravanserai. The core of the information system is an ontology-based schema to capture general and domain-based features of caravanserai by conceptualizing multifaceted links. Lexical links which are mapped from upper level sources are defined to give meaning, quotation and derivation to terms. Upper level links are proposed to give parent-child relations, part-whole relations or associative relations to building components or divisions represented as entities in terminology schema. The major contribution of the research is to conceptualize domain based links for architectural heritage. After studying different thesauruses or standards related to architectural classification or spatial reasoning, three schemas were defined as construction, services and spatial configuration. They acquire qualitative relations between building elements or divisions of a selected corpus of caravanserais. The paper concludes with technical and domain-based assessment of the ontology by publishing the ontology online in Web protégé and using the knowledge to classify 140 cases of the corpus of desert on route caravanserais of Safavid Period. Future work is to publish the RDF ontology as Linked Data.</p></td></tr><tr><td>862</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_9">EmbNum: Semantic Labeling for Numerical Values with Deep Metric Learning</a></td></tr><tr><td colspan=3><p>Semantic labeling for numerical values is a task of assigning semantic labels to unknown numerical attributes. The semantic labels could be numerical properties in ontologies, instances in knowledge bases, or labeled data that are manually annotated by domain experts. In this paper, we refer to semantic labeling as a retrieval setting where the label of an unknown attribute is assigned by the label of the most relevant attribute in labeled data. One of the greatest challenges is that an unknown attribute rarely has the same set of values with the similar one in the labeled data. To overcome the issue, statistical interpretation of value distribution is taken into account. However, the existing studies assume a specific form of distribution. It is not appropriate in particular to apply open data where there is no knowledge of data in advance. To address these problems, we propose a neural numerical embedding model (</p></td></tr><tr><td>863</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_5">Incorporating Text into the Triple Context for Knowledge Graph Embedding</a></td></tr><tr><td colspan=3><p>Knowledge graph embedding, aiming to represent entities and relations in a knowledge graph as low-dimensional real-value vectors, has attracted the attention of a large number of researchers. However, most of the embedding methods ignore the incompleteness of the knowledge graphs and they focus on the triples themselves in the knowledge graphs. In this paper, we try to introduce the information of texts to enhance the performances based on contextual model for knowledge graph embedding. Based on the assumption of the distant supervision, the sentences in texts contains abundant semantic information of the triples in knowledge graph, so that these semantic information can be utilized to relief the incompleteness of knowledge graphs and enhance the performances of knowledge graph embedding. Compared with state-of-the-art systems, preliminary evaluation results show that our proposed method obtains the better results in Hits@10.</p></td></tr><tr><td>864</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_6">On Enhancing Visual Query Building over KGs Using Query Logs</a></td></tr><tr><td colspan=3><p>Knowledge Graphs have recently gained a lot of attention and have been successfully applied in both academia and industry. Since KGs may be very large: they may contain millions of entities and triples relating them to each other, to classes, and assigning them data values, it is important to provide endusers with effective tools to explore information encapsulated in KGs. In this work we present a visual query system that allows users to explore KGs by intuitively constructing tree-shaped conjunctive queries. It is known that systems of this kind suffer from the problem of information overflow: when constructing a query the users have to iteratively choose from a potentially very long list of options, sich as, entities, classes, and data values, where each such choice corresponds to an extension of the query new filters. In order to address this problem we propose an approach to substantially reduce such lists with the help of ranking and by eliminating the so-called deadends, options that yield queries with no answers over a given KG.</p></td></tr><tr><td>865</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_24">Erratum to: Refinement-Based OWL Class Induction with Convex Measures</a></td></tr><tr><td colspan=3><p>nan</p></td></tr><tr><td>866</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_2">More Is Better: Sequential Combinations of Knowledge Graph Embedding Approaches</a></td></tr><tr><td colspan=3><p>Constructing and maintaining large-scale good quality knowledge graphs present many challenges. Knowledge graph completion has been regarded a promising direction in the knowledge graph community. The majority of current work for knowledge graph completion approaches do not take the schema of a target knowledge graph as input. As a result, the triples generated by these approaches are not necessarily consistent with the schema of the target knowledge graph. This paper proposes to improve the correctness of knowledge graph completion based on Schema Aware Triple Classification (SATC), which enables sequential combinations of knowledge graph embedding approaches. Extensive experiments show that our proposed approaches can significantly improve the correctness of the new triples produced by knowledge graph embedding methods.</p></td></tr><tr><td>867</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_4">Ranking Diagnoses for Inconsistent Knowledge Graphs by Representation Learning</a></td></tr><tr><td colspan=3><p>When a knowledge graph (KG) is growing e.g. by knowledge graph completion, it might become inconsistent with the logical theory which formalizing the schema of the KG. A common approach to restoring consistency is removing a minimal set of triples from the KG, called a diagnosis of the KB. However, there can be a large number of diagnoses. It is hard to manually select the best one among these diagnoses to restore consistency. To alleviate the selection burden, this paper studies automatic methods for ranking diagnoses so that people can merely focus on top diagnoses when seeking the best one. An approach to ranking diagnoses through representation learning aka knowledge graph embedding is proposed. Given a set of diagnoses, the approach first learns the embedding of the complement set of the union of all diagnoses, then for every diagnosis, incrementally learns an embedding of the complement set of the diagnosis and employs the embedding to estimate the removal cost of the diagnosis, and finally ranks diagnoses by removal costs. To evaluate the approach, four knowledge graphs with logical theories are constructed from the four great classical masterpieces of Chinese literature. Experimental results on these datasets show that the proposed approach is significantly more effective than classical random methods in ranking the best diagnoses at top places.</p></td></tr><tr><td>868</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_1">Knowledge Driven Intelligent Survey Systems for Linguists</a></td></tr><tr><td colspan=3><p>In this paper, we propose Knowledge Graph (KG), an articulated underlying semantic structure, to be a semantic bridge between human and systems. To illustrate our proposal, we focus on KG based intelligent survey systems. In state of the art systems, knowledge is hard-coded or implicit in these systems, making it hard for researchers to reuse, customise, link, or transmit the structured knowledge. Furthermore, such systems do not facilitate dynamic interaction based on the semantic structure. We design and implement a knowledge-driven intelligent survey system which is based on knowledge graph, a widely used technology that facilitates sharing and querying hypotheses, survey content, results, and analyses. The approach is developed, implemented, and tested in the field of Linguistics. Syntacticians and morphologists develop theories of grammar of natural languages. To evaluate theories, they seek intuitive grammaticality (well-formedness) judgments from native speakers, which either support a theory or provide counter-evidence. Our preliminary experiments show that a knowledge graph based linguistic survey can provide more nuanced results than traditional document-based grammaticality judgment surveys by allowing for tagging and manipulation of specific linguistic variables.</p></td></tr><tr><td>869</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_23">Linked Urban Open Data Including Social Problems’ Causality and Their Costs</a></td></tr><tr><td colspan=3><p>There are various urban problems, such as suburban crime, dead shopping street, and littering. However, various factors are socially intertwined; thus, structural management of the related data is required for visualizing and solving such problems. Moreover, in order to implement the action plans, local governments first need to grasp the cost-effectiveness. Therefore, this paper aims to construct Linked Open Data (LOD) that include causal relations of urban problems and the related cost information in the budget. We first designed a data schema that represents the urban problems’ causality and extended the schema to include budget information based on QB4OLAP. Next, we semi-automatically enriched instances according to the schema using natural language processing and crowdsourcing. Finally, as use cases of the resulting LOD, we provided example queries to extract the relationships between several problems and the particular cost information. We found several causes that lead to the vicious circle of urban problems and for the solutions of those problems, we suggest to a local government which actions should be addressed.</p></td></tr><tr><td>870</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_22">Semantically Enhanced Case Adaptation for Dietary Menu Recommendation of Diabetic Patients</a></td></tr><tr><td colspan=3><p>
Dietary menu planning for diabetic patients is a complicated tasks involving specific and common-sense knowledge. Case-based approach has been used to provide recommendation in the case where ratings were not easily available for domains such as menu planning. Among the important but yet difficult tasks in the case-based approach is case adaptation. To successfully support case adaptation, the constraint-based approach and food composition ontology were employed. Constraints knowledge were represented as production rules and exploits the food ontology to support adaptation. An ontological approach is also proposed to perform the inference process to satisfy the multiple design constraints.
</p></td></tr><tr><td>871</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_3">Knowledge Graph-Based Core Concept Identification in Learning Resources</a></td></tr><tr><td colspan=3><p>The automatic identification of core concepts addressed by a learning resource is an important task in favor of organizing content for educational purposes and for the next generation of learner support systems. We present a set of strategies for core concept identification on the basis of a semantic representation built using the open and available knowledge in the so-called Knowledge Graphs (KGs). Different unsupervised weighting strategies, as well as a supervised method that operates on the semantic representation, were implemented for core concept identification. In order to test the effectiveness of the proposed strategies, a human-expert annotated dataset of 96 learning resources extracted from MOOCs was built. In our experiments, we show the capacity of the semantic representation for the core-concept identification task as well as the superiority of the supervised method.</p></td></tr><tr><td>872</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_21">Semantic IoT: Intelligent Water Management for Efficient Urban Outdoor Water Conservation</a></td></tr><tr><td colspan=3><p>Water depletion is critical in the dry tropics due to drought, increased development and demographic or economic shifts. Although educational initiatives have improved urban indoor water-use, excessive outdoor wastage still occurs because in most urban areas residential users only have a biannual reading of quantity available to make informed or educated decisions on necessary or unnecessary consumption. For example, the average consumer will water lawns during a designated non-restricted time. The amount of water they use is determined arbitrarily (i.e., either by sight or by blocks of time). In many cases, water is wasted due to over saturation, automated sprinklers that cannot sense precipitation, poor placement of sprinkler direction, etc. Outdoor water use efficiency could be maximized if water flow was shut off when an area of lawn has had sufficient water based on a more intelligent monitoring system. This paper describes the development of an intelligent water management and information system that integrates real-time sensed data (soil moisture, etc.) and Web-available information to make dynamic decisions on water release for lawns and fruit trees. The initial pilot-prototype combines Semantic Technologies with Internet of Things to decrease urban outdoor water-use and educate residents on best water usage strategies.</p></td></tr><tr><td>873</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_20">User Participatory Construction of Open Hazard Data for Preventing Bicycle Accidents</a></td></tr><tr><td colspan=3><p>Recently, bicycle-related accidents, e.g., collision accidents at intersection increase and account for approximately 20% of all traffic accidents in Japan; thus, it is regarded as one of the serious social problems. However, the Traffic Accident Occurrence Map released by the Japanese Metropolitan Police Department is currently based on accident information records, and thus there are a number of near-miss events, which are overlooked in the map but will be useful for preventing the possible accidents. Therefore, we detect locations with high possibility of bicycle accidents using user participatory sensing and offer them drivers and government officials as Open Hazard Data (OHD) to prevent future bicycle accident. This paper uses smartphone sensors to obtain data for acceleration, location, and handle rotation information. Then, by classifying those data with convolutional neural networks, it was confirmed that the locations, where sudden braking occurred can be detected with an accuracy of 80%. In addition, we defined an RDF model for OHD that is currently publicly available. In future, we plan to develop applications using OHD, e.g., notifying alerts when users are approaching locations where near-miss events have occurred.</p></td></tr><tr><td>874</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_19">Development of Semantic Web-Based Imaging Database for Biological Morphome</a></td></tr><tr><td colspan=3><p>We introduce the RIKEN Microstructural Imaging Metadatabase, a semantic web-based imaging database in which image metadata are described using the Resource Description Framework (RDF) and detailed biological properties observed in the images can be represented as Linked Open Data. The metadata are used to develop a large-scale imaging viewer that provides a straightforward graphical user interface to visualise a large microstructural tiling image at the gigabyte level. We applied the database to accumulate comprehensive microstructural imaging data produced by automated scanning electron microscopy. As a result, we have successfully managed vast numbers of images and their metadata, including the interpretation of morphological phenotypes occurring in sub-cellular components and biosamples captured in the images. We also discuss advanced utilisation of morphological imaging data that can be promoted by this database.</p></td></tr><tr><td>875</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_17">KG-Buddhism: The Chinese Knowledge Graph on Buddhism</a></td></tr><tr><td colspan=3><p>One of the most important elements in human society is religion, which provides moralities to help regulate human behaviours. However, the Web lacks a specialized knowledge graph on religion. To facilitate religious knowledge sharing, we aim to build </p></td></tr><tr><td>876</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_18">Semantic Graph Analysis for Federated LOD Surfing in Life Sciences</a></td></tr><tr><td colspan=3><p>Currently, Linked Open Data (LOD) is increasingly used when publishing life science databases. To facilitate flexible use of such databases, we employ a method that uses federated query search along a path of class–class relationships. However, an effective method for federated query search requires analysis of the structure the relationships form for LOD datasets. Therefore, we constructed a graph of class–class relationships among 43 SPARQL endpoints and analyzed the connectivity of the graph. As a result, we found that (1) the sizes of connected components follow a power law; thus we should deal with the classes separately according to the size of connected components, (2) only the largest and second largest connected components have paths among classes from two or more SPARQL endpoints, and the datasets of each of the two connected components share ontologies, and (3) key classes that connect SPARQL endpoints are primarily upper-level concepts in the biological domain.</p></td></tr><tr><td>877</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_16">Cross-Lingual Taxonomy Alignment with Bilingual Knowledge Graph Embeddings</a></td></tr><tr><td colspan=3><p>Recently, different knowledge graphs have become the essential components of many intelligent applications, but no research has explored the use of knowledge graphs to cross-lingual taxonomy alignment (CLTA), which is the task of mapping each category in the source taxonomy of one language onto a ranked list of most relevant categories in the target taxonomy of another language. In this paper, we study how to perform CLTA with a multilingual knowledge graph. Firstly, we identify the candidate matched categories in the target taxonomy for each category in the source taxonomy. Secondly, we find the relevant knowledge denoted as triples for each category in the given taxonomies. Then, we propose two different bilingual knowledge graph embedding models called BTransE and BTransR to encode triples of different languages into the same vector space. Finally, we perform CLTA based on the vector representations of the relevant RDF triples for each category. Preliminary experimental results show that our approach is comparable and complementary to the state-of-the-art method.</p></td></tr><tr><td>878</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_15">Enhancing Knowledge Graph Embedding from a Logical Perspective</a></td></tr><tr><td colspan=3><p>Knowledge graph embedding aims to represent entities and relations in a knowledge graph as low-dimensional real-value vectors. Most existing studies exploit only structural information to learn these vectors. This paper studies how logical information expressed as RBox axioms in OWL 2 is used for embedding. The involvement of RBox axioms could prevent existing methods from learning predictive vectors. For example, the symmetric, reflexive or transitive relations can be declared by RBox axioms, but popular translation-based methods are unable to learn distinguishable vectors for multiple these relations in the ideal case. To overcome these limitations introduced by the involvement of RBox axioms, this paper proposes to enhance existing translation-based methods by logical pre-completion and bi-directional projection of entities. Experimental results demonstrate that these enhancements improve the predictive performance in link prediction and triple classification.</p></td></tr><tr><td>879</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_10">Publishing E-RDF Linked Data for Many Agents by Single Third-Party Server</a></td></tr><tr><td colspan=3><p>
Linked data is one of the most successful practices in semantic web, which has led to the opening and interlinking of data. Though many agents (mostly academic organizations and government) have published a large amount of linked data, numerous agents such as private companies and industries either do not have the ability or do not want to make an additional effort to publish linked data. Thus, for agents who are willing to open part of their data but do not want to make an effort, the task can be undertaken by a professional third-party server (together with professional experts) that publishes linked data for these agents. Consequently, when a single third-party server is on behalf of multiple agents, it is also responsible to organize these multiple-source URIs (data) in a systematic way to make them referable, satisfying the 4-star data principles, as well as protect the confidential data of these agents. In this paper, we propose a framework to leverage these challenges and design a URI standard based on our proposed E-RDF, which extends and optimizes the existing 5-star linked data principles. Also, we introduce a customized data filtering mechanism to protect the confidential data. For validation, we implement a prototype system as a third-party server that publishes linked data for a number of agents. It demonstrates well-organized 5-star linked data plus E-RDF and shows the additional advantages of data integration and interlinking among agents.
</p></td></tr><tr><td>880</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_14">Mining Inverse and Symmetric Axioms in Linked Data</a></td></tr><tr><td colspan=3><p>In the context of Linked Open Data, substantial progress has been made in mining of property subsumption and equivalence axioms. However, little progress has been made in determining if a predicate is symmetric or if its inverse exists within the data. Our study of popular linked datasets such as DBpedia, YAGO and their associated ontologies has shown that they contain very few inverse and symmetric property axioms. The state-of-the-art approach ignores the open-world nature of linked data and involves a time-consuming step of preparing the input for the rule-miner. To overcome these shortcomings, we propose a schema-agnostic unsupervised method to discover inverse and symmetric axioms from linked datasets. For mining inverse property axioms, we find that other than support and confidence scores, a new factor called predicate-preference factor (</p></td></tr><tr><td>881</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_12">A New Sentiment and Topic Model for Short Texts on Social Media</a></td></tr><tr><td colspan=3><p>Nowadays plenty of user-generated posts, e.g., tweets and sina weibos, are published on social media and the posts imply the public’s opinions towards various topics. Joint sentiment/topic models are widely applied in detecting sentiment-aware topics on the lengthy documents. However, the characteristics of posts, i.e., short texts, on social media pose new challenges: (1) context sparsity problem of posts makes traditional sentiment-topic models inapplicable; (2) conventional sentiment-topic models are designed for flat documents without structure information, while publishing users, publishing timeslices and hashtags of posts provide rich structure information for these posts. In this paper, we firstly devise a method to mine potential hashtags, based on explicit hashtags, to further enrich structure information for posts, then we propose a novel Sentiment Topic Model for Posts (STMP) which aggregates posts with the structure information, i.e., timeslices, users and hashtags, to alleviate the context sparsity problem. Experiments on Sentiment140 and Twitter7 show STMP outperforms previous models both in sentiment classification and sentiment-aware topic extraction.</p></td></tr><tr><td>882</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_13">Semi-supervised Stance-Topic Model for Stance Classification on Social Media</a></td></tr><tr><td colspan=3><p>Stance detection aims to automatically determine from text whether the author of the text is in favor of, against, or neutral towards a issue. Social media, such as Sina Weibo, reflects the general public’s stances towards different issues. Detecting and summarizing stances towards specific issues from social media is an important and challenging task. Although stance detection on social media has been studied before, previous work, most of which are based on supervised learning, may not work well because they suffer from its heavy dependence on training data. Other weakly supervised method also use some heuristic rules to select the posts with specific stances as training data, but these selected posts often concentrate on a few subtopics of the specific issue, these weakly supervised method can only train a biased stance classifier. To better detect stances toward specific issues, we consider to detect stances with a small number of labeled training data and a mass of unlabeled data. To integrate the supervised information into our model, we combine a discriminative maximum entropy (Max-Ent) component with the generative component. The Max-Ent component leverages hand-crafted features from labeled data to separate different stances. In this paper, we propose a semi-supervised topic model, Semi-Supervised Stance Topic Model (SSTM), that model stances and topics of the posts on social media. Since the posts on social media are short texts, we also incorporate the structural information of the posts, i.e., gender information, location information and time information, to aggregate posts for alleviating the context sparsity of the posts. The model has been evaluated on the selected posts on sina weibo, which talk about “the verbal battle of Han han and Fang zhouzi”, to classify the stance of each posts. Preliminary experiments have shown promising results achieved by SSTM. Moreover, we also analyze the common difficulties in stance detection on social media. Finally, we also visualize the subtopics of the given issue generated by SSTM.</p></td></tr><tr><td>883</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_11">Missing RDF Triples Detection and Correction in Knowledge Graphs</a></td></tr><tr><td colspan=3><p>Knowledge graphs (KGs) have become a powerful asset in information science and technology. To foster enhancing search, information retrieval and question answering domains KGs offer effective structured information. KGs represent real-world entities and their relationships in Resource Description Framework (RDF) triples format. Despite the large amount of knowledge, there are still missing and incorrect knowledge in the KGs. We study the graph patterns of interlinked entities to discover missing and incorrect RDF triples in two KGs - DBpedia and YAGO. We apply graph-based approach to map similar object properties and apply similarity based approach to map similar datatype properties. Our propose methods can utilize those similar ontology properties and efficiently discover missing and incorrect RDF triples in DBpedia and YAGO.</p></td></tr><tr><td>884</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_9">Entity Linking in Queries Using Word, Mention and Entity Joint Embedding</a></td></tr><tr><td colspan=3><p>Entity linking in queries is an important task for connecting search engines and knowledge bases. This task is very challenging because queries are usually very short and there is very limited context information for entity disambiguation. This paper proposes a new accurate and efficient entity linking approach for search queries. The proposed approach first jointly learns word, mention and entity embeddings in a unified space, and then computes a set of features for entity disambiguation based on the learned embeddings. The entity linking problem is solved as a ranking problem in our approach, a ranking SVM is trained to accurately predict entity links. Experiments on real data show that our proposed approach achieves better performance than comparison approaches.</p></td></tr><tr><td>885</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_8">Resolving Range Violations in DBpedia</a></td></tr><tr><td colspan=3><p>DBpedia, a large-scale multi-disciplinary knowledge graph extracted from structured data in Wikipedia, is an essential part of the Linked Open Data (LOD). However, several previous works report many types of errors existing in DBpedia. The crucial one is </p></td></tr><tr><td>886</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_3">Refined JST Thesaurus Extended with Data from Other Open Life Science Data Sources</a></td></tr><tr><td colspan=3><p>We are developing a refined Japan Science and Technology (JST) thesaurus with thirty-five relations to enable description of rigorous relationships among concepts. In this study, we prepared an environment for performing SPARQL queries and evaluated the JST thesaurus in the life sciences by comparing query results with the originals. Based on the results of the investigation, we constructed a fibrinolysis network from the thesaurus as a collection of concepts connected with fibrinolysis within three steps, and we discovered that fibrinolysis was associated with fifty-four concepts, including sixteen diseases and twelve physiological phenomena. Subsequently, using the sub-classified relations, we divided the sixteen diseases into two diseases that developed after fibrinolysis progressed, seven diseases that shared common molecules in the development mechanism with fibrinolysis, and other associated conditions. Furthermore, we mapped concepts between the JST thesaurus, ChEBI, and Gene Ontology by matching the labels and synonyms. As a result, we could integrate the fibrinolysis network with thirty-seven chemicals, including four antifibrinolytic agents and twenty-seven human gene products that can regulate fibrinolysis. Thus, we were able to handle the information relating to a series of molecules, molecular-level biological phenomena, and diseases by integrating the refined JST thesaurus with information regarding chemicals and gene products from other resources.
</p></td></tr><tr><td>887</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_2">Data Structuring for Launching Web Services Triggered by Media Content</a></td></tr><tr><td colspan=3><p>There are some efforts to inspire the viewers to buy something or visit somewhere when they are viewing such objects or places on the TV programs or web sites. It is required to link some specific services, which can be run on smartphones or tablets, to content on TV or a web site. However, simple combination of content and possible services still requires some operations taken by the viewers such as typing search words in the applications or the pages of the services. Such required actions may decline their motivations to use the services. Therefore, the authors propose the data model that allows to seek matching of an entity within content with various services, and to generate service launch information dynamically changed by combination of an entity and a service. The data model is based on combination of ontology class structure and reasoning. In this paper, effectiveness of the data model is shown by the prototype applications which call various web services to inspire viewers for subsequent actions in accordance with media content.
</p></td></tr><tr><td>888</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_7">The Identity Problem in Description Logic Ontologies and Its Application to View-Based Information Hiding</a></td></tr><tr><td colspan=3><p>The work in this paper is motivated by a privacy scenario in which the identity of certain persons (represented as anonymous individuals) should be hidden. We assume that factual information about known individuals (i.e., individuals whose identity is known) and anonymous individuals is stored in an ABox and general background information is expressed in a TBox, where both the TBox and the ABox are publicly accessible. The identity problem then asks whether one can deduce from the TBox and the ABox that a given anonymous individual is equal to a known one. Since this would reveal the identity of the anonymous individual, such a situation needs to be avoided. We first observe that not all Description Logics (DLs) are able to derive any such equalities between individuals, and thus the identity problem is trivial in these DLs. We then consider DLs with nominals, number restrictions, or function dependencies, in which the identity problem is non-trivial. We show that in these DLs the identity problem has the same complexity as the instance problem. Finally, we consider an extended scenario in which users with different rôles can access different parts of the TBox and ABox, and we want to check whether, by a sequence of rôle changes and queries asked in each rôle, one can deduce the identity of an anonymous individual.</p></td></tr><tr><td>889</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_6">Energy-Efficiency of OWL Reasoners—Frequency Matters</a></td></tr><tr><td colspan=3><p>While running times of ontology reasoners have been studied extensively, studies on energy-consumption of reasoning are scarce, and the energy-efficiency of ontology reasoning is not fully understood yet. Earlier empirical studies on the energy-consumption of ontology reasoners focused on reasoning on smart phones and used measurement methods prone to noise and side-effects. This paper presents an evaluation of the energy-efficiency of five state-of-the-art OWL reasoners on an ARM single-board computer that has built-in sensors to measure the energy consumption of CPUs and memory precisely. Using such a machine gives full control over installed and running software, active clusters and CPU frequencies, allowing for a more precise and detailed picture of the energy consumption of ontology reasoning. Besides evaluating the energy consumption of reasoning, our study further explores the relationship between computation power of the CPU, reasoning time, and energy consumption.</p></td></tr><tr><td>890</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_1">Building Wikipedia Ontology with More Semi-structured Information Resources</a></td></tr><tr><td colspan=3><p>Wikipedia has been recently drawing attention as a semi-structured information resource for the automatic building of ontology. This paper describes a method of building general-purpose “lightweight ontology” by semi-automatically extracting the Is-a relation (rdfs:subClassOf), class-instance relation (rdf:type), concepts such as Triple, and a relation between concepts from information that includes category trees, define statements, lists and Wikipedia infoboxes. Also, we evaluate the built ontology by comparing it with other Wikipedia ontologies, such as YAGO and DBpedia.</p></td></tr><tr><td>891</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_5">Reasoning on Context-Dependent Domain Models</a></td></tr><tr><td colspan=3><p>Modelling context-dependent domains is hard, as capturing multiple context-dependent concepts and constraints easily leads to inconsistent models or unintended restrictions. However, current semantic technologies not yet support reasoning on context-dependent domains. To remedy this, we introduced ConDL, a set of novel description logics tailored to reason on contextual knowledge, as well as JConHT, a dedicated reasoner for ConDL ontologies. ConDL enables reasoning on the consistency and satisfiability of context-dependent domain models, e.g., Compartment Role Object Models (CROM). We evaluate the suitability and efficiency of our approach by reasoning on a modelled banking application and measuring the performance on randomly generated models.</p></td></tr><tr><td>892</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_32">Road Traffic Question Answering System Using Ontology</a></td></tr><tr><td colspan=3><p>Many people use social media to report and receive road traffic information, e.g., car accidents and congestions. We have implemented a Twitter-based traffic-related information reposting (retweeting) system, which users usually referred to as @traffy. To improve on our works, we propose an ontology-based Thai-language question answering system that gathers real-time traffic data from Twitter. The data collected are converted into traffic incident knowledge of </p></td></tr><tr><td>893</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_31">Ontology Based Suggestion Distribution System </a></td></tr><tr><td colspan=3><p>The digitization of modern cities has brought cities to a new level. There are still many new areas yet to be discovered in this new ecosystem. Today, there is an urgent need for smarter cities to support the growing population. One particular problem is citizens do not know which city department to give their suggestions to. This paper presents a system for distributing suggestions from citizens to the right city officials based on ontology knowledge base. We use data from official websites to construct our ontology and do experiments with actual suggestions from citizens. The experiments show some promising results.</p></td></tr><tr><td>894</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_30">An Ontology-Based Intelligent Speed Adaptation System for Autonomous Cars</a></td></tr><tr><td colspan=3><p> </p></td></tr><tr><td>895</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_4">Refinement-Based OWL Class Induction with Convex Measures</a></td></tr><tr><td colspan=3><p>Beam-search may be used to iteratively explore and evaluate </p></td></tr><tr><td>896</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_27">A Keyword Exploration for Retrieval from Biomimetics Databases</a></td></tr><tr><td colspan=3><p>Biomimetics contributes to innovative engineering by imitating the models, systems, and elements of nature. Biomimetics research requires the development of a biomimetics database including widely varied knowledge across different domains. Interoperability of knowledge among those domains is necessary to create such a database. Ontologies clarify concepts that appear in target domains and help to improve interoperability. Furthermore, linked data technologies are very effective for integrating a database with existing biological diversity databases. In this paper, we propose a keyword exploration technique to find appropriate keywords for retrieving meaningful knowledge from various biomimetics databases. Such a technique could support idea creation by users based on a biomimetics ontology. This paper shows a prototype of the biomimetics ontology and keyword exploration tool.</p></td></tr><tr><td>897</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_29">A Semantic Keyword Search Based on the Bidirectional Fix Root Query Graph Construction Algorithm</a></td></tr><tr><td colspan=3><p>In the last few years, the large amount of personal information in RDF format is widely deployed. To access the semantic information, it needs a semantic formal query (e.g. SPARQL query). However, this kind of query requires users to know the ontology structure and master its syntax. This paper proposes the X-SKengine, the semantic keyword search engine for specific expert discovery domain. The X-SKengine transforms the user keywords to the SPARQL query using a bidirectional fix root query graph construction algorithm which is able to compute the query graphs without limitation of the directions of relationships in ontologies. The experiment was conducted to compare the capability of SPARQL query construction between X-SKengine and the previous version. The results show that X-SKengine can automatically construct SPARQL queries relevant to meaning of user keywords for various ontology structures.</p></td></tr><tr><td>898</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_28">Choosing Related Concepts for Intelligent Exploration</a></td></tr><tr><td colspan=3><p>How to explore semantic data such as linked data, knowledge graph, ontologies etc. and get appropriate information from them are very important techniques for intelligent application systems based on them. In this article, we focus on intelligent exploration of ontologies since ontologies provide systematized knowledge to understand target domain and contribute to deep understanding of semantic data. The authors propose a novel conceptual search method called “Multistep Expansion based Concept Search” to get appropriate concepts from ontologies according to the user’s intentions and purpose.</p></td></tr><tr><td>899</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_24">Personalized Model Combination for News Recommendation in Microblogs</a></td></tr><tr><td colspan=3><p>Facing large amount of accessible data everyday on the Web, it is difficult for people to find relevant news articles, hence the importance of news recommendation. Focused on the information to be used and the way to model it, each of the existing models proposes its own algorithm to recommend different news to different users. For these models, personalization is only done at the recommendation level. But if the user chooses a model that is not appropriate for him, the recommendation may fail to work accurately. Therefore, personalization should also be done at the model level. In our proposed model, the first level is defining four atomic recommendation models that make fully use of the social and content information of users and the second level is adapting to each user that atomic models effectively used. Experiments conducted on two real datasets built from </p></td></tr><tr><td>900</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_26">Template-Driven Semantic Parsing for Focused Web Crawler</a></td></tr><tr><td colspan=3><p>We present Template-Driven Semantic Parser (TDSP) capable to represent, at least to some degree, the semantics of Web pages being processed. Data extraction process realized by means of TDSP is driven by a set of instructions stored in an easily modifiable XML-based template. In order to enhance the precision of Web page data extraction, the TDSP template format allows to use a specialized Expression Language (EL). The template may be easily created and modified using a tool called Visual Template Designer. TDSP provides an output document containing an RDF graph composed of triples that represent the website resources under exploration. In accordance to the Semantic Web paradigm, each resource has its semantics assigned and is connected to other resources by means of one or many relations. The semantic types of the resources and the relations between them are predefined in an ontology of Web artifacts.</p></td></tr><tr><td>901</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_25">A Model for Enriching Multilingual Wikipedias Using Infobox and Wikidata Property Alignment</a></td></tr><tr><td colspan=3><p>Wikipedia supports a large converged data with millions of contributions in more than 287 languages currently. Its content changes rapidly and continuously every hour with thousands of edits which trigger many challenges for Wikipedia in controlling, associating and balancing article content among language editions. This paper provides some processes to enrich Wikipedia content, which will retrieve semantic relations based on alignment between infobox properties and Wikidata properties in various languages. Then, the outcomes mainly contribute these semantic relations back to Wikidata and Wikipedias, especially ones are based on the Latin alphabet. The case study will offer a specific case about aligning biological infoboxes and detecting missing interwiki links of biological species at Vietnamese Wikipedia and English Wikipedia.</p></td></tr><tr><td>902</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_22">Development of the Belief Culture Ontology and Its Application: Case Study of the GreaterMekong Subregion</a></td></tr><tr><td colspan=3><p>In this paper, the development of an ontology that represents the knowledge of belief culture in the Greater Mekong subregion(GMS) is presented. The ontology was carefully designed to specify the concepts relevant to intangible and tangible cultural heritage and the relations among them. The knowledge domain in this work focuses on the cultural context and implicit attributes of the GMS as an initial case study. To further illustrate the potential of the developed ontology, a semantic search application was implemented and then evaluated by experts. On the evaluation processes, several complicated queries were designed in order to fully utilize the relations among ontological classes, and the results were returned accurately. The evaluation proved that the ontology was welldefined in aspects of its hierarchical structure and relations from intermediate concept layers.</p></td></tr><tr><td>903</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_23">Representations of Psychological Functions of Peer Support Services for Diabetic Patients</a></td></tr><tr><td colspan=3><p>One of the functions of peer support services for diabetic patients is psychological changes through communications among patients. Medical professionals in the practice of this research request peer support services through a web system. The design of the psychological functions requires tailoring depending on contexts. An important thing for the adaptive design is to discuss the needed psychological functions among designers such as medical professionals, patients, and researchers. However, since the designers tend to set intentions of psychological functions by taking a seat-of-the-pants approach, even for the designers, describing their intentions of psychological functions is not easy. In this paper, we propose a framework to represent psychological functions for the designers to share and discuss intentions of psychological functions in web systems.</p></td></tr><tr><td>904</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_20">RDFa Parser for Hybrid Resources</a></td></tr><tr><td colspan=3><p>In this paper, we propose several mash-up design methodologies that are intended for enhancing human-readable aspects of semantically annotated data resources such as LOD. Proposals are based on the features provided by RDFa format which is an extension of RDF format. So called “semantic parser” have been developed which is an application software to provide data-conversion/transformation service through web API. The parser provides several hybrid features for both human-readable (HR) and machine-readable (MR) resources by using RDFa format.</p></td></tr><tr><td>905</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_19">Optimizing SPARQL Query Processing on Dynamic and Static Data Based on Query Time/Freshness Requirements Using Materialization</a></td></tr><tr><td colspan=3><p>To integrate various Linked Datasets, data warehousing and live query processing provide two extremes for optimized response time and quality respectively. The first approach provides very fast responses but with low-quality because changes of original data are not immediately reflected on materialized data. The second approach provides accurate responses but it is notorious for long response times. A hybrid SPARQL query processor provides a middle ground between two specified extremes by splitting the triple patterns of SPARQL query between live and local processors based on a predetermined coherence threshold specified by the administrator. Considering quality requirements while splitting the SPARQL query, enables the processor to eliminate the unnecessary live execution and releases resources for other queries. This requires estimating the quality of response provided with current materialized data, compare it with user requirements and determine the most selective sub-queries which can boost the response quality up to the specified level with least computational complexity. In this work, we propose solutions for estimating the freshness of materialized data, as one dimension of the quality, by extending cardinality estimation techniques. Experimental results show that we can estimate the freshness of materialized data with a low error rate.</p></td></tr><tr><td>906</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_21">Ontology Construction and Schema Merging Using an Application-Independent Ontology for Humanitarian Aid in Disaster Management</a></td></tr><tr><td colspan=3><p>Humanitarian aid information, e.g., information on the occurrences of disaster situations, victims, shelters, resources, and facilities, is usually rapidly dynamic, ambiguous, and huge. A system of humanitarian aid often involves data items from multidisciplinary environments, some of which have similar meanings but appear structurally different in various data sources. To achieve semantic interoperability among humanitarian aid information systems to be exchanged meaningful information, this paper contributes a methodology for construction of an application-independent ontology and proposes a guideline for merging information from different databases through the application-independent ontology that helps people to integrate systems with minimal modification. We demonstrate how to develop an ontology for </p></td></tr><tr><td>907</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_18">Graph Pattern Based RDF Data Compression</a></td></tr><tr><td colspan=3><p>nan</p></td></tr><tr><td>908</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_14">A Lightweight Treatment of Inexact Dates</a></td></tr><tr><td colspan=3><p>This paper presents a </p></td></tr><tr><td>909</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_16">Mining Type Information from Chinese Online Encyclopedias</a></td></tr><tr><td colspan=3><p>Recently, there is an increasing interest in extracting or mining type information from Web sources. Type information stating that an instance is of a certain type is an important component of knowledge bases. Although there has been some work on obtaining type information, most of current techniques are either language-dependent or to generate one or more general types for a given instance because of type sparseness. In this paper, we present a novel approach for mining type information from Chinese online encyclopedias. More precisely, we mine type information from abstracts, infoboxes and categories of article pages in Chinese encyclopedia Web sites. In particular, most of the generated Chinese type information is inferred from categories of article pages through an attribute propagation algorithm and a graph-based random walk method. We conduct experiments over Chinese encyclopedia Web sites: Baidu Baike, Hudong Baike and Chinese Wikipedia. Experimental results show that our approach can generate large scale and high-quality Chinese type information with types of appropriate granularity.</p></td></tr><tr><td>910</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_13">Publishing Danish Agricultural Government Data as Semantic Web Data</a></td></tr><tr><td colspan=3><p>Recent advances in Semantic Web technologies have led to a growing popularity of the Linked Open Data movement. Only recently, the Danish government has joined the movement and published several datasets as Open Data. These raw datasets are difficult to process automatically and combine with other data sources on the Web. Hence, our goal is to convert such data into RDF and make it available to a broader range of users and applications as Linked Open Data. In this paper, we discuss our experiences based on the particularly interesting use case of agricultural data as agriculture is one of the most important industries in Denmark. We describe the process of converting the data and discuss the particular problems that we encountered with respect to the considered datasets. We additionally evaluate our result based on several queries that could not be answered based on existing sources before.</p></td></tr><tr><td>911</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_15">A Multi-strategy Learning Approach to Competitor Identification</a></td></tr><tr><td colspan=3><p>Competitor identification tries to find competitors of some entity in a given field, which is the key to the success of market intelligence. Manually collecting competitors is labor-intensive and time consuming. So automatic approaches are proposed for this purpose. However, these approaches suffer from the following two main challenges. Competitor information might not only be contained in semi-structured sources like lists or tables, but also be mentioned in free texts. The diversity of its sources make competitor identification quite difficult. Also, these competitors might not always occur in form of their full names. The occurrences of name variants further increase the diversity, and make the task more challenging. In this paper, we propose a novel unsupervised approach to identify competitors from prospectuses based on a multi-strategy learning algorithm. More precisely, we first extract competitors from lists using some predefined heuristic rules. By leveraging redundancies among competitor information in lists, tables, and texts, these competitors are fed as seeds to distantly supervise the learning process to find table columns and text patterns containing competitors. The whole process is iteratively performed. In each iteration, the newly discovered competitors of high confidence from various sources are treated as new seeds for bootstrapping. The experimental results show the effectiveness of our approach without human intentions and external knowledge bases. Moreover, the approach significantly outperforms traditional named entity recognition approaches.</p></td></tr><tr><td>912</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_17">G-Diff: A Grouping Algorithm for RDF Change Detection on MapReduce</a></td></tr><tr><td colspan=3><p>Linked Data is a collection of RDF data that can grow exponentially and change over time. Detecting changes in RDF data is important to support Linked Data consuming applications with version management. Traditional approaches for change detection are not scalable. This has led researchers to devise algorithms on the MapReduce framework. Most works simply take a URI as a Map key. We observed that it is not efficient to handle RDF data with a large number of distinct URIs since many Reduce tasks have to be created. Even though the Reduce tasks are scheduled to run simultaneously, too many small Reduce tasks would increase the overall running time. In this paper, we propose G-Diff, an efficient MapReduce algorithm for RDF change detection. G-Diff groups triples by URIs during Map phase and sends the triples to a particular Reduce task rather than multiple Reduce tasks. Experiments on real datasets showed that the proposed approach takes less running time than previous works.</p></td></tr><tr><td>913</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_11">Building of Industrial Parts LOD for EDI - A Case Study -</a></td></tr><tr><td colspan=3><p>A wide variety of mechanical parts are used as products in the area of manufacturing. The code systems of product information are necessary for realizing Electronic Data Interchange (EDI) of business-to-business. However, each code systems are designed and maintained by different industry associations. Thus, we built an industrial parts Linked Open Data (LOD), which we called “N-ken LOD” based on a screw product code system (N-ken Code) maintained by Osaka fasteners cooperative association (Daibyokyo). In this paper, we first describe building of N-ken LOD, then how we linked it to external datasets like DBpedia, and built product supplier relations in order to support the EDI.</p></td></tr><tr><td>914</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_12">inteSearch: An Intelligent Linked Data Information Access Framework</a></td></tr><tr><td colspan=3><p>Information access over linked data requires to determine subgraph(s), in linked data’s underlying graph, that correspond to the required information need. Usually, an information access framework is able to retrieve richer information by checking of a large number of possible subgraphs. However, on the fly checking of a large number of possible subgraphs increases information access complexity. This makes an information access frameworks less effective. A large number of contemporary linked data information access frameworks reduce the complexity by introducing different heuristics but they suffer on retrieving richer information. Or, some frameworks do not care about the complexity. However, a practically usable framework should retrieve richer information with lower complexity. In linked data information access, we hypothesize that pre-processed data statistics of linked data can be used to efficiently check a large number of possible subgraphs. This will help to retrieve comparatively richer information with lower data access complexity. Preliminary evaluation of our proposed hypothesis shows promising performance.</p></td></tr><tr><td>915</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_10">CURIOS Mobile: Linked Data Exploitation for Tourist Mobile Apps in Rural Areas</a></td></tr><tr><td colspan=3><p>As mobile devices proliferate and their computational power has increased rapidly over recent years, mobile applications have become a popular choice for visitors to enhance their travelling experience. However, most tourist mobile apps currently use narratives generated specifically for the app and often require a reliable Internet connection to download data from the cloud. These requirements are difficult to achieve in rural settings where many interesting cultural heritage sites are located. Although Linked Data has become a very popular format to preserve historical and cultural archives, it has not been applied to a great extent in tourist sector. In this paper we describe an approach to using Linked Data technology for enhancing visitors’ experience in rural settings. In particular, we present CURIOS Mobile, the implementation of our approach and an initial evaluation from a case study conducted in the Western Isles of Scotland.</p></td></tr><tr><td>916</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_9">Link Prediction in Linked Data of Interspecies Interactions Using Hybrid Recommendation Approach</a></td></tr><tr><td colspan=3><p>Linked Open Data for ACademia (LODAC) together with National Museum of Nature and Science have started collecting linked data of interspecies interaction and making link prediction for future observations. The initial data is very sparse and disconnected, making it very difficult to predict potential missing links using only one prediction model alone. In this paper, we introduce Link Prediction in Interspecies Interaction network (LPII) to solve this problem using hybrid recommendation approach. Our prediction model is a combination of three scoring functions, and takes into account collaborative filtering, community structure, and biological classification. We have found our approach, LPII, to be more accurate than other combinations of scoring functions. Using significance testing, we confirm that these three scoring functions are significant for LPII and they play different roles depending on the conditions of linked data. This shows that LPII can be applied to deal with other real-world situations of link prediction.
</p></td></tr><tr><td>917</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_23">Ontology Development for Interoperable Database to Share Data in Service Fields</a></td></tr><tr><td colspan=3><p>Sharing data from various systems such as sensor networks, time and motion study data, and text data is important. To process and manage the collected data, the authors have proposed a database framework called the COTO database. As described in this paper, the authors propose a COTO-ontology as the basis of the COTO database. The COTO-ontology was developed based on the data collected by DANCE, which is an evaluation tool of quality of care, from the real settings. It will be applied as a platform for the evaluation of robotic devices used for nursing care.
</p></td></tr><tr><td>918</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_22">Towards an Enterprise Entity Hub: Integration of General and Enterprise Knowledge</a></td></tr><tr><td colspan=3><p>Today’s enterprises possess vast quantities of data generated by their own applications and services across different organisational systems. However, the data tends to be stored in many data-silos with redundant, duplicated, and outdated information. This means that finding the right information and obtaining valuable insights is difficult. To address this problem, we develop the enterprise entity hub that enables users to search, analyse, share, and filter information across large-scale data sources. One of its core components, the base knowledge is used to organise and interlink the organisation’s knowledge model, we then construct a base knowledge through the integration of several data sources, and expand this knowledge by interlinking a set of enterprise content.</p></td></tr><tr><td>919</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_24">Efficiently Finding Paths Between Classes to Build a SPARQL Query for Life-Science Databases</a></td></tr><tr><td colspan=3><p>Many databases in life science are provided in Resource Description Framework (RDF) model with SPARQL Protocol and RDF Query Language (SPARQL) endpoints. However, it may be difficult for users who are not familiar with Semantic Web technologies to write a SPARQL query. Therefore, assisting users to build SPARQL queries is important task to expand the range of users of RDF databases. We developed a web application called SPARQL Builder (</p></td></tr><tr><td>920</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_21">Contrasting RDF Stream Processing Semantics</a></td></tr><tr><td colspan=3><p>The increasing popularity of RDF Stream Processing (RSP) has led to developments of data models and processing engines which diverge in several aspects, ranging from the representation of RDF streams to semantics. Benchmarking systems such as LSBench, SRBench, CSRBench, and YABench were introduced as attempts to compare different approaches, focusing mainly on the operational aspects. The recent logic-based LARS framework provides a theoretical underpinning to analyze stream processing/reasoning semantics. In this work, we use LARS to compare the semantics of two typical RSP engines, namely C-SPARQL and CQELS, identify conditions when they agree on the output, and discuss situations where they disagree. The findings give insights that might prove to be useful for the RSP community in developing a common core for RSP.</p></td></tr><tr><td>921</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_19">Meta-Level Properties for Reasoning on Dynamic Data</a></td></tr><tr><td colspan=3><p>Dynamic features are important for data processing when dealing with real applications. In this paper we introduce a methodology for validating the construction of ontological knowledge base and optimising the query answering with such ontologies. In this paper, we firstly introduce some meta-properties of dynamic for ontologies. These meta-properties impose several constraints on the taxonomic structure of an ontology. We then investigate how to build up a meta-ontology with the constrains on these meta-properties. The goal of our methodology is </p></td></tr><tr><td>922</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_15">A Contrastive Study on Semantic Prosodies of Minimal Degree Adverbs in Chinese and English</a></td></tr><tr><td colspan=3><p>From the perspective of cross-language, this paper, by using Chinese and English corpus, examines the semantic prosody similarities and differences between </p></td></tr><tr><td>923</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_20">Distance-Based Ranking of Negative Answers</a></td></tr><tr><td colspan=3><p>Suggesting negative answers to users is a solution to the problem of insufficiently many answers in conjunctive query answering over description logic (DL) ontologies. Negative answers are complementary to certain answers and have explanations in the given ontology. Before being suggested to users, negative answers need to be ranked. However, there is no method for ranking negative answers by now. To fill this gap, this paper studies ranking methods that require only information on the given query and the given ontology. Three distance-based methods are proposed. Experimental results demonstrate that these methods can effectively rank negative answers to those conjunctive queries that have certain answers in the given DL ontology.</p></td></tr><tr><td>924</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_17">Answer Type Identification for Question Answering</a></td></tr><tr><td colspan=3><p>Question Answering research has long recognised that the identification of the type of answer being requested is a fundamental step in the interpretation of a question as a whole. Previous strategies have ranged from trivial keyword matches, to statistical analyses, to well-defined algorithms based on shallow syntactic parses with user-interaction for ambiguity resolution. A novel strategy combining deep NLP on both syntactic and dependency parses with supervised learning is introduced and results that improve on extant alternatives reported. The impact of the strategy on QALD is also evaluated with a proprietary Question Answering system and its positive results analysed.</p></td></tr><tr><td>925</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_16">A Graph Traversal Based Approach to Answer Non-Aggregation Questions over DBpedia</a></td></tr><tr><td colspan=3><p>We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems.</p></td></tr><tr><td>926</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_18">PROSE: A Plugin-Based Paraconsistent OWL Reasoner</a></td></tr><tr><td colspan=3><p>The study of paraconsistent reasoning with ontologies is especially important for the Semantic Web since knowledge is not always perfect within it. Quasi-classical semantics is proven to rationally draw more meaningful conclusions even from an inconsistent ontology with the stronger inference power of paraconsistent reasoning. In our previous work, we have conceived a quasi-classical framework called </p></td></tr><tr><td>927</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_14">Identifying an Agent’s Preferences Toward Similarity Measures in Description Logics</a></td></tr><tr><td colspan=3><p>In Description Logics (DLs), concept similarity measures (CSMs) aim at identifying a degree of commonality between two given concepts and are often regarded as a generalization of the classical reasoning problem of equivalence. That is, any two concepts are equivalent if their similarity degree is one, and vice versa. When two concepts are not quite equivalent but similar, nevertheless, a problem may arise as to which aspects of commonality should play more important role than others. This work presents the so-called </p></td></tr><tr><td>928</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_7">An Information Literacy Ontology and Its Use for Guidance Plan Design – An Example on Problem Solving</a></td></tr><tr><td colspan=3><p>Recently, it is very important to educate about information literacy since information techniques are rapidly developed. However, common view and definition on information literacy are not established enough. Therefore, it is required to systematize concepts related to information literacy. This article discusses an experimental development of Information Literacy Ontology and its use for guidance plan design.</p></td></tr><tr><td>929</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_12">Evaluating and Comparing Web-Scale Extracted Knowledge Bases in Chinese and English</a></td></tr><tr><td colspan=3><p>DBpedia and YAGO are the two main data sources serving as the hub of Linking Open Data (LOD), and they both contain Chinese data. Zhishi.me and SSCO extract Chinese knowledge from Wikipedia and other Chinese Encyclopedic Web sites like Baidu-Baike and Hudong-Baike. The quality of these Knowledge Bases (KBs) are not well investigated while their qualities are key to smart applications. In this paper, we evaluate three large Chinese KBs including DBpedia Chinese, zhishi.me and SSCO, and further compare them with English KBs. Since traditional methods on evaluating Web ontology can not be easily adapted to web-scale extracted KBs, we design two metric sets considering Richness and Correctness based on a quasi-formal conceptual representation to measure and compare these KBs. We also design a novel metric set on overlapped instances of different KBs to make the metric results comparable. Finally, we employ random sampling to reduce human efforts for assessing the correctness. The findings in these KBs give a detailed status report of the current situation of extracted KBs in both Chinese and English.</p></td></tr><tr><td>930</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_6">Ontology Based Inferences Engine for Veterinary Diagnosis</a></td></tr><tr><td colspan=3><p>Motivated on knowledge representation and veterinary domain this project aims at using semantic technologies to develop a tool which supports veterinary diagnosis. For this purpose an ontology based inference engine was developed following the diagnosis task definition provided by CommonKADS methodology. OWL was the language used for representing the ontologies, they were built using Protégé and processed using the Jena API. The inference engine was tested with two different ontologies. This shows the versatility of the developed tool that can easily be used to diagnose different types of diseases. This is an example of the application of CommonKADS diagnosis template using ontologies. We are currently working to make diagnoses in other domains of knowledge.</p></td></tr><tr><td>931</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_8">A Roadmap for Navigating the Life Sciences Linked Open Data Cloud</a></td></tr><tr><td colspan=3><p>Multiple datasets that add high value to biomedical research have been exposed on the web as a part of the Life Sciences Linked Open Data (LSLOD) Cloud. The ability to easily navigate through these datasets is crucial for personalized medicine and the improvement of drug discovery process. However, navigating these multiple datasets is not trivial as most of these are only available as isolated SPARQL endpoints with very little vocabulary reuse. The content that is indexed through these endpoints is scarce, making the indexed dataset opaque for users. In this paper, we propose an approach for the creation of an active Linked Life Sciences Data Roadmap, a set of configurable rules which can be used to discover links (roads) between biological entities (cities) in the LSLOD cloud. We have catalogued and linked concepts and properties from 137 public SPARQL endpoints. Our Roadmap is primarily used to dynamically assemble queries retrieving data from multiple SPARQL endpoints simultaneously. We also demonstrate its use in conjunction with other tools for selective SPARQL querying, semantic annotation of experimental datasets and the visualization of the LSLOD cloud. We have evaluated the performance of our approach in terms of the time taken and entity capture. Our approach, if generalized to encompass other domains, can be used for road-mapping the entire LOD cloud.</p></td></tr><tr><td>932</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_5">Employing </a></td></tr><tr><td colspan=3><p>Fuzzy Description Logics generalize crisp ones by providing membership degree semantics for concepts and roles by fuzzy sets. Recently, answering of conjunctive queries has been investigated and implemented in optimized reasoner systems based on the rewriting approach for crisp DLs. In this paper we investigate how to employ such existing implementations for crisp query answering in </p></td></tr><tr><td>933</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_13">Computing the Semantic Similarity of Resources in DBpedia for Recommendation Purposes</a></td></tr><tr><td colspan=3><p>The Linked Open Data cloud has been increasing in popularity, with DBpedia as a first-class citizen in this cloud that has been widely adopted across many applications. Measuring similarity between resources and identifying their relatedness could be used for various applications such as item-based recommender systems. To this end, several similarity measures such as </p></td></tr><tr><td>934</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_4">Reasoning for </a></td></tr><tr><td colspan=3><p>This works is motivated by a real-world case study where it is necessary to integrate and relate existing ontologies through </p></td></tr><tr><td>935</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_11">Automatic Generation of Semantic Data for Event-Related Medical Guidelines</a></td></tr><tr><td colspan=3><p>Medical Guidelines pay an important role in medical decision making systems. Medical guidelines are usually involved with event-related actions or procedure. However, little research has been done on how event-related medical guidelines can be converted into its semantic representation such as RDF/OWL data. This paper proposes an approach of automatic generation of semantic data for event-related medical guidelines. This generation is achieved by using the logic programming language Prolog with the support of medical ontologies such as SNOMED CT. We will report the experiments with the automatic generation of the semantic data for event-related Chinese medical guidelines, and show the relevant results.
</p></td></tr><tr><td>936</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_2">On Desirable Properties of the Structural Subsumption-Based Similarity Measure</a></td></tr><tr><td colspan=3><p>Checking for subsumption relation is the main reasoning service readily available in classical DL reasoners. With their binary response stating whether two given concepts are in the subsumption relation, it is adequate for many applications relied on the service. However, in several specific applications, there often exists the case that requires an investigation for concepts that are not directly in a subclass-superclass relation but shared some commonality. In this case, providing merely a crisp response is apparently insufficient. To achieve this, the similarity measure for DL </p></td></tr><tr><td>937</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_3">A Graph-Based Approach to Ontology Debugging in DL-Lite</a></td></tr><tr><td colspan=3><p>Ontology debugging is an important nonstandard reasoning task in ontology engineering which provides the explanations of the causes of incoherence in an ontology. In this paper, we propose a graph-based algorithm to calculate minimal incoherence-preserving subterminology (MIPS) of an ontology in a light-weight ontology language, DL-Lite. We first encode a DL-Lite ontology to a graph, then calculate all the MIPS of an ontology by backtracking some pairs of nodes in the graph. We implement the algorithm and conduct experiments over some real ontologies. The experimental results show that our debugging system is efficient and outperforms the existing systems.</p></td></tr><tr><td>938</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_1">Revisiting Default Description Logics – and Their Role in Aligning Ontologies</a></td></tr><tr><td colspan=3><p>We present a new approach to extend the Web Ontology Language (OWL) with the capabilities to reason with defaults. This work improves upon the previously established results on integrating defaults with description logics (DLs), which were shown to be decidable only when the application of defaults is restricted to named individuals in the knowledge base. We demonstrate that the application of defaults (integrated with DLs) does not have to be restricted to named individuals to retain decidability and elaborate on the application of defaults in the context of ontology alignment and ontology-based systems.</p></td></tr><tr><td>939</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_9">Leveraging Chinese Encyclopedia for Weakly Supervised Relation Extraction</a></td></tr><tr><td colspan=3><p>In the research of named-entity relation extraction based on supervision, selecting relation features for traditional methods are usually finished by people, and it’s hard to implement these methods for large-scale corpus. On the other hand, fixing relation types is the premise, so the practicabilities of these methods are not so ideal. This paper presents a weakly supervised method for Chinese named-entity relation extraction without man-made annotations, and the relation types in this method are not chosen artificially. The method collects entity relation types from the structured knowledge in encyclopedia pages, and then automatically annotates the relation instances existing in the texts based on these relation types. Simultaneously, the syntactic and semantic features of entity relations will be considered in this method, then the machine learning data will be completed, finally we use Support Vector Machine (SVM) model to train relation classifiers from training data, and these classifiers could try to extract entity relations from testing data. We carry out the experiment with the data from Chinese Baidu Encyclopedia pages, and the results show the effectiveness of this method, the overall F1 value reaches to 83.12 %. In order to probe the universality of this method, we also use the acquired relation classifiers to extract entity relations from news texts, and the results manifest that this method owns certain universality.</p></td></tr><tr><td>940</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_6">ERA-RJN: A SPARQL-Rank Based Top-k Join Query Optimization</a></td></tr><tr><td colspan=3><p>With the wide use of RDF data, searching and ranking semantic data with SPARQL has become a research hot-spot. While there is no much research work on top-k join queries optimization in RDF native stores. This paper proposes a new rank-join operator algorithm ERA-RJN on the basis of SPARQL-RANK algebra, making use of the advantage of random access availability in RDF native storage. This paper implements the ERA-RJN operator on the ARQ-RANK platform, and performs experiments, verifies the high efficiency of ERA-RJN algorithm dealing with SPARQL top-k join query in RDF native storage</p></td></tr><tr><td>941</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_10">Improving Knowledge Base Completion by Incorporating Implicit Information</a></td></tr><tr><td colspan=3><p>Over the past few years, many large Knowledge Bases (KBs) have been constructed through relation extraction technology but they are still often incomplete. As a supplement to training a more powerful extractor, Knowledge Base Completion which aims at learning new facts based on existing ones has recently attracted much attention. Most of the existing methods, however, are only utilizing the explicit facts in a single KB. By analyzing the data, we find that some implicit information should also been captured for a more comprehensive consideration during completion process. These information include the intrinsic properties of KBs (e.g. relational constraints) and potential synergies between various KBs (i.e. semantic similarity). For the former, we distinguish the missing data by using relational constraints to reduce the data sparsity. For the later, we incorporate two semantical regularizations into the learning model to encode the semantic similarity. Experimental results show that our approach is better than the methods that consider only explicit facts or only a single knowledge base, and achieves significant accuracy improvements in binary relation prediction.</p></td></tr><tr><td>942</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_5">Alignment Aware Linked Data Compression</a></td></tr><tr><td colspan=3><p>The success of linked data has resulted in a large amount of data being generated in a standard RDF format. Various techniques have been explored to generate a compressed version of RDF datasets for archival and transmission purpose. However, these compression techniques are designed to compress a given dataset without using any external knowledge, either through a compact representation or removal of semantic redundancies present in the dataset. In this paper, we introduce a novel approach to compress RDF datasets by exploiting alignments present across various datasets at both instance and schema level. Our system generates lossy compression based on the confidence value of relation between the terms. We also present a comprehensive evaluation of the approach by using reference alignment from OAEI.</p></td></tr><tr><td>943</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_7">CNME: A System for Chinese News Meta-Data Extraction</a></td></tr><tr><td colspan=3><p>News mining has gained increasing attention because of the overwhelming news produced everyday. Lots of news portals such as Sina (</p></td></tr><tr><td>944</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_8">Bootstrapping Yahoo! Finance by Wikipedia for Competitor Mining</a></td></tr><tr><td colspan=3><p>Competitive intelligence, one of the key factors of enterprise risk management and decision support, depends on knowledge bases that contain a large amount of competitive information. A variety of finance websites have collected competitive information manually, which can be used as knowledge bases. Yahoo! Finance is one of the largest and most successful finance websites among them. However, they have problems of incompleteness, lack of competitive domain, and not-in-time updating. Wikipedia, which was built with collective wisdom and contains plenty of useful information in various forms, can solve the above-mentioned problems effectively, thus helping build a more comprehensive knowledge base. In this paper, we propose a novel semi-supervised approach to identify competitor information and competitive domain from Wikipedia based on a multi-strategy learning algorithm. More precisely, we leverage seeds of competition between companies and competition between products to distantly supervise the learning process to find text patterns in free texts. Considering that competitive information can be inferred from events, we design a learning-based method to determine event description sentences. The whole process is iteratively performed. The experimental results show the effectiveness of our approach. Moreover, the results extracted from Wikipedia supplement 14,000 competitor pairs and 8,000 competitive domains between rival companies to Yahoo! Finance.</p></td></tr><tr><td>945</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_2">RDF Graph Visualization by Interpreting Linked Data as Knowledge</a></td></tr><tr><td colspan=3><p>It is known that Semantic Web and Linked Open Data (LOD) are powerful technologies for knowledge management, and explicit knowledge is expected to be presented by RDF format (Resource Description Framework), but normal users are far from RDF due to technical skills required. As we learn, a concept-map or a node-link diagram can enhance the learning ability of learners from beginner to advanced user level, so RDF graph visualization can be a suitable tool for making users be familiar with Semantic technology. However, an RDF graph generated from the whole query result is not suitable for reading, because it is highly connected like a hairball and less organized. To make a graph presenting knowledge be more proper to read, this research introduces an approach to sparsify a graph using the combination of three main functions: graph simplification, triple ranking, and property selection. These functions are mostly initiated based on the interpretation of RDF data as knowledge units together with statistical analysis in order to deliver an easily-readable graph to users. A prototype is implemented to demonstrate the suitability and feasibility of the approach. It shows that the simple and flexible graph visualization is easy to read, and it creates the impression of users. In addition, the attractive tool helps to inspire users to realize the advantageous role of linked data in knowledge management.
</p></td></tr><tr><td>946</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_3">Linked Open Vocabulary Recommendation Based on Ranking and Linked Open Data</a></td></tr><tr><td colspan=3><p>The vocabulary space of the Semantic Web includes more than 500 vocabularies according to the Linked Open Vocabularies (LOV) initiative that maintains the directory list and provides search functionality on top of the curated data. Domain experts and researchers have populated it to facilitate the interpretation and exchange of information in the Web of Data. The abundance of vocabularies and terms available in the LOV space, on one hand aims to cover the major knowledge management needs, but on the other hand it could be cumbersome for a non-expert or even a vocabulary expert to find the correct way through the collection. To address this problem, we present an approach that helps to identify the most appropriate set of LOV vocabulary terms for a given Web content context by leveraging the existing dynamics within the LOV graph and the usage patterns in the LOD cloud. The paper describes the framework architecture that enables the discovery of vocabularies; it focuses on the corresponding metrics and algorithm, and discusses the outcomes of the applied experiments.</p></td></tr><tr><td>947</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_23">Classification of News by Topic Using Location Data</a></td></tr><tr><td colspan=3><p>In this work, we will consider news articles to determine geo-localization of their information and classify their topics on the basis of an available open data source: OpenStreetMap (OSM). We propose a knowledge-based conceptual and computational approach that disambiguates place names (i.e., geo-objects and regions) mentioned in news articles in terms of geographic coordinates. The geo-located news articles are analyzed to identify local topics: we found that the mentioned geo-objects are a good proxy to classify news topics.</p></td></tr><tr><td>948</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_4">Heuristic-Based Configuration Learning for Linked Data Instance Matching</a></td></tr><tr><td colspan=3><p>Instance matching in linked data has become increasingly important because of the rapid development of linked data. The goal of instance matching is to detect co-referent instances that refer to the same real-world objects. In order to realize such instances, instance matching systems use a configuration, which specifies the matching properties, similarity measures, and other settings of the matching process. For different repositories, the configuration is varied to adapt with the particular characteristics of the input. Therefore, the automation of configuration creation is very important. In this paper, we propose </p></td></tr><tr><td>949</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_24">Monitoring and Automating Factories Using Semantic Models</a></td></tr><tr><td colspan=3><p>Keeping factories running at any time is a critical task for every manufacturing enterprise. Optimizing the flows of goods and services inside and between factories is a challenge that attracts much attention in research and business. The idea to fully describe a factory in a digital form to improve decision making is called a virtual factory. While promising virtual factory frameworks have been proposed, their semantic models lack depth and suffer from limited expressiveness. We propose an enhanced semantic model of a factory, which enables views spanning from the high level of supply chains to the low level of machines on the shop floor. The model includes a mapping to relational production databases to support federated queries on different legacy systems in use. We evaluate the model in a production line use case, demonstrating that it can be used for typical factory tasks, such as assembly line identification or machine availability checks.</p></td></tr><tr><td>950</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_20">Linking Named Entity in a Question with DBpedia Knowledge Base</a></td></tr><tr><td colspan=3><p>The emerging Linked Open Data provides an opportunity to answer the natural language question based on knowledge bases (KB). One challenge of the question answering (QA) problem is to link the entity mention in the question with the entity in the existing knowledge base. This study proposes an approach to link entity mention with a DBpedia entity. We propose an entity-centric indexing model to help search candidate entities in KB. After obtaining the candidate entities, we expand the context of the entity mention with WordNet and ConceptNet, we compute the context similarity between the expanded context and the property value of the candidate entity and the popularity of the candidate entity. Finally, we rerank the candidate entities by leveraging these features. Evaluations are performed on DBpedia version 2015, the evaluation tests show that our approach is promising in dealing with linking named entity in DBpedia.</p></td></tr><tr><td>951</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_22">Enabling Spatial OLAP Over Environmental and Farming Data with QB4SOLAP</a></td></tr><tr><td colspan=3><p>Governmental organizations and agencies have been making large amounts of spatial data available on the Semantic Web (SW). However, we still lack efficient techniques for analyzing such large amounts of data as we know them from relational database systems, e.g., multidimensional (MD) data warehouses and On-line Analytical Processing (OLAP). A basic prerequisite to enable such advanced analytics is a well-defined schema, which can be defined using the QB4SOLAP vocabulary that provides sufficient context for spatial OLAP (SOLAP). In this paper, we address the challenging problem of MD querying with SOLAP operations on the SW by applying QB4SOLAP to a non-trivial spatial use case based on real-world open governmental data sets across various spatial domains. We describe the process of combining, interpreting, and publishing disparate spatial data sets as a spatial data cube on the SW and show how to query it with SOLAP operators.</p></td></tr><tr><td>952</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_21">Hypercat RDF: Semantic Enrichment for IoT</a></td></tr><tr><td colspan=3><p>The rapid growth of sensor networks and smart devices has led to the generation of an increasing amount of information. Such information typically originates from various sources and is published in different formats. One of the key prerequisites for the Internet of Things (IoT) is interoperability. The Hypercat specification defines a lightweight JSON-based hypermedia catalogue, and is tailored towards the existing needs of industry. In this work, we propose a semantic enrichment of Hypercat, defining an RDF-based catalogue. We propose an ontology that captures the core of the Hypercat RDF specification and provides a mapping mechanism between existing JSON and proposed RDF properties. Finally, we propose a new type of search, called </p></td></tr><tr><td>953</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_19">Towards Multi-target Search of Semantic Association</a></td></tr><tr><td colspan=3><p>Semantic association represents group relationship among objects in linked data. Searching semantic associations is complicated, which involves the search of multiple objects and the search of their group relationships simultaneously. In this paper, we propose this kind of search as a multi-target search, and we compare it to traditional search tasks, which we classify as single-target search. A novel search model is introduced, and the notion of virtual document is used to extract linguistic information of semantic associations. Multi-target search is finally fulfilled by a PageRank-like ranking scheme and a top-</p></td></tr><tr><td>954</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_1">Modeling and Querying Spatial Data Warehouses on the Semantic Web</a></td></tr><tr><td colspan=3><p>The Semantic Web (SW) has drawn the attention of data enthusiasts, and also inspired the exploitation and design of multidimensional data warehouses, in an unconventional way. Traditional data warehouses (DW) operate over static data. However multidimensional (MD) data modeling approach can be dynamically extended by defining both the schema and instances of MD data as RDF graphs. The importance and applicability of MD data warehouses over RDF is widely studied yet none of the works support a spatially enhanced MD model on the SW. Spatial support in DWs is a desirable feature for enhanced analysis, since adding encoded spatial information of the data allows to query with spatial functions. In this paper we propose to empower the spatial dimension of data warehouses by adding spatial data types and topological relationships to the existing QB4OLAP vocabulary, which already supports the representation of the constructs of the MD models in RDF. With QB4SOLAP, spatial constructs of the MD models can be also published in RDF, which allows to implement spatial and metric analysis on spatial members along with OLAP operations. In our contribution, we describe a set of spatial OLAP (SOLAP) operations, demonstrate a spatially extended metamodel as, QB4SOLAP, and apply it on a use case scenario. Finally, we show how these SOLAP queries can be expressed in SPARQL.</p></td></tr><tr><td>955</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_17">Non-hierarchical Relation Extraction of Chinese Text Based on Scalable Corpus</a></td></tr><tr><td colspan=3><p>As for ontology construction from Chinese text, the non-hierarchical relation extraction is harder than the concept extraction and its extraction effect is still not satisfactory. In this paper, we put forward a scalable corpus model, which uses Tongyici Cilin and word2vec to calculate terms’ similarity and add the qualified candidate terms to the corpora. In this way we can expand the scalable corpus while extracting non-hierarchical relations. In turn, the scalable corpus that has been expanded with the new terms will facilitate the non-hierarchical relation extraction further. We carry out the experiment with Chinese texts in the domain of Computer, whose results show that with expansion of the corpus, the extraction effect will be better and better.</p></td></tr><tr><td>956</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_18">Entity Linking in Web Tables with Multiple Linked Knowledge Bases</a></td></tr><tr><td colspan=3><p>The World-Wide Web contains a large scale of valuable relational data, which are embedded in HTML tables (i.e. Web tables). To extract machine-readable knowledge from Web tables, some work tries to annotate the contents of Web tables as RDF triples. One critical step of the annotation is entity linking (EL), which aims to map the string mentions in table cells to their referent entities in a knowledge base (KB). In this paper, we present a new approach for EL in Web tables. Different from previous work, the proposed approach replaces a single KB with multiple linked KBs as the sources of entities to improve the quality of EL. In our approach, we first apply a general graph-based algorithm to EL in Web tables with each single KB. Then, we leverage the existing and newly learned “</p></td></tr><tr><td>957</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_16">PIWD: A Plugin-Based Framework for Well-Designed SPARQL</a></td></tr><tr><td colspan=3><p>In the real world datasets (e.g., DBpedia query log), queries built on well-designed patterns containing only AND and OPT operators (for short, WDAO-patterns) account for a large proportion among all SPARQL queries. In this paper, we present a plugin-based framework for all SELECT queries built on WDAO-patterns, named PIWD. The framework is based on a parse tree called </p></td></tr><tr><td>958</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_15">Data Analysis of Hierarchical Data for RDF Term Identification</a></td></tr><tr><td colspan=3><p>Generating Linked Data based on existing data sources requires the modeling of their information structure. This modeling needs the identification of potential entities, their attributes and the relationships between them and among entities. For databases this identification is not required, because a data schema is always available. However, for other data formats, such as hierarchical data, this is not always the case. Therefore, analysis of the data is required to support </p></td></tr><tr><td>959</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_14">Towards Answering Provenance-Enabled SPARQL Queries Over RDF Data Cubes</a></td></tr><tr><td colspan=3><p>The SPARQL 1.1 standard has made it possible to formulate analytical queries in SPARQL. While some approaches have become available for processing analytical queries on RDF data cubes, little attention has been paid to answering provenance-enabled queries over such data. Yet, considering provenance is a prerequisite to being able to validate if a query result is trustworthy. The main challenge for existing triple stores is the way provenance can be encoded in standard triple stores based on context values (named graphs). Hence, in this paper we analyze the suitability of existing triple stores for answering provenance-enabled queries on RDF data cubes, identify their shortcomings, and propose an index to handle the high number of context values that provenance encoding typically entails. Our experimental results using the Star Schema Benchmark show the feasibility and scalability of our index and query evaluation strategies.</p></td></tr><tr><td>960</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_10">Linked Data Collection and Analysis Platform for Music Information Retrieval</a></td></tr><tr><td colspan=3><p>There has been extensive research on music information retrieval (MIR), such as signal processing, pattern mining, and information retrieval. In such studies, audio features extracted from music are commonly used, but there is no open platform for data collection and analysis of audio features. Therefore, we build the platform for the data collection and analysis for MIR research. On the platform, we represent the music data with Linked Data, which are in a format suitable for computer processing, and also link data fragments to each other. By adopting the Linked Data, the music data will become easier to publish and share, and there is an advantage that complex music analysis will be facilitated. In this paper, we first investigate the frequency of the audio features used in previous studies on MIR for designing the Linked Data schema. Then, we build a platform, that automatically extracts the audio features and music metadata from YouTube URIs designated by users, and adds them to our Linked Data DB. Finally, the sample queries for music analysis and the current record of music registrations in the DB are presented.</p></td></tr><tr><td>961</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_13">ASPG: Generating OLAP Queries for SPARQL Benchmarking</a></td></tr><tr><td colspan=3><p>The increasing use of data analytics on Linked Data leads to the requirement for SPARQL engines to efficiently execute Online Analytical Processing (OLAP) queries. While SPARQL 1.1 provides basic constructs, further development on optimising OLAP queries lacks benchmarks that mimic the data distributions found in Link Data. Existing work on OLAP benchmarking for SPARQL has usually adopted queries and data from relational databases, which may not well represent Linked Data. We propose an approach that maps typical OLAP operations to SPARQL and a tool named ASPG to automatically generate OLAP queries from real-world Linked Data. We evaluate ASPG by constructing a benchmark called DBOBfrom the online DBpedia endpoint, and use DBOB to measure the performance of the Virtuoso engine.</p></td></tr><tr><td>962</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_11">Semantic Data Acquisition by Traversing Class–Class Relationships Over Linked Open Data</a></td></tr><tr><td colspan=3><p>Linked Open Data (LOD), a powerful mechanism for linking different datasets published on the World Wide Web, is expected to increase the value of data through mashups of various datasets on the Web. One of the important requirements for LOD is to be able to find a path of resources connecting two given classes. Because each class contains many instances, inspecting all of the paths or combinations of the instances results in an explosive increase of computational complexity. To solve this problem, we have proposed an efficient method that obtains and prioritizes a comprehensive set of connections over resources by traversing class–class relationships of interest. To put our method into practice, we have been developing a tool for LOD exploration. In this paper, we introduce the technologies used in the tool, focusing especially on the development of a measure for predicting whether a path of class–class relationships has connected triples or not. Because paths without connected triples can be predicted and removed, using the prediction measure enables us to display more paths from which users can obtain data that interests them.</p></td></tr><tr><td>963</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_9">A Preliminary Investigation Towards Improving Linked Data Quality Using Distance-Based Outlier Detection</a></td></tr><tr><td colspan=3><p>With more and more data being published on the Web as Linked Data, Web Data quality is becoming increasingly important. While quite some work has been done with regard to quality assessment of Linked Data, only few works have addressed quality improvement. In this article, we present a preliminary an approach for identifying potentially incorrect RDF statements using distance-based outlier detection. Our method follows a three stage approach, which automates the whole process of finding potentially incorrect statements for a certain property. Our preliminary evaluation shows that a high precision is maintained with different settings.</p></td></tr><tr><td>964</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_8">RIKEN MetaDatabase: A Database Platform as a Microcosm of Linked Open Data Cloud in the Life Sciences</a></td></tr><tr><td colspan=3><p>The amount and heterogeneity of life-science datasets published on the Web have considerably increased recently. However, biomedical scientists face numerous serious difficulties in finding, using and publishing useful databases. In order to solve these issues, we developed a Resource Description Framework-based database platform, called RIKEN MetaDatabase, which allows biologists to easily develop, publish and integrate databases. The platform manages metadata of both research data and individual data described with standardised vocabularies and ontologies, and has a simple browser-based graphical user interface for viewing data including tabular and graphical views. The platform was released in April 2015, and 110 databases including mammalian, plant, bioresource and image databases with 21 ontologies have been published through this platform as of July 2016. This paper describes the technical knowledge obtained through the development and operation of RIKEN MetaDatabase as a challenge for accelerating life-science data distribution promotion.</p></td></tr><tr><td>965</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_7">A MapReduce-Based Approach for Prefix-Based Labeling of Large XML Data</a></td></tr><tr><td colspan=3><p>A massive amount of XML (Extensible Markup Language) data is available on the web, which can be viewed as tree data. One of the fundamental building blocks of information retrieval from tree data is answering structural queries. Various labeling schemes have been suggested for rapid structural query processing. We focus on the prefix-based labeling scheme that labels each node with a concatenation of its parent’s label and its child order. This scheme has been adapted in RDF (Resource Description Framework) data management systems that index RDF data in tree by grouping subjects. Recently, a MapReduce-based algorithm for the prefix-based labeling scheme was suggested. We observe that this algorithm fails to keep label size minimized, which makes the prefix-based labeling scheme difficult for massive real-world XML datasets. To address this issue, we propose a MapReduce-based algorithm for prefix-based labeling of XML data that reduces label size by adjusting the order of label assignments based on the structural information of the XML data. Experiments with real-world XML datasets show that the proposed approach is more effective than previous works.</p></td></tr><tr><td>966</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_5">Compression Algorithms for Log-Based Recovery in Main-Memory Data Management</a></td></tr><tr><td colspan=3><p>
With the dramatic increases in performance requirement of computer hardware and decreases in its cost in recent years, the relevant research in main-memory database is becoming more and more popular and has a prosperous future. Log-based recovery, which is one of its most important research directions, is a set of problems accompanied by volatile memory. Its problem of stagnation in memory/CPU resulted from the slow I/O speed of non-volatile storage now needs to be addressed urgently. However, there is no specific platform for log-based recovery research. So the study aims to address this issue.
</p></td></tr><tr><td>967</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_4">SQuaRE: A Visual Approach for Ontology-Based Data Access</a></td></tr><tr><td colspan=3><p>We present the SPARQL Query and R2RML mappings Environment (SQuaRE) which provides a visual interface for creating mappings expressed in R2RML. SQuaRE is a web-based tool with easy to use interface that can be applied in the ontology-based data access applications. We describe SQuaRE’s main features, its architecture as well as implementation details. We compare SQuaRE with other similar tools and describe our future development plans.</p></td></tr><tr><td>968</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_2">Inquiry into RDF and OWL Semantics</a></td></tr><tr><td colspan=3><p>The purpose of this paper is to present the higher order formalization of RDF and OWL with setting up ontological meta-modeling criteria through the discussion of Russell’s </p></td></tr><tr><td>969</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_3">Designing of Ontology for Domain Vocabulary on Agriculture Activity Ontology (AAO) and a Lesson Learned</a></td></tr><tr><td colspan=3><p>This paper proposes Agriculture Activity Ontology (AAO) as a basis of the core vocabulary of agricultural activity. Since concepts of agriculture activities are formed by the various context such as purpose, means, crop, and field, we organize the agriculture activity ontology as a hierarchy of concepts discriminated by various properties such as purpose, means, crop and field. The vocabulary of agricultural activity is then defined as the subset of the ontology. Since the ontology is consistent, extendable, and capable of some inferences thanks to Description Logics, so the vocabulary inherits these features. The vocabulary is also linked to existing vocabularies such as AGROVOC. It is expected to use in the data format in the agricultural IT system. The vocabulary is adopted as the part of “the guideline for agriculture activity names for agriculture IT systems” issued by Ministry of Agriculture, Forestry and Fisheries (MAFF), Japan. Also we investigated the usefulness of the ontology as the method for defining the domain vocabulary.</p></td></tr><tr><td>970</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_6">An Empirical Study on Property Clustering in Linked Data</a></td></tr><tr><td colspan=3><p>Properties are used to describe entities, a part of which are likely to be clustered together to constitute an aspect. For example, </p></td></tr><tr><td>971</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_1">How Can Reasoner Performance of ABox Intensive Ontologies Be Predicted?</a></td></tr><tr><td colspan=3><p>Reasoner performance prediction of ontologies in OWL 2 language has been studied so far from different dimensions. One key aspect of these studies has been the prediction of how much time a particular task for a given ontology will consume. Several approaches have adopted different machine learning techniques to predict time consumption of ontologies already. However, these studies focused on capturing general aspects of the ontologies (i.e., mainly the complexity of their TBoxes), while paying little attention to ABox intensive ontologies. To address this issue, in this paper, we propose to improve the representativeness of ontology metrics by developing new metrics which focus on the ABox features of ontologies. Our experiments show that the proposed metrics contribute to overall prediction accuracy for all ontologies in general without causing side-effects.</p></td></tr><tr><td>972</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_32">A Comparison of Unsupervised Taxonomical Relationship Induction Approaches for Building Ontology in RDF Resources</a></td></tr><tr><td colspan=3><p>Automatically generated ontology can describe the relationship of meta-data in Linked Data or other RDF resources generated from programs, and advances the utility of the data sets. Hierarchical document clustering methods used to generate concept hierarchies from retrieved documents or social tags can be used for constructing taxonomy or ontology for Linked Data and RDF documents. This paper introduces a framework for building an ontology using the hierarchical document clustering methods and compares the performance of three classic algorithms that are UPGMA, Subsumption, and EXT for building the ontology. The experiment shows EXT is the best algorithm to build the ontology for RDF resources and demonstrates that the quality of the ontology generated can be affected by the number of concepts that are used to represent the entities and to formalize the classes in the ontology.</p></td></tr><tr><td>973</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_31">Reasoning Driven Configuration of Linked Data Content Management Systems</a></td></tr><tr><td colspan=3><p>The web of data has continued to expand thanks to the principles of Linked Data, increasing its impact on the web both in its depth and range of data sources. However tools allowing ordinary web users to contribute to this web of data are still lacking. In this paper we propose Linked Data CMS, an approach allowing existing web content management system (CMS) software to be configured to display a web site based on a group of ontology classes, by making use of a </p></td></tr><tr><td>974</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_30">Concept Learning Algorithm for Semantic Web Based on the Automatically Searched Refinement Condition</a></td></tr><tr><td colspan=3><p>Today, the web is the huge data repository which contains excessively growing with uncountable size of data. From the view point of data, Semantic Web is the advanced version of World Wide Web, which aims machine understandable web based on the structured data. For the advent of Semantic Web, its data has been rapidly increased with various areas. In this paper, we proposed novel decision tree algorithm, which called Semantic Decision Tree, to learning the covered knowledge beyond the Semantic Web based ontology. For this purpose, we newly defined six different refinements based on the description logic constructors. Refinements are replaced the features of traditional decision tree algorithms, and these refinements are automatically searched by our proposed decision tree algorithm based on the structure information of ontology. Additional information from the ontology is also used to enhance the quality of decision tree results. Finally, we test our algorithm by solving the famous rule induction problems, and we can get perfect answers with useful decision tree results. In addition, we expect that our proposed algorithm has strong advantage to learn decision tree algorithm on complex and huge size of ontology.</p></td></tr><tr><td>975</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_26">MAPSOM: User Involvement in Ontology Matching</a></td></tr><tr><td colspan=3><p>This paper presents a semi-automatic similarity aggregating system for ontology matching problem. The system consists of two main parts. The first part is aggregation of similarity measures with the help of self-organizing map. The second part incorporates user feedback for refining self-organizing map outcomes. The system calculates different similarity measures (e.g., string-based similarity measure, WordNet-based similarity measure...) to cover different causes of semantic heterogeneity. The next step is similarity aggregation by means of the self-organizing map and the ward clustering. The final step is the active learning phase for results tuning. We implemented this idea as MAPSOM framework. Our experimental results show that MAPSOM framework can be used for problems where the highest precision is needed.</p></td></tr><tr><td>976</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_29">An Automatic sameAs Link Discovery from Wikipedia</a></td></tr><tr><td colspan=3><p>Spelling variants of words or word sense ambiguity takes many costs in such processes as Data Integration, Information Searching, data preprocessing for Data Mining, and so on. It is useful to construct relations between a word or phrases and a representative name of the entity to meet these demands. To reduce the costs, this paper discusses how to automatically discover “sameAs” and “meaningOf” links from Japanese Wikipedia. In order to do so, we gathered relevant features such as IDF, string similarity, number of hypernym, and so on. We have identified the link-based score on salient features based on SVM results with 960,000 anchor link pairs. These case studies show us that our link discovery method goes well with more than 70 % precision/recall rate.</p></td></tr><tr><td>977</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_28">An Automatic Instance Expansion Framework for Mapping Instances to Linked Data Resources</a></td></tr><tr><td colspan=3><p>Linked Data is an utterly valuable component for semantic technologies because it can be used for accessing and distributing knowledge from one data source to other data sources via structured links. Therefore, mapping instances to Linked Data resources plays a key role for consuming knowledge in Linked Data resources so that we can understand instances more precisely. Since an instance, which can be aligned to Linked Data resources, is enriched its information by other instances, the instance then is full of information, which perfectly describes itself. Nevertheless, mapping instances to Linked Data resources is still challenged due to the heterogeneity problem and the multiple data source problem as well. Most techniques focus on mapping instances between two specific data sources and deal with the heterogeneity problem. Mapping instances particularly relying on two specific data sources is not enough because it will miss an opportunity to map instances to other sources. We therefore present the Instance Expansion Framework, which automatically discover and map instances more than two specific data sources in Linked Data resources. The framework consists of three components: Candidate Selector, Instance Matching and Candidate Expander. Experiments show that the Candidate Expander component is significantly important for mapping instances to Linked Data resources.</p></td></tr><tr><td>978</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_25">Utilizing Weighted Ontology Mappings on Federated SPARQL Querying</a></td></tr><tr><td colspan=3><p>Technologies to allow the heterogeneity of ontologies on the web of data have a key role on the emergence of Linked Open Data. Much research exists on generating better ontology mappings, and also they produce ‘weights’ (i.e. confidence) of each generated mapping. Although the semantics of such weighted ontology mappings has been discussed, how they can effectively be used in querying remains as a crucial issue. In this paper we show how such weighted ontology mappings can effectively be used in SPARQL-based querying on heterogeneous data sources by slightly extending the syntax of SPARQL query language. We show how such an extended query can be translated to a standard SPARQL query and how the extended query can customize the behavior of processing the query by reflecting the user’s demands.</p></td></tr><tr><td>979</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_24">Personalized Search System Based on User Profile</a></td></tr><tr><td colspan=3><p>With the development of Web technologies and the improvement of information technology standards, Internet has entered an age of information explosion. However, extraneous information is displayed on the top of the search results and the user interest in the search results in a text match without or seldom taking into account the search intents of the users. For most of the search engines, they either cannot become aware of the user interest properly or cannot find the information which users need efficiently. In our study, we solve these problems. We store users’ search history in the user profile, and relocate the results of search history by the particular subject. The proposed method can provide a personalized search service that imparts higher priority to the user documentation saw, which is positioned at the top of the search results. On the basis of the proposed method, we developed a system with which the corresponding experiment has been performed to verify our proposed method. The experiment result shows the validity of our proposed method and the importance of personalized search.</p></td></tr><tr><td>980</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_27">Automatic and Dynamic Book Category Assignment Using Concept-Based Book Ontology</a></td></tr><tr><td colspan=3><p>We propose concept-based book ontology for automatic and dynamic category assignment to books through collaborative filtering. It is general for authors or book systems to assign one or more categories to books, but determining book categories based on book reviews have long been neglected. Popularization of online reviews has generated abundant reviews, and it is valuable to additively consider these reviews for assigning relevant book categories. The proposed concept-based book ontology is constructed by conceptual categories that are extracted from the existing book category hierarchy using semantic relationships. Moreover, category-specific review words are constructed through collaborative filtering with the semantically related review words. We built an automatic and dynamic book category assignment prototype system using the concept-based book ontology with the Amazon book department data and confirmed the effectiveness of our approach through empirical evaluations.</p></td></tr><tr><td>981</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_19">A Formal Model for RDF Dataset Constraints</a></td></tr><tr><td colspan=3><p>Linked Data has forged new ground in developing easy-to-use, distributed databases. The prevalence of this data has enabled a new genre of social and scientific applications. At the same time, Semantic Web technology has failed to significantly displace SQL or XML in industrial applications, in part because it offers no equivalent schema publication and enforcement mechanisms to ensure data consistency. The RDF community has recognized the need for a formal mechanism to publish verifiable assertions about the structure and content of RDF Graphs, RDF Datasets and related resources. We propose a formal model that could serve as a foundation for describing the various types invariants, pre- and post-conditions for RDF datasets and then demonstrate how the model can be used to analyze selected example constraints.</p></td></tr><tr><td>982</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_20">Location-Based Mobile Recommendations by Hybrid Reasoning on Social Media Streams</a></td></tr><tr><td colspan=3><p>In this paper, we introduce BOTTARI: an augmented reality application that offers personalized and location-based recommendations of Point Of Interests based on sentiment analysis with geo-semantic query and reasoning. We present a mobile recommendation platform and application working on semantic technologies (knowledge representation and query for geo-social data, and inductive and deductive stream reasoning), and the lesson learned in deploying BOTTARI in Insadong. We have been collecting and analyzing tweets for three years to rate the few hundreds of restaurants in the district. The results of our study show the commercial feasibility of BOTTARI.</p></td></tr><tr><td>983</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_23">Ranking the Results of DBpedia Retrieval with SPARQL Query</a></td></tr><tr><td colspan=3><p>In recent years, a number of Semantic Web databases have been actively open to public as common resources on the Web by the effort of Linked Open Data community project. Due to this, we need a good method to search necessary data from those databases, depending on various purposes. In this study, we propose two methods to rank the results retrieved by a SPARQL query (especially, a SELECT query), using the information about the frequency of each property in a data set and the links between RDF resources. In order to evaluate our proposed methods, we set two cases for using SPARQL queries, and then rank the query results in each case. The usefulness of our proposed method has been confirmed by subject experiments.</p></td></tr><tr><td>984</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_22">XML Multi-core Query Optimization Based on Task Preemption and Data Partition</a></td></tr><tr><td colspan=3><p>In XML query optimization, most algorithms still use the traditional serial mode, so they can hardly take full advantage of multi-core resources. This paper proposes data partition for the XML database so that the new approach reaches load balance between different cores. Each thread processes the sub-regional data independently to reduce synchronization and communication overhead between cores. This paper also discusses the usage of task preemption in multi-core querying. According to experiments, our strategy requires less time consumption and gains better workload balance than both NBP and SBP.</p></td></tr><tr><td>985</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_18">TLDRet: A Temporal Semantic Facilitated Linked Data Retrieval Framework</a></td></tr><tr><td colspan=3><p>Temporal features, such as date and time or time of an event, employ concise semantics for any kind of information retrieval, and therefore for linked data information retrieval. However, we have found that most linked data information retrieval techniques pay little attention on the power of temporal feature inclusion. We propose a keyword-based linked data information retrieval framework, called TLDRet, that can incorporate temporal features and give more concise results. Preliminary evaluation of our system shows promising performance.</p></td></tr><tr><td>986</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_16">Constructing Event Corpus from Inverted Index for Sentence Level Crime Event Detection and Classification</a></td></tr><tr><td colspan=3><p>Event detection identifies interesting events from web pages and in this paper, a new approach is proposed to identify the event instances associated with an interested event type. The terms that are related to criminal activities, its co-occurrence terms and the associated sentences are considered from web documents. These sentence patterns are processed by POS tagging. Since, there is no knowledge on the sentences for the first instances, they are clustered using decision tree. Rules are formulated using pattern clusters. Priorities are assigned to the clusters based on the importance of patterns. The importance of the patterns defines the semantic relation towards event instances. Considering the priorities, weights are assigned for the rules. Artificial Neural Network (ANN) is used to classify the sentences to detect event instances based on the gained knowledge. Here ANN is used for training the weighted sentence patterns to learn the event instances of the specific event type. It is observed that the constructed rule is effective in classifying the sentences to identify event instance. The combination of these sentence patterns of the event instances are updated into the corpus. The proposed approach is encouraging when compared with other comparative approaches.</p></td></tr><tr><td>987</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_15">Constructing City Ontology from Expert for Smart City Management</a></td></tr><tr><td colspan=3><p>City Ontology plays an important role in smart city management for data integration, reasoning decision support etc. With these managerial domain knowledge scattered among a large number of experts, researchers face a huge challenge constructing a complete ontology for city management. This paper presents a simple yet efficient method for non-computer science experts to construct an ontology. We use a middle part that acts as a transition layer called activity model which is later merged into the city managerial ontology. We prove the effectiveness of this method by constructing a managerial ontology for two departments in Karamay’s Smart City Program.
</p></td></tr><tr><td>988</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_21">Towards Exploratory Relationship Search: A Clustering-Based Approach</a></td></tr><tr><td colspan=3><p>Searching and browsing relationships between entities is an important task in many domains. RDF facilitates searching by explicitly representing a relationship as a path in a graph with meaningful labels. As the Web of RDF data grows, hundreds of relationships can be found between a pair of entities, even under a small length constraint and within a single data source. To support users with various information needs in interactively exploring a large set of relationships, existing efforts mainly group the results into faceted categories. In this paper, we practice another direction of exploratory search, namely clustering. Our approach automatically groups relationships into a dynamically generated hierarchical clustering according to their schematic patterns, which also meaningfully label these clusters to effectively guide exploration and discovery. To demonstrate it, we implement our approach in the RelClus system based on DBpedia, and conduct a preliminary user study as well as a performance testing.</p></td></tr><tr><td>989</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_17">Parallel OWL Reasoning: Merge Classification</a></td></tr><tr><td colspan=3><p>Our research is motivated by the ubiquitous availability of multiprocessor computers and the observation that available </p></td></tr><tr><td>990</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_13">Ontology Construction Support for Specialized Books</a></td></tr><tr><td colspan=3><p>In this paper, we present a support system for ontology construction just based on a given specialized book with lowest possible cost. The system tries to combine minimum hand and mostly automatic constructions. It extracts required information for the ontology design from the specialized book with presenting yes-no selections to an expert of the specialized book. The ontology construction is performed automatically just using the answers of the expert. The constructed ontology is reliably and highly technical since it is constructed on the basis of the specialized book. In addition, since user operations are restricted only to yes-no selection, any expert can make use of our system without any special knowledge about ontology.</p></td></tr><tr><td>991</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_12">Advanced Semantic Web Services for Diet Therapy with Linked Data and Mediation Rules</a></td></tr><tr><td colspan=3><p>Instead of conventional Semantic Web Services, such as OWL-S and WSMO, LOSs (Linked Open Services) recently come up with more LOD (Linked Open Data) activities. Here is discussed how to construct advanced LOS architecture with LOD and mediation rules that bridge the gaps between different LOD endpoints and then how well the service goes well with the questionnaire survey of a diabetic. Furthermore, the LOS architecture should be evaluated by function extendibility and modifiability to another task.</p></td></tr><tr><td>992</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_9">Federating Heterogeneous Biological Resources on the Web: A Case Study on TRP Channel Ontology Construction</a></td></tr><tr><td colspan=3><p>TRP (Transient receptor potential) channel is a biological component which could be of factors in severe diseases such as heart attack and cancer. In order for researchers to easily search for protein-protein interactions for mammalian TRP channel, TRIP Database was created. However, TRIP Database does not contain information about proteins in details, making researchers in turn visit other database services such as UniProt and PDB. In this paper, we propose a semantic TRP Ontology made from TRIP Database, allowing users to be given the collected contents from other relevant databases as well using a single request. As a practical scenario, we generate RDF triples from TRIP Database by referring to TRP Ontology designed to have links with UniProt. A federated way of collecting contents from two different services is proposed.</p></td></tr><tr><td>993</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_10">Publishing a Disease Ontologies as Linked Data</a></td></tr><tr><td colspan=3><p>Publishing open data as linked data is a significant trend in not only the Semantic Web community but also other domains such as life science, government, media, geographic research and publication. One feature of linked data is the instance-centric approach, which assumes that considerable linked instances can result in valuable knowledge. In the context of linked data, ontologies offer a common vocabulary and schema for RDF graphs. However, from an ontological engineering viewpoint, some ontologies offer systematized knowledge, developed under close cooperation between domain experts and ontology engineers. Such ontologies could be a valuable knowledge base for advanced information systems. Although ontologies in RDF formats using OWL or RDF(S) can be published as linked data, it is not always convenient to use other applications because of the complicated graph structures. Consequently, this paper discusses RDF data models for publishing ontologies as linked data. As a case study, we focus on a disease ontology in which diseases are defined as causal chains.</p></td></tr><tr><td>994</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_11">Adapting Gloss Vector Semantic Relatedness Measure for Semantic Similarity Estimation: An Evaluation in the Biomedical Domain</a></td></tr><tr><td colspan=3><p>Automatic methods of ontology alignment are essential for establishing interoperability across web services. These methods are needed to measure semantic similarity between two ontologies’ entities to discover reliable correspondences. While existing similarity measures suffer from some difficulties, semantic relatedness measures tend to yield better results; even though they are not completely appropriate for the ‘equivalence’ relationship (e.g. “</p></td></tr><tr><td>995</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_8">TANLION – TAmil Natural Language Interface for Querying ONtologies</a></td></tr><tr><td colspan=3><p>An ontology contains numerous information in a formal way which cannot be easily understood by casual users. Rendering a natural language interface to ontologies will be more useful for such users, as it allows them to retrieve the necessary information without knowing about the formal specifications existing in the ontologies. Until now, most such interfaces have only been built for accepting user queries in English. Besides English, providing interface to ontologies in other native and regional languages should also be explored, as it enables the casual users to gather information from an ontology without any language barrier. One of the most popular languages in South Asia is Tamil, a classical language. In this paper, we present our research experience in developing TANLION, a Tamil interface for querying ontologies. It accepts a Tamil query from an end-user and tries to recognize the information that the user needs. If that information is available in the ontology, our system retrieves it and presents to the user in Tamil.</p></td></tr><tr><td>996</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_14">Belief Base Revision for Datalog+/- Ontologies</a></td></tr><tr><td colspan=3><p>Datalog+/- is a family of emerging ontology languages that can be used for representing and reasoning over lightweight ontologies in Semantic Web. In this paper, we propose an approach to performing belief base revision for Datalog+/- ontologies. We define a kernel based belief revision operator for Datalog+/- and study its properties using extended postulates, as well as an algorithm to revise Datalog+/- ontologies. Finally, we give the complexity results by showing that query answering for a revised linear Datalog+/- ontology is tractable.</p></td></tr><tr><td>997</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_6">Toward Interlinking Asian Resources Effectively: Chinese to Korean Frequency-Based Machine Translation System</a></td></tr><tr><td colspan=3><p>Interlinking Asian resources on the Web is a significant, but mostly unexplored and undeveloped task. Toward the goal of interlinking Asian online resources effectively, we propose a novel method that links Chinese and Korean resources together on the basis of a new machine translation system, which is built upon a frequency-based model operated through the Google Ngram Viewer. The study results show that Chinese characters can be mapped to corresponding Korean characters with the average accuracy of 73.1 %. This research is differentiated from the extant research by focusing on the Chinese pronunciation system called Pinyin. The proposed approach is directly applicable to voice translation applications as well as textual translations applications.</p></td></tr><tr><td>998</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_5">Implementing Mobility Service Based on Japanese Linked Data</a></td></tr><tr><td colspan=3><p>This study aims at developing a web service with Japanese Linked Data and evaluating the service. In Japan, government sets Open Data as a new strategy in Information Technology field and focuses on “Linked Open Data (LOD)” [</p></td></tr><tr><td>999</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_4">Towards Sentiment Analysis on Parliamentary Debates in Hansard</a></td></tr><tr><td colspan=3><p>This paper reports on our ongoing work on the analysis of sentiments, i.e., individual and collective stances, in Hansard (Hansard is a publicly available transcript of UK Parliamentary debates). Significant work has been carried out in the area of sentiment analysis particularly on reviews and social media but less so on political transcripts and debates. Parliamentary transcripts and debates are significantly different from blogs and reviews, e.g., the presence of sarcasm, interjections, irony and digression from the topic are commonplace increasing the complexity and difficulty in applying standard sentiment analysis techniques. In this paper we present our sentiment analysis methodology for parliamentary debate using known lexical and syntactic rules, word associations for the creation of a heuristic classifier capable of identifying sentiment carrying sentences and MP stance.</p></td></tr><tr><td>1000</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_7">CASIA-KB: A Multi-source Chinese Semantic Knowledge Base Built from Structured and Unstructured Web Data</a></td></tr><tr><td colspan=3><p>Knowledge bases play a crucial role in intelligent systems, especially in the Web age. Many domain dependent and general purpose knowledge bases have been developed to support various kinds of applications. In this paper, we propose the CASIA-KB, a Chinese semantic knowledge base built from various Web resources. CASIA-KB utilizes Semantic Web and Natural Language Processing techniques and mainly focuses on declarative knowledge. Most of the knowledge is textual knowledge extracted from structured and unstructured sources, such as Web-based Encyclopedias (where more formal and static knowledge comes from), Microblog posts and News (where most updated factual knowledge comes from). CASIA-KB also aims at bringing in images and videos (which serve as non-textual knowledge) as relevant knowledge for specific instances and concepts since they bring additional interpretation and understanding of textual knowledge. For knowledge base organization, we briefly discussed the current ontology of CASIA-KB and the entity linking efforts for linking semantically equivalent entities together. In addition, we build up a SPARQL endpoint with visualization functionality for query processing and result presentation, which can produce query output in different formats and with result visualization supports. Analysis on the entity degree distributions of each individual knowledge source and the whole CASIA-KB shows that each of the branch knowledge base follows power law distribution and when entities from different resources are linked together to build a merged knowledge base, the whole knowledge base still keeps this structural property.</p></td></tr><tr><td>1001</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_3">Ontology-Based Information System</a></td></tr><tr><td colspan=3><p>We describe a novel way for creating information systems based on ontologies. The described solution is aimed at domain experts who would benefit from being able to quickly prototype fully-functional, web-based information system for data input, editing and analysis. The systems backbone is SPARQL 1.1 endpoint that enables organization users to view and edit the data, while outside users can get read-only access to the endpoint. The system prototype is implemented and successfully tested with Latvian medical data ontology with 60 classes and imported 5 000 000 data-level triples.</p></td></tr><tr><td>1002</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_2">Context-Aware Music Recommendation with Serendipity Using Semantic Relations</a></td></tr><tr><td colspan=3><p>A goal for the creation and improvement of music recommendation is to retrieve users’ preferences and select the music adapting to the preferences. Although the existing researches achieved a certain degree of success and inspired future researches to get more progress, problem of the cold start recommendation and the limitation to the similar music have been pointed out. Hence we incorporate concept of serendipity using ‘renso’ alignments over Linked Data to satisfy the users’ music playing needs. We first collect music-related data from Last.fm, Yahoo! Local, Twitter and LyricWiki, and then create the ‘renso’ relation on the Music Linked Data. Our system proposes a way of finding suitable but novel music according to the users’ contexts. Finally, preliminary experiments confirm balance of accuracy and serendipity of the music recommendation.</p></td></tr><tr><td>1003</td><td>2013</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-06826-8_1">Semantic Web Services for University Course Registration</a></td></tr><tr><td colspan=3><p>Semantic web services, with proper security procedures, have the potential to open up the computing infrastructure of an organization for smooth integration with external applications in a very controlled way, and at a very fine level of granularity. Furthermore, the process of using the provided functionality, consisting of discovery, invocation and execution of web services may be automated to a large extent. In this paper, we show how semantic web services and service-oriented computing can facilitate this integration in the education domain. Specifically, we use the Rule variant of Web Services Modeling Language (WSML) to semantically specify the functionality of web services for on-line course registration, a goal for consuming the provided functionality, as well as the ontologies needed for a shared terminology between the service and goal.</p></td></tr><tr><td>1004</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_28">Building Linked Open University Data: Tsinghua University Open Data as a Showcase</a></td></tr><tr><td colspan=3><p>Linked Open University Data applies semantic web and linked data technology to university data scenario, aiming at building interlinked semantic data around university information, providing possibility for unified inner- and inter- school information query and comparison. This paper proposes a general process of building linked open university data, with procedures covering choosing datasets and vocabularies, collecting and processing data, building RDF and interlink, etc. Tsinghua University Open Data is used to demonstrate the process. Tsinghua University consist of 5 well-formed, interconnected datasets, with a number of interesting applications has been built on top of them. Finally, remarkable points about data collecting and processing is discussed.</p></td></tr><tr><td>1005</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_24">Learning Complex Mappings between Ontologies</a></td></tr><tr><td colspan=3><p>In this paper, we introduce a new approach for constructing complex mappings between ontologies by transforming it to a rule learning process. Derived from the classical Inductive Logic Programming, our approach uses instance mappings as training data and employs tailoring heuristics to improve the learning efficiency. Empirical evaluation shows that our generated Horn-rule mappings are meaningful.</p></td></tr><tr><td>1006</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_25">Discovering and Ranking New Links for Linked Data Supplier</a></td></tr><tr><td colspan=3><p>For new data supplier who wants to join the web of data club, it’s difficult to find new links between local repository and data sets in the web of data to make local data well-connected or harmonize with other data. The purpose of this research is not for finding similar entities but discovering new potential link for helping users have more choice for using multiple links instead of only using “owl:sameAs”. The approach use information retrieval technique index the data sets and Page Rank and graph theory analyze RDF document to filter links. We implemented our method using Dbpedia data sets and two open ontologies, the results showed our approach can discover new links with highly accuracy.</p></td></tr><tr><td>1007</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_26">Probabilistic Multi-Context Systems</a></td></tr><tr><td colspan=3><p>The concept of contexts is widely used in artificial intelligence. Several recent attempts have been made to formalize multi-context systems (MCS) for ontology applications. However, these approaches are unable to handle probabilistic knowledge. This paper introduces a formal framework for representing and reasoning about uncertainty in multi-context systems (called p-MCS). Some important properties of p-MCS are presented and an algorithm for computing the semantics is developed. Examples are also used to demonstrate the suitability of p-MCS.</p></td></tr><tr><td>1008</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_22">GoRelations: An Intuitive Query System for DBpedia</a></td></tr><tr><td colspan=3><p>Although a formal query language, SPARQL, is available for accessing DBpedia, it remains challenging for users to query the knowledge unless they are familiar with the syntax of SPARQL and the underlying ontology. We have developed both an intuitive </p></td></tr><tr><td>1009</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_21">RDFa</a></td></tr><tr><td colspan=3><p>RDFa is a syntactic format that allows RDF triples to be integrated into hypertext content of HTML/XHTML documents. Although a growing number of methods or tools have been designed attempting at generating or digesting RDFa, comparatively little work has been carried out on finding a generic solution for publishing existing RDF data sets with the RDFa serialisation format. This paper proposes a generic and lightweight approach to generating semantically-enriched hypertext content by embedding RDF triples derived from diverse provenances in terms of a concept of topic nodes which will be automatically recommended by our discovery algorithm. RDFa</p></td></tr><tr><td>1010</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_20">Interacting with Linked Data via Semantically Annotated Widgets</a></td></tr><tr><td colspan=3><p>The continuous growth of the Linked Data Web brings us closer to the original vision of the Web as an interconnected network of machine-readable resources. There is, however, an essential aspect in principle still missing from this vision, i.e., the ability for the Web user to interact directly with the Linked Data in a read/write manner. In this paper we introduce a lifecycle and associated mechanism to enable a domain-agnostic read/write interaction with Linked Data in the context of a single data provider. Our solution uses an ontology to build a binding front-end for a given RDF model, in addition to RDFa to maintain the semantics of the resulting form/widget components. On the processing side, a RESTful Web service is provided to seamlessly manage semantic widgets and their associated data, and hence enable the read/write data interaction mechanism. The evaluation shows that the generation process presents no performance issues, while the content overhead required for the actual form-data binding is kept to a minimum.</p></td></tr><tr><td>1011</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_19">What Should I Link to? Identifying Relevant Sources and Classes for Data Linking</a></td></tr><tr><td colspan=3><p>With more data repositories constantly being published on the Web, choosing appropriate data sources to interlink with newly published datasets becomes a non-trivial problem. It is necessary to choose both the repositories to link to and the relevant subsets of these repositories, which contain potentially matching individuals. In order to do this, detailed information about the content and structure of semantic repositories is often required. However, retrieving and processing such information for a potentially large number of datasets is practically unfeasible. In this paper, we propose an approach which utilises an existing semantic web index in order to identify potentially relevant datasets for interlinking and rank them. Furthermore, we adapt instance-based ontology schema matching to extract relevant subsets of selected data source and, in this way, pre-configure data linking tools.</p></td></tr><tr><td>1012</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_17">Knowledge-Driven Diagnostic System for Traditional Chinese Medicine</a></td></tr><tr><td colspan=3><p>Recognizing diseases from theoretical perspective can help ordinary people have a general understanding of medicine. The usual process of identifying syndromes or diseases in Traditional Chinese Medicine (TCM) is by confirming the frequently symptom patterns. Semantic Web and ontologies introduce well-structured controlled vocabularies for biomedical science. The direct correspondence between symptoms and syndromes can be formatted to semantic inference rules as a additional knowledge upon a medical ontology.</p></td></tr><tr><td>1013</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_27">Web Schema Construction Based on Web Ontology Usage Analysis</a></td></tr><tr><td colspan=3><p>The ultimate vision of the semantic web is to enable computers to understand and process the information published on the web. This vision is being primarily achieved by web ontologies which semantically annotate the data. In order to effectively access the structured data mainly published in RDF format, one needs to understand not only the prevalent vocabularies being used by the community, but also the extent and the patterns of its usage. In this paper, we achieve this by proposing a framework that analyzes the domain ontology usage and rank terms (classes, properties and attributes) based on multi-criteria characteristics that include population, coverage, and structure. We consider a purpose-built RDF dataset to select the popular terms and construct the schema based on the ranking, enabling the semantic web application to acquire information from the web of data effectively and efficiently.</p></td></tr><tr><td>1014</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_13">Semantic Caching for Semantic Web Applications</a></td></tr><tr><td colspan=3><p>Ontology debugging helps users to understand the unsatisfiability of a concept in an ontology by finding minimal unsatisfiability-preserving sub-ontologies (MUPS) of the ontology for the concept. Although existing approaches have shown good performance for some real life ontologies, they are still inefficient to handle ontologies that have many MUPS for an unsatisfiable concept. In this paper, we propose an efficient approach to debugging ontologies based on a set of patterns. Patterns provide general information to explain unsatisfiability but are not dependent on a specific ontology. In this approach, we make use of a set of heuristic strategies and construct a directed graph w.r.t. the hierarchies where the depth-first search strategy can be used to search paths. The experiments show that our approach has gained a significant improvement over the state of the art and can find considerable number of MUPS.</p></td></tr><tr><td>1015</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_16">Operational Semantics for SPARQL Update</a></td></tr><tr><td colspan=3><p>Concurrent fine grained updates are essential for using RDF stores in dynamic modern Web applications, where users increasingly contribute content as often as they read content. SPARQL Update is a language proposed by the W3C for fine grained updates for RDF stores. In this work we propose an operational semantics for an update language for RDF, which models core features of SPARQL Update. Firstly, an abstract syntax for RDF and updates is presented. Secondly, the operational semantics is defined using relations over the abstract syntax. The operational semantics specifies all possible operational behaviours of updates in the presence of an RDF store. The specification is useful as a common reference for compiler engineers and as a foundation for the static analysis of updates.</p></td></tr><tr><td>1016</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_14">Evaluating Graph Traversal Algorithms for Distributed SPARQL Query Optimization</a></td></tr><tr><td colspan=3><p>Distributed SPARQL queries enable users to retrieve information by exploiting the increasing amount of linked data being published. However, industrial-strength distributed SPARQL query processing is still at its early stage for efficiently answering queries. Previous research shows that it is possible to apply methods from graph theory to optimize the performance of distributed SPARQL. In this paper we describe a framework that can simulate arbitrary RDF data networks to evaluate different approaches of distributed SPARQL query processing. Using this framework we further explore the graph traversal algorithms for distributed SPARQL optimization. We present an implementation of a Minimum-Spanning-Tree-based (MST-based) algorithm for distributed SPARQL processing, the performance of which is compared to other approaches using this evaluation framework. The contribution of this paper is to show that a MST-based approach seems to perform much better than other non graph-traversal-based approaches, and to provide an evaluation framework for evaluating distributed SPARQL processing.</p></td></tr><tr><td>1017</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_12">Enhancing Source Selection for Live Queries over Linked Data via Query Log Mining</a></td></tr><tr><td colspan=3><p>Traditionally, Linked Data query engines execute SPARQL queries over a materialised repository which on the one hand, guarantees fast query answering but on the other hand requires time and resource consuming preprocessing steps. In addition, the materialised repositories have to deal with the ongoing challenge of maintaining the index which is – given the size of the Web – practically unfeasible. Thus, the results for a given SPARQL query are potentially out-dated. Recent approaches address the result freshness problem by answering a given query directly over dereferenced query relevant Web documents. Our work investigate the problem of an efficient selection of query relevant sources under this context. As a part of query optimization, source selection tries to estimate the minimum number of sources accessed in order to answer a query. We propose to summarize and index sources based on frequently appearing query graph patterns mined from query logs. We verify the applicability of our approach and empirically show that our approach significantly reduces the number of relevant sources estimated while keeping the overhead low.</p></td></tr><tr><td>1018</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_23">Proposed SKOS Extensions for BioPortal Terminology Services</a></td></tr><tr><td colspan=3><p>The National Center for Biomedical Ontology (NCBO) BioPortal provides common access for browsing and querying a large set of ontologies that are commonly used in biomedical communities. One of our missions is to align lexical features (i.e., textual definitions) that are commonly used in these ontologies across different representation formats with standard tags and to represent them in a standard way to the users. The Simple Knowledge Organization System (SKOS) is a recommendation of the World-Wide-Web Consortium (W3C) for a common data model for sharing and linking knowledge organization systems on the Semantic Web. The BioPortal is in the process of adopting SKOS in the backend representation for its content. During this process, we discovered that there exists a set of commonly-used lexical features shared by the biomedical ontologies that SKOS does not yet represent. In this paper, we discuss our proposed SKOS extensions to cover this set of commonly used lexical features, the rationales, and the detailed description of each proposed construct.</p></td></tr><tr><td>1019</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_15">BipRank: Ranking and Summarizing RDF Vocabulary Descriptions</a></td></tr><tr><td colspan=3><p>When searching for RDF vocabularies, users often feel hindered by the lengthy description of a retrieved vocabulary from judging its relevance. A natural strategy for dealing with this issue is to generate a summary of the vocabulary description that compactly carries its main theme and reveals its relevance to the user’s information need. In this paper, we present a new solution to this problem of vocabulary summarization, which has been defined as ranking and selecting RDF sentences in our previous work. Firstly, we propose a novel bipartite graph representation of vocabulary description, on which we carry out a stochastic analysis of a random surfer’s behavior, from which we derive a new centrality measure for RDF sentences called BipRank. Further, we improve it by investigating the patterns of RDF sentences and employing their statistical features. Then, we combine BipRank with query relevance and cohesion metrics into an aggregate objective function to be optimized for the selection of RDF sentences. Our experiments on real-world vocabularies demonstrate the superiority of our approach to the baseline, and also validate its scalability in practice.</p></td></tr><tr><td>1020</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_11">GeniUS: Generic User Modeling Library for the Social Semantic Web</a></td></tr><tr><td colspan=3><p>In this paper, we present GeniUS, a generic topic and user modeling library for the Social Semantic Web that enriches the semantics of social data and status messages particularly. Given a stream of messages, it allows for generating topic and user profiles that summarize the stream according to domain- and application-specific needs which can be specified by the requesting party. Therefore, GeniUS can be applied in various application settings. In this paper, we analyze and evaluate GeniUS in six different application domains. Given users’ status messages from Twitter, we investigate the quality of profiles that are generated by different GeniUS user modeling strategies for supporting various recommendation tasks ranging from product recommendations to more specific recommendations as required in book or software product stores. Our evaluation shows that GeniUS succeeds in inferring the semantic meaning of Twitter status messages. We prove that it can successfully adapt to a given domain and application context allowing for tremendous improvements of the recommendation quality when domain-specific semantic filtering is applied to remove noise from the profiles.</p></td></tr><tr><td>1021</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_18">LODDO: Using Linked Open Data Description Overlap to Measure Semantic Relatedness between Named Entities</a></td></tr><tr><td colspan=3><p>Measuring semantic relatedness plays an important role in information retrieval and Natural Language Processing. However, little attention has been paid to measuring semantic relatedness between named entities, which is also very significant. As the existing knowledge based approaches have the entity coverage issue and the statistical based approaches have unreliable result to low frequent entities, we propose a more comprehensive approach by leveraging Linked Open Data (LOD) to solve these problems. LOD consists of lots of data sources from different domains and provides rich a priori knowledge about the entities in the world. By exploiting the semantic associations in LOD, we propose a novel algorithm, called LODDO, to measure the semantic relatedness between named entities. The experimental results show the high performance and robustness of our approach.</p></td></tr><tr><td>1022</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_10">A New Matchmaking Approach Based on Abductive Conjunctive Query Answering</a></td></tr><tr><td colspan=3><p>To perform matchmaking in Web-based scenarios where data are often incomplete, we propose an extended conjunctive query answering (CQA) problem, called </p></td></tr><tr><td>1023</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_7">Dynamic </a></td></tr><tr><td colspan=3><p>In ontological theories, is-a hierarchy must represent the essential property of things and hence should be single-inheritance, since the essential property of things cannot exist in multiple. However, we cannot avoid multiperspective issues when we build an ontology because the user often want to understand things from their own viewpoints. Especially, in the Semantic Web, the variety of users causes the variety of viewpoints to capture target domains. In order to tackle this multi-perspective issue, we adopt an approach of dynamically generating is-a hierarchies according to the viewpoints of users from an ontology using single-inheritance. This article discusses a framework for dynamic is-a hierarchy generation with ontological consideration on is-a hierarchies generated by it. Then, the author shows its implementation as a new function of Hozo and its applications to a medical ontology for dynamically generation of is-a hierarchies of disease. Through the function, users can understand an ontology from a variety of viewpoints. As a result, it could contribute to comprehensive understanding of the ontology and its target world.</p></td></tr><tr><td>1024</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_5">Semantic Flow Networks: Semantic Interoperability in Networks of Ontologies</a></td></tr><tr><td colspan=3><p>In an open context such as the Semantic Web, information providers usually rely on different ontologies to semantically characterize contents. In order to enable interoperability at a semantic level, ontologies underlying information sources must be linked by discovering alignments, that is, set of correspondences or mappings. The aim of this paper is to provide a formal model (i.e., </p></td></tr><tr><td>1025</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_6">Building a Large Scale Knowledge Base from Chinese Wiki Encyclopedia</a></td></tr><tr><td colspan=3><p>DBpedia has been proved to be a successful structured knowledge base, and large scale Semantic Web data has been built by using DBpedia as the central interlinking-hubs of the Web of Data in English. But in Chinese, due to the heavily imbalance in size (no more than one tenth) between English and Chinese in Wikipedia, there are few Chinese linked data are published and linked to DBpedia, which hinders the structured knowledge sharing both within Chinese resources and cross-lingual resources. This paper aims at building large scale Chinese structured knowledge base from Hudong, which is one of the largest Chinese Wiki Encyclopedia websites. In this paper, an upper-level ontology schema in Chinese is first learned based on the category system and Infobox information in Hudong. Totally, there are 19542 concepts are inferred, which are organized in hierarchy with maximally 20 levels. 2381 properties with domain and range information are learned according to the attributes in the Hudong Infoboxes. Then, 802593 instances are extracted and described using the concepts and properties in the learned ontology. These extracted instances cover a wide range of things, including persons, organizations, places and so on. Among all the instances, 62679 of them are linked to identical instances in DBpedia. Moreover, the paper provides RDF dump or SPARQL to access the established Chinese knowledge base. The general upper-level ontology and wide coverage makes the knowledge base a valuable Chinese semantic resource. It not only can be used in Chinese linked data building, the fundamental work for building multi lingual knowledge base across heterogeneous resources of different languages, but also can largely facilitate many useful applications of large-scale knowledge base such as knowledge question-answering and semantic search.</p></td></tr><tr><td>1026</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_4">Constructing Virtual Documents for Ontology Matching Using MapReduce</a></td></tr><tr><td colspan=3><p>Ontology matching is a crucial task for data integration and management on the Semantic Web. The ontology matching techniques today can solve many problems from heterogeneity of ontologies to some extent. However, for matching large ontologies, most ontology matchers take too long run time and have strong requirements on running environment. Based on the MapReduce framework and the virtual document technique, in this paper, we propose a 3-stage MapReduce-based approach called V-Doc+ for matching large ontologies, which significantly reduces the run time while keeping good precision and recall. Firstly, we establish four MapReduce processes to construct virtual document for each entity (class, property or instance), which consist of a simple process for the descriptions of entities, an iterative process for the descriptions of blank nodes and two processes for exchanging the descriptions with neighbors. Then, we use a word-weight-based partition method to calculate similarities between entities in the corresponding reducers. We report our results from two experiments on an OAEI dataset and a dataset from the biology domain. Its performance is assessed by comparing with existing ontology matchers. Additionally, we show how run time is reduced with increasing the size of cluster.</p></td></tr><tr><td>1027</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_8">Mid-Ontology Learning from Linked Data</a></td></tr><tr><td colspan=3><p>The Linking Open Data(LOD) cloud is a collection of linked Resource Description Framework (RDF) data with over 26 billion RDF triples. Consuming linked data is a challenging task because each data set in the LOD cloud has specific ontology schema, and familiarity with ontology schema is required in order to query various linked data sets. However, manually checking each data set is time-consuming, especially when many data sets from various domains are used. This difficulty can be overcome without user interaction by using an automatic method that integrates different ontology schema. In this paper, we propose a Mid-Ontology learning approach that can automatically construct a simple ontology, linking related ontology predicates (class or property) in different data sets. Our Mid-Ontology learning approach consists of three main phases: data collection, predicate grouping, and Mid-Ontology construction. Experimental results show that our Mid-Ontology learning approach successfully integrates diverse ontology schema, and effectively retrieves related information.</p></td></tr><tr><td>1028</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_9">An Ontological Formulation and an OPM Profile for Causality in Planning Applications</a></td></tr><tr><td colspan=3><p>In this paper, we propose an ontological formulation of the planning domain and its OWL 2 formalization. The proposed metamodel conceptualizes planning rules and actions and the </p></td></tr><tr><td>1029</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_1">A Method of Contrastive Reasoning with Inconsistent Ontologies</a></td></tr><tr><td colspan=3><p>Contrastive reasoning is the reasoning with contrasts which are expressed as contrary conjunctions like the word ”but” in natural language. Contrastive answers are more informative for reasoning with inconsistent ontologies, as compared with the usual simple Boolean answer, i.e., either ”yes” or ”no”. In this paper, we propose a method of computing contrastive answers from inconsistent ontologies. The proposed approach has been implemented in the system CRION (Contrastive Reasoning with Inconsistent ONtologies) as a reasoning plug-in in the LarKC (Large Knowledge Collider) platform. We report several experiments in which we apply the CRION system to some realistic ontologies. This evaluation shows that contrastive reasoning is a useful extension to the existing approaches of reasoning with inconsistent ontologies.</p></td></tr><tr><td>1030</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_3">RP-Filter: A Path-Based Triple Filtering Method for Efficient SPARQL Query Processing</a></td></tr><tr><td colspan=3><p>With the rapid increase of RDF data, the SPARQL query processing has received much attention. Currently, most RDF databases store RDF data in a relational table called triple table and carry out several join operations on the triple tables for SPARQL query processing. However, the execution plans with many joins might be inefficient due to a large amount of intermediate data being passed between join operations. In this paper, we propose a triple filtering method called RP-Filter to reduce the amount of intermediate data. RP-Filter exploits the path information in the query graphs and filters the triples which would not be included in final results in advance of joins. We also suggest an efficient relational operator RFLT which filters triples by means of RP-Filter. Experimental results on synthetic and real-life RDF data show that RP-Filter can reduce the intermediate results effectively and accelerate the SPARQL query processing.</p></td></tr><tr><td>1031</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_33">Interlinking Korean Resources on the Web</a></td></tr><tr><td colspan=3><p>LOD (Linked Open Data) is an international endeavor to interlink structured data on the Web and create the Web of Data on a global level. In this paper, we report about our experience of applying existing LOD frameworks, most of which are designed to run only in European language environments, to Korean resources to build linked data. Through the localization of Silk, we identified localized similarity measures as essential for interlinking Korean resources. Specifically, we built new algorithms to measure distance between Korean strings and to measure distance between transliterated Korean strings. A series of empirical tests have found that the new measures substantially improve the performance of Silk with high precision for matching Korean strings and with high recall for matching transliterated Korean strings. We expect the localization issues described in this paper to be applicable to many non-Western countries.</p></td></tr><tr><td>1032</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_31">Korean Linked Data on the Web: Text to RDF</a></td></tr><tr><td colspan=3><p>Interlinking data coming from different sources has been a long standing goal [4] aiming to increase reusability, discoverability, and as a result the usefulness of information. Nowadays, Linked Open Data (LOD) tackles this issue in the context of semantic web. However, currently most of the web data is stored in relational databases and published as unstructured text. This triggers the need of (i) combining the current semantic technologies with relational databases; (ii) processing text integrating several NLP tools, and being able to query the outcome using the standard semantic web query language: SPARQL; and (iii) linking the outcome with the LOD cloud. The work presented here shows a solution for the needs listed above in the context of Korean language, but our approach can be adapted to other languages as well.</p></td></tr><tr><td>1033</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_30">Linking Open Data Resources for Semantic Enhancement of User–Generated Content</a></td></tr><tr><td colspan=3><p>This paper describes our experiences in developing a Linking Open Data (LOD) resource for Taiwanese Geographic Names (LOD TGN), extracting Taiwanese place names found in Facebook posts, and linking such place names to the entries in LOD TGN. The aim of this study is to enhance the semantics of User-Generated Content (UGC) through the use of LOD resources, so that for example the content of Facebook posts can be more reusable and discoverable. 9E24QThis study actually is a development of a geospatial semantic annotation method for Facebook posts through the use of LOD resources.</p></td></tr><tr><td>1034</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_29">Towards a Data Hub for Biodiversity with LOD</a></td></tr><tr><td colspan=3><p>Because of a huge variety of biological studies focused on different targets, i.e., from molecules to ecosystem, data produced and used in each field is also managed independently so that it is difficult to know the relationship among them. We aim to build a data hub with LOD to connect data in different biological fields to enhance search and use of data across the fields. We build a prototype data hub on taxonomic information on species, which is a key to retrieve data and link to databases in different fields. We also demonstrate how the data hub can be used with an application to assist search on other database.</p></td></tr><tr><td>1035</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_32">Issues for Linking Geographical Open Data of GeoNames and Wikipedia</a></td></tr><tr><td colspan=3><p>It is now possible to use various geographical open data sources such as GeoNames and Wikipedia to construct geographic information systems. In addition, these open data sources are integrated by the concept of Linked Open Data. There have been several attempts to identify links between existing data, but few studies have focused on the quality of such links. In this paper, we introduce an automatic link discovery method for identifying the correspondences between GeoNames entries and Wikipedia pages, based on Wikipedia category information. This method finds not only appropriate links but also inconsistencies between two databases. Based on this integration results, we discuss the type of inconsistencies for making consistent Linked Open Data.</p></td></tr><tr><td>1036</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_26">DashSearch LD: Exploratory Search for Linked Data</a></td></tr><tr><td colspan=3><p>Although a large number of datasets gathered as Linked Open Data (LOD) is better for data sharing and re-using, the datasets themselves become more difficult to understand. Since each dataset has its own data structure, we need to understand datasets individually. In addition, since the entities in datasets are interconnected, we need to understand the interconnections between datasets. In other words, understanding the data is crucial for exploiting LOD. In this paper, we show a novel system called DashSearch LD to understand and use LOD with an exploratory search approach. The user interactively explores datasets by viewing and selecting entities in the datasets. Specifically, the user manipulates widgets on the screen by moving and overlapping them with a mouse to check entities, draw detail data on them, and obtain other entities linked by the widgets.</p></td></tr><tr><td>1037</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_28">Development of Linked Open Data for Bioresources</a></td></tr><tr><td colspan=3><p>The broad dissemination of information is a key issue in improving access to existing bioresources. We attempted to develop Linked Open Data (LOD) for bioresources available at the RIKEN BioResource Center. The LOD consists of standardized, structured data available openly on the World Wide Web, including published bioresource information for 5,000 mouse strains and 3,600 cell lines. The LOD includes links to publically available information, such as genes, alleles, and ontologies, providing phenotypic information through the BioLOD website. As a result, information on mouse strains and cell lines have been connected to various data items in public databases and other project-oriented databases. Thus, through the use of LOD, dispersed efforts to produce different databases can be easily combined. Through these efforts, we expect to contribute to the global improvement of access to bioresources.</p></td></tr><tr><td>1038</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_25">Improving the Performance of the DL-Learner SPARQL Component for Semantic Web Applications</a></td></tr><tr><td colspan=3><p>The vision of the Semantic Web is to make use of semantic representations on the largest possible scale - the Web. Large knowledge bases such as DBpedia, OpenCyc, GovTrack are emerging and freely available as Linked Data and SPARQL endpoints. Exploring and analysing such knowledge bases is a significant hurdle for Semantic Web research and practice. As one possible direction for tackling this problem, we present an approach for obtaining complex class expressions from objects in knowledge bases by using Machine Learning techniques. We describe in detail how they leverage existing techniques to achieve scalability on large knowledge bases available as SPARQL endpoints or Linked Data. The algorithms are made available in the open source DL-Learner project and we present several real-life scenarios in which they can be used by Semantic Web applications. Because of the wide usage of the method in several well-known tools, we optimized and benchmarked the existing algorithms and show that we achieve an approximately 3-fold increase in speed, in addition to a more robust implementation.</p></td></tr><tr><td>1039</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_27">Entity-Based Semantic Search on Conversational Transcripts Semantic</a></td></tr><tr><td colspan=3><p>This paper describes the implementation of a semantic web search engine on conversation styled transcripts. Our choice of data is Hansard, a publicly available conversation style transcript of parliamentary debates. The current search engine implementation on Hansard is limited to running search queries based on keywords or phrases hence lacks the ability to make semantic inferences from user queries. By making use of knowledge such as the relationship between members of parliament, constituencies, terms of office, as well as topics of debates the search results can be improved in terms of both relevance and coverage. Our contribution is not algorithmic instead we describe how we exploit a collection of external data sources, ontologies, semantic web vocabularies and named entity extraction in the analysis of underlying semantics of user queries as well as the semantic enrichment of the search index thereby improving the quality of results.</p></td></tr><tr><td>1040</td><td>2011</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-29923-0_2">Parallel ABox Reasoning of </a></td></tr><tr><td colspan=3><p>In order to support the vision of the Semantic Web, ontology reasoning needs to be highly scalable and efficient. A natural way to achieve scalability and efficiency is to develop parallel ABox reasoning algorithms for tractable OWL 2 profiles to distribute the load between different computation units within a reasoning system. So far there have been some work on parallel ABox reasoning algorithms for the pD* fragment of OWL 2 RL. However, there is still no work on parallel ABox reasoning algorithm for OWL 2 EL, which is the language for many influential ontologies (such as the SNOMED CT ontology). In this paper, we extend a parallel TBox reasoning algorithm [5] for </p></td></tr><tr><td>1041</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_24">Location-Based Concept in Activity Log Ontology for Activity Recognition in Smart Home Domain</a></td></tr><tr><td colspan=3><p>Activity recognition plays an important role in several researches. Nevertheless, the existing researches suffer various kinds of problems when human has a different lifestyle. To address these shortcomings, this paper proposes the activity log in the context-aware infrastructure ontology in order to interlink the history user’s context and current user’s context. In this approach, the location-based concept is built into the activity log for producing the description logic (DL) rules. The relationship between activities in the same location is investigated for making the result of activity recognition more accurately. We also conduct the semantic ontology search (SOS) system for evaluating the effectiveness of our proposed ideas. The semantic data can be retrieved through SOS system, including, human activity and activity of daily living (ADL). The results from SOS system showed the advantage overcome the existing system when uses the location-based concept in activity log ontology.</p></td></tr><tr><td>1042</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_23">Applying Semantic Technologies to Public Sector: A Case Study in Fraud Detection</a></td></tr><tr><td colspan=3><p>Fraudulent claims cost both the public and private sectors an enormous amount of money each year. The existence of data silos is considered one of the main barriers to cross-region, cross-department, and cross-domain data analysis that can detect abnormalities not easily seen when focusing on single data sources. An evident advantage of leveraging Linked Data and semantic technologies is the smooth integration of distributed data sets. This paper reports a proof-of-concept study in the benefit fraud detection area. We believe that the design considerations, study outcomes, and learnt lessons can help making decisions of how one should adopt semantic technologies in similar contexts.</p></td></tr><tr><td>1043</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_21">A Community-Driven Approach to Development of an Ontology-Based Application Management Framework</a></td></tr><tr><td colspan=3><p>Although the semantic web standards are established, applications and uses of the data are relatively limited. This is partly due to high learning curve and efforts demanded in building semantic web and ontology-based applications. In this paper, we describe an ontology application management framework that aims to simplify creation and adoption of a semantic web application. The framework supports application development in ontology- database mapping, recommendation rule management and application templates focusing on semantic search and recommender system applications. We present some case studies that adopted our application framework in their projects. Evolution of the software tool significantly profited from the semantic web research community in Thailand who has contributed both in terms of the tool development and adoption support.</p></td></tr><tr><td>1044</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_20">Topica – Profiling Locations through Social Streams</a></td></tr><tr><td colspan=3><p>This paper presents work in interlinking social stream information with geographical spaces through the use of Linked Data technologies. The paper focuses on filtering, enriching, structuring and interlinking microposts of localised (i.e. geo-tagged) social streams (a.k.a localised forums) to profile geographical areas (e.g., cities, countries). For this purpose, we enriched social streams extracted from Twitter, Facebook and TripAdvisor and structured them into well-known vocabularies and data models, such as SIOC and SKOS. To integrate this information into a location profile we introduce the </p></td></tr><tr><td>1045</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_19">A German Natural Language Interface for Semantic Search</a></td></tr><tr><td colspan=3><p>Semantic data is the key for an efficient information retrieval. It relies on a well-defined structure and enables automated processing. Therefore, more and more ontologies are specified, extended and interlinked. By now, only the query language SPARQL provides a precise access to semantic data. Since most common users are overstrained in formulating queries, which satisfy the structure of semantic data, more search-interface approaches emerge aiming at good usability and correct answers. We implemented a Natural Language Interface (NLI), that answers questions formulated in German natural language. In order to query the domain ontology, the user query is translated into SPARQL first. Since domain-ontology resources are required for the SPARQL-query formulation, this paper introduces an approach for the identification of resources in user query. We show a path-based identification of semantically similar resources and a similarity measure. After running 100 test questions, our system achieves a precision and recall of 66%.</p></td></tr><tr><td>1046</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_22">When Mommy Blogs Are Semantically Tagged</a></td></tr><tr><td colspan=3><p>OWL 2-supported Semantic Tagging is a non compulsory yet decisive and highly influential component of a multidisciplinary knowledge architecture framework which synergetically combines the Semantic and the Social Webs. The facility consists of a semantic tagging layer based on OWL 2 axioms and expressions enticing social network users, typically mommy bloggers, to annotate their chaos of textual data with natural language verbalized versions of ontological elements. This paper provides a comprehensive short summary of the overall framework along with its backbone metamodel and its parenting analysis and surveillance ontology ParOnt, laying a particular emphasis on its semantic expression-based tagging feature, and accordingly highlighting the attained gains and improvements in terms of effective results, services and recommendations, all falling in the scope of public parenting orientation and awareness.</p></td></tr><tr><td>1047</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_17">Development of the Method for the Appropriate Selection of the Successor by Applying Metadata to the Standardization Reports and Members</a></td></tr><tr><td colspan=3><p>In businesses and organizations, it is difficult to find the successor for various activities by considering a person’s knowledge and actual experience. In this study, we find the successor to a member of a standardization activity. By assigning metadata to profiles and annual activity reports of members engaged in standardization activities, the relationship between the profiles and the annual activity reports is described as an RDF graph and visualized with nodes and links. This paper has two objectives. Objective-1 is the development and evaluation of a method to design the best combination of search queries to discover an appropriate successor. Objective-2 is the proposal and evaluation of an easy and understandable visualization method of the successor search results obtained in objective-1. The proposed procedure nominates candidates for the successor effectively and the results are visualized in the case study.</p></td></tr><tr><td>1048</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_16">An Ontological Framework for Decision Support</a></td></tr><tr><td colspan=3><p>In the last few years, ontologies have been successfully exploited by Decision Support Systems (</p></td></tr><tr><td>1049</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_18">A Native Approach to Semantics and Inference in Fine-Grained Documents</a></td></tr><tr><td colspan=3><p>This paper proposes a novel approach for enhancing document excerpts with semantic structures that are treated as first-class citizens, i.e., integrated at the system level. Providing support for semantics at the system level is in contrast with existing solutions that implement semantics as an add-on using intermediate descriptors. A framework and a toolset inspired by the Semantic Web Stack have been integrated into the </p></td></tr><tr><td>1050</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_14">Navigation-Induced Knowledge Engineering by Example</a></td></tr><tr><td colspan=3><p>Knowledge Engineering is a costly, tedious and often time-consuming task, for which light-weight processes are desperately needed. In this paper, we present a new paradigm - Navigation-induced Knowledge Engineering by Example (NKE) - to address this problem by producing structured knowledge as a result of users navigating through an information system. Thereby, NKE aims to reduce the costs associated with knowledge engineering by framing it as navigation. We introduce and define the NKE paradigm and demonstrate it with a proof-of-concept prototype which creates OWL class expressions based on users navigating in a collection of resources. The overall contribution of this paper is twofold: (i) it introduces a novel paradigm for knowledge engineering and (ii) it provides evidence for its technical feasibility.</p></td></tr><tr><td>1051</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_15">FacetOntology: Expressive Descriptions of Facets in the Semantic Web</a></td></tr><tr><td colspan=3><p>The formal structure of the information on the Semantic Web lends itself to faceted browsing, an information retrieval method where users can filter results based on the values of properties (“facets”). Numerous faceted browsers have been created to browse RDF and Linked Data, but these systems use their own ontologies for defining how data is queried to populate their facets. Since the source data is the same format across these systems (specifically, RDF), we can unify the different methods of describing how to query the underlying data, to enable compatibility across systems, and provide an extensible base ontology for future systems. To this end, we present FacetOntology, an ontology that defines how to query data to form a faceted browser, and a number of transformations and filters that can be applied to data before it is shown to users. FacetOntology overcomes limitations in the expressivity of existing work, by enabling the full expressivity of SPARQL when selecting data for facets. By applying a </p></td></tr><tr><td>1052</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_13">Leveraging the Crowdsourcing of Lexical Resources for Bootstrapping a Linguistic Data Cloud</a></td></tr><tr><td colspan=3><p>We present a declarative approach implemented in a comprehensive open-source framework based on </p></td></tr><tr><td>1053</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_11">Keyword-Driven Resource Disambiguation over RDF Knowledge Bases</a></td></tr><tr><td colspan=3><p>Keyword search is the most popular way to access information. In this paper we introduce a novel approach for determining the correct resources for user-supplied queries based on a hidden Markov model. In our approach the user-supplied query is modeled as the observed data and the background knowledge is used for parameter estimation. We leverage the semantic relationships between resources for computing the parameter estimations. In this approach, query segmentation and resource disambiguation are mutually tightly interwoven. First, an initial set of potential segments is obtained leveraging the underlying knowledge base; then, the final correct set of segments is determined after the most likely resource mapping was computed. While linguistic analysis (e.g. named entity, multi-word unit recognition and POS-tagging) fail in the case of keyword-based queries, we will show that our statistical approach is robust with regard to query expression variance. Our experimental results reveal very promising results.</p></td></tr><tr><td>1054</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_12">An Automated Template Selection Framework for Keyword Query over Linked Data</a></td></tr><tr><td colspan=3><p>Template-based information access, in which templates are constructed for keywords, is a recent development of linked data information retrieval. However, most such approaches suffer from ineffective template management. Because linked data has a structured data representation, we assume the data’s inside statistics can effectively influence template management. In this work, we use this influence for template creation, template ranking, and scaling. Our proposal can effectively be used for automatic linked data information retrieval and can be incorporated with other techniques such as ontology inclusion and sophisticated matching to further improve performance.</p></td></tr><tr><td>1055</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_10">The Dynamic Generation of Refining Categories in Ontology-Based Search</a></td></tr><tr><td colspan=3><p>In the era of information revolution, the amount of digital contents is growing explosively with the advent of personal smart devices. The consumption of the digital contents makes users depend heavily on search engines to search what they want. Search requires tedious review of search results from users currently, and so alleviates it; predefined and fixed categories are provided to refine results. Since fixed categories never reflect the difference of queries and search results, they often contain insensible information. This paper proposes a method for the dynamic generation of refining categories under the ontology-based semantic search systems. It specifically suggests a measure for dynamic selection of categories and an algorithm to arrange them in an appropriate order. Finally, it proves the validity of the proposed approach by using some evaluative measures.</p></td></tr><tr><td>1056</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_5">Accessing Relational Data on the Web with SparqlMap</a></td></tr><tr><td colspan=3><p>The vast majority of the structured data of our age is stored in relational databases. In order to link and integrate this data on the Web, it is of paramount importance to make relational data available according to the RDF data model and associated serializations. In this article we present </p></td></tr><tr><td>1057</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_8">Interlinking Linked Data Sources Using a Domain-Independent System</a></td></tr><tr><td colspan=3><p>Linked data interlinking is the discovery of every </p></td></tr><tr><td>1058</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_9">Instance Coreference Resolution in Multi-ontology Linked Data Resources</a></td></tr><tr><td colspan=3><p>Web of linked data is one of the main principles for realization of semantic web ideals. In recent years, different data providers have produced many data sources in the Linking Open Data (LOD) cloud upon different schemas. Isolated published linked data sources are not themselves so beneficial for intelligent applications and agents in the context of semantic web. It is not possible to take advantage of the linked data potential capacity without integrating various data sources. The challenge of integration is not limited to instances; rather, schema heterogeneity affects discovering instances with the same identity. In this paper we propose a novel approach, SBUEI, for instance co-reference resolution between various linked data sources even with heterogeneous schemas. For this purpose, SBUEI considers the entity co-reference resolution problem in both schema and instance levels. The process of matching is applied in both levels consecutively to let the system discover identical instances. SBUEI also applies a new approach for consolidation of linked data in instance level. After finding identical instances, SBUEI searches locally around them in order to find more instances that are equal. Experiments show that SBUEI obtains promising results with high precision and recall.</p></td></tr><tr><td>1059</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_7">Active Learning of Domain-Specific Distances for Link Discovery</a></td></tr><tr><td colspan=3><p>Discovering cross-knowledge-base links is of central importance for manifold tasks across the Linked Data Web. So far, learning link specifications has been addressed by approaches that rely on standard similarity and distance measures such as the Levenshtein distance for strings and the Euclidean distance for numeric values. While these approaches have been shown to perform well, the use of standard similarity measure still hampers their accuracy, as several link discovery tasks can only be solved sub-optimally when relying on standard measures. In this paper, we address this drawback by presenting a novel approach to learning string similarity measures concurrently across multiple dimensions directly from labeled data. Our approach is based on learning linear classifiers which rely on learned edit distance within an active learning setting. By using this combination of paradigms, we can ensure that we reduce the labeling burden on the experts at hand while achieving superior results on datasets for which edit distances are useful. We evaluate our approach on three different real datasets and show that our approach can improve the accuracy of classifiers. We also discuss how our approach can be extended to other similarity and distance measures as well as different classifiers.</p></td></tr><tr><td>1060</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_6">Protect Your RDF Data!</a></td></tr><tr><td colspan=3><p>The explosion of digital content and the heterogeneity of enterprise content sources have pushed existing data integration solutions to their boundaries. Although RDF can be used as a representation format for integrated data, enterprises have been slow to adopt this technology. One of the primary inhibitors to its widespread adoption in industry is the lack of fine grained access control enforcement mechanisms available for RDF. In this paper, we provide a summary of access control requirements based on our analysis of existing access control models and enforcement mechanisms. We subsequently: (i) propose a set of access control rules that can be used to provide support for these models over RDF data; (ii) detail a framework that enforces access control restrictions over RDF data; and (iii) evaluate our implementation of the framework over real-world enterprise data.</p></td></tr><tr><td>1061</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_3">Ontological Modeling of Interoperable Abnormal States</a></td></tr><tr><td colspan=3><p>Exchanging huge volumes of data is a common concern in various fields. One issue has been the difficulty of cross-domain sharing of knowledge because of its highly heterogeneous nature. We constructed an ontological model of abnormal states from the generic to domain-specific level. We propose a unified form to describe an abnormal state as a "property", and then divide it into an "attribute" and a "value" in a qualitative form. This approach promotes interoperability and flexibility of quantitative raw data, qualitative information, and generic/abstract knowledge. By developing an </p></td></tr><tr><td>1062</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_2">Get My Pizza Right: Repairing Missing is-a Relations in </a></td></tr><tr><td colspan=3><p>With the increased use of ontologies in semantically-enabled applications, the issue of debugging defects in ontologies has become increasingly important. These defects can lead to wrong or incomplete results for the applications. Debugging consists of the phases of detection and repairing. In this paper we focus on the repairing phase of a particular kind of defects, i.e. the missing relations in the is-a hierarchy. Previous work has dealt with the case of taxonomies. In this work we extend the scope to deal with </p></td></tr><tr><td>1063</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_35">Semantic Telecommunications Network Capability Services</a></td></tr><tr><td colspan=3><p>The providing of user-centric services in the B3G/4G network presents a great challenge in the service architecture. To narrow the semantic gap of Telecommunications Network and Internet in the service layer, we introduce the vision of User Centric Intelligent Service Environment (UCISE). The ubiquitous service ecosystem and the semantic service integration architecture of telecommunications network and Internet are proposed. To provide the semantic Telecommunications Network Capability Services (TNCS), we present a semantic description approach for TNCS by using the profile hierarchy of OWL-S. Then based on this approach, we demonstrate the cheapest click-to-call application case. In this way, the ontology-based accurate discovery, matching for telecommunication network capability services can be supported. This will facilitate the semantic convergence of telecommunications network and Internet in the service layer.</p></td></tr><tr><td>1064</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_34">Snippet Generation for Semantic Web Search Engines</a></td></tr><tr><td colspan=3><p>With the development of the Semantic Web, more and more ontologies are available for exploitation by semantic search engines. However, while semantic search engines support the retrieval of candidate ontologies, the final selection of the most appropriate ontology is still difficult for the end users. In this paper, we extend existing work on ontology summarization to support the presentation of ontology snippets. The proposed solution leverages a new semantic similarity measure to generate snippets that are based on the given query. Experimental results have shown the potential of our solution in this problem domain that is largely unexplored so far.</p></td></tr><tr><td>1065</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_33">A Robust Ontology-Based Method for Translating Natural Language Queries to Conceptual Graphs</a></td></tr><tr><td colspan=3><p>A natural language interface is always desirable for a search system. While performance of machine translation for general texts with acceptable computational costs seems to reach a limit, narrowing down the domain to one of queries reduces the complexity and enables better translation correctness. This paper proposes a query translation method that is robust to ill-formed questions and exploits knowledge of an ontology for semantic search. It uses conceptual graphs as the target language for the translation. As a logical interlingua with smooth mapping to and from natural language, conceptual graphs simplify translation rules and can be easily converted to other formal query languages. Experiment results of the method on the TREC 2002 and TREC 2007 data sets are also presented and discussed.</p></td></tr><tr><td>1066</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_4">SkyPackage: From Finding Items to Finding a Skyline of Packages on the Semantic Web</a></td></tr><tr><td colspan=3><p>Enabling complex querying paradigms over the wealth of available Semantic Web data will significantly impact the relevance and adoption of Semantic Web technologies in a broad range of domains. While the current predominant paradigm is to retrieve a list of items, in many cases the actual intent is satisfied by reviewing the lists and assembling compatible items into lists or packages of resources such that each package collectively satisfies the need, such as assembling different collections of places to visit during a vacation. Users may place constraints on individual items, and the compatibility of items within a package is based on global constraints placed on packages, like total distance or time to travel between locations in a package. Finding such packages using the traditional item-querying model requires users to review lists of possible multiple queries and assemble and compare packages manually.</p></td></tr><tr><td>1067</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_32">A Segmentation-Based Approach for Approximate Query over Distributed Ontologies</a></td></tr><tr><td colspan=3><p>With the popularity of semantic information systems distributed on the Web, there is an arising challenge to provide efficient query answering support for these systems. However, common approaches for distributed query answering either exhibit performance disadvantages or loss of completeness in an unbalanced way. In this paper, we introduce a novel approach for </p></td></tr><tr><td>1068</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_30">Exposing Heterogeneous Data Sources as SPARQL Endpoints through an Object-Oriented Abstraction</a></td></tr><tr><td colspan=3><p>The Web of Data vision raises the problem of how to expose existing data sources on the Web without requiring heavy manual work. In this paper, we present our approach to facilitate SPARQL queries over heterogeneous data sources.</p></td></tr><tr><td>1069</td><td>2012</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-37996-3_1">A Resolution Procedure for Description Logics with Nominal Schemas</a></td></tr><tr><td colspan=3><p>We present a polynomial resolution-based decision procedure for the recently introduced description logic </p></td></tr><tr><td>1070</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_27">Extracting Semantic Frames from Thai Medical-Symptom Phrases with Unknown Boundaries</a></td></tr><tr><td colspan=3><p>Due to the limitations of language-processing tools for the Thai language, pattern-based information extraction from Thai documents requires supplementary techniques. Based on sliding-window rule application and extraction filtering, we present a framework for extracting semantic information from medical-symptom phrases with unknown boundaries in Thai free-text information entries. A supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand-tagged training symptom phrases. Two filtering components are introduced: one uses a classification model for predicting rule application across a symptom-phrase boundary, the other uses extraction distances observed during rule learning for resolving conflicts arising from overlapping-frame extractions. In our experimental study, we focus our attention on two basic types of symptom phrasal descriptions: one is concerned with abnormal characteristics of some observable entities and the other with human-body locations at which symptoms appear. The experimental results show that the filtering components improve precision while preserving recall satisfactorily.</p></td></tr><tr><td>1071</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_29">Named Entity Disambiguation: A Hybrid Statistical and Rule-Based Incremental Approach</a></td></tr><tr><td colspan=3><p>The rapidly increasing use of large-scale data on the Web makes named entity disambiguation become one of the main challenges to research in Information Extraction and development of Semantic Web. This paper presents a novel method for detecting proper names in a text and linking them to the right entities in Wikipedia. The method is hybrid, containing two phases of which the first one utilizes some heuristics and patterns to narrow down the candidates, and the second one employs the vector space model to rank the ambiguous cases to choose the right candidate. The novelty is that the disambiguation process is incremental and includes several rounds that filter the candidates, by exploiting previously identified entities and extending the text by those entity attributes every time they are successfully resolved in a round. We test the performance of the proposed method in disambiguation of names of people, locations and organizations in texts of the news domain. The experiment results show that our approach achieves high accuracy and can be used to construct a robust named entity disambiguation system.</p></td></tr><tr><td>1072</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_25">Semantic Assistants – User-Centric Natural Language Processing Services for Desktop Clients</a></td></tr><tr><td colspan=3><p>Today’s knowledge workers have to spend a large amount of time and manual effort on creating, analyzing, and modifying textual content. While more advanced semantically-oriented analysis techniques have been developed in recent years, they have not yet found their way into commonly used desktop clients, be they generic (e.g., word processors, email clients) or domain-specific (e.g., software IDEs, biological tools). Instead of forcing the user to leave his current context and use an external application, we propose a “Semantic Assistants” approach, where semantic analysis services relevant for the user’s current task are offered directly within a desktop application. Our approach relies on an OWL ontology model for context and service information and integrates external natural language processing (NLP) pipelines through W3C Web services.</p></td></tr><tr><td>1073</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_24">Semantically Conceptualizing and Annotating Tables</a></td></tr><tr><td colspan=3><p>Enabling a system to automatically conceptualize and annotate a human-readable table is one way to create interesting semantic-web content. But exactly “how?” is not clear. With conceptualization and annotation in mind, we investigate a semantic-enrichment procedure as a way to turn syntactically observed table layout into semantically coherent ontological concepts, relationships, and constraints. Our semantic-enrichment procedure shows how to make use of auxiliary world knowledge to construct rich ontological structures and to populate these ontological structures with instance data. The system uses auxiliary knowledge (1) to recognize concepts and which data values belong to which concepts, (2) to discover relationships among concepts and which data-value combinations represent relationship instances, and (3) to discover constraints over the concepts and relationships that the data values and data-value combinations should satisfy. Experimental evaluations indicate that the automatic conceptualization and annotation processes perform well, yielding F-measures of 90% for concept recognition, 77% for relationship discovery, and 90% for constraint discovery in web tables selected from the geopolitical domain.</p></td></tr><tr><td>1074</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_31">Integrating Lightweight Reasoning into Class-Based Query Refinement for Object Search</a></td></tr><tr><td colspan=3><p>More and more RDF data have been published online to be consumed. Ordinary Web users also expect to experience more intelligent services promised by the Semantic Web, such as object search based on structured data. We implemented the Falcons search engine to meet the challenge. To enable keyword search, for each object, we construct and index a virtual document that includes textual descriptions of its neighboring resources. Typing information is used to serve class-based query refinement, and class-inclusion reasoning is performed to discover implicit types of objects. A method of recommending subclasses is implemented to enable navigating class hierarchies for incremental query refinement. We also report on lessons learned from Web-scale experiments.</p></td></tr><tr><td>1075</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_23">Catriple: Extracting Triples from Wikipedia Categories</a></td></tr><tr><td colspan=3><p>As an important step towards bootstrapping the Semantic Web, many efforts have been made to extract triples from Wikipedia because of its wide coverage, good organization and rich knowledge. One kind of important triples is about Wikipedia articles and their non-isa properties, e.g. (Beijing, country, China). Previous work has tried to extract such triples from Wikipedia infoboxes, article text and categories. The infobox-based and text-based extraction methods depend on the infoboxes and suffer from a low article coverage. In contrast, the category-based extraction methods exploit the widespread categories. However, they rely on predefined properties, which is too effort-consuming and explores only very limited knowledge in the categories. This paper automatically extracts properties and triples from the less explored Wikipedia categories so as to achieve a wider article coverage with less manual effort. We manage to realize this goal by utilizing the syntax and semantics brought by super-sub category pairs in Wikipedia. Our prototype implementation outputs about 10M triples with a 12-level confidence ranging from 47.0% to 96.4%, which cover 78.2% of Wikipedia articles. Among them, 1.27M triples have confidence of 96.4%. Applications can on demand use the triples with suitable confidence.</p></td></tr><tr><td>1076</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_20">Consolidating User-Defined Concepts with StYLiD</a></td></tr><tr><td colspan=3><p>Information sharing can be effective with structured data. However, there are several challenges for having structured data on the web. Creating structured concept definitions is difficult and multiple conceptualizations may exist due to different user requirements and preferences. We propose consolidating multiple concept definitions into a unified virtual concept and formalize our approach. We have implemented a system called StYLiD to realize this. StYLiD is a social software for sharing a wide variety of structured data. Users can freely define their own structured concepts. The system consolidates multiple definitions for the same concept by different users. Attributes of the multiple concept versions are aligned semi-automatically to provide a unified view. It provides a flexible interface for easy concept definition and data contribution. Popular concepts gradually emerge from the cloud of concepts while concepts evolve incrementally. StYLiD supports linked data by interlinking data instances including external resources like Wikipedia.</p></td></tr><tr><td>1077</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_19">STAN: Social, Trusted Annotation Network</a></td></tr><tr><td colspan=3><p>Annotated data play an important role in enhancing the usability of information resources. Single users can be easily frustrated by the task of annotating. Collaborative approaches to annotation have been applied to web resources, but have not yet been applied to the task of local documents, due in part to the lack of a uniform identification method. In this paper, we use hash-based virtual URIs for identifying documents, and introduce the concept of a STAN (Social, Trusted Annotation Network), which enables collaborative annotation of documents through their URIs. STAN also incorporates quantitative trust rates between users in social networks based on their interactions with each other. The STAN framework is described, demonstrating how these trust networks are constructed through collaborative annotation. Finally, we evaluate the usefulness of collaborative annotation and the feasibility of the resulting trust rates through empirical experiment.</p></td></tr><tr><td>1078</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_26">Exploiting Gene Ontology to Conceptualize Biomedical Document Collections</a></td></tr><tr><td colspan=3><p>As biomedical science progresses, ontologies play an increasingly important role in easing the understanding of biomedical information. Although much research, such as Gene Ontology annotation, has been proposed to utilize ontologies to help users understand biomedical information easily, most of the research does not focus on capturing gene-related terms and their relationships within biomedical document collections. Understanding key gene-related terms as well as their semantic relationships is essential for comprehending the conceptual structure of biomedical document collections and avoiding information overload for users. To address this issue, we propose a novel approach called ‘GOClonto’ to automatically generate ontologies for conceptualization of biomedical document collections. Based on GO (Gene Ontology), GOClonto extracts gene-related terms from biomedical text, applies latent semantic analysis to identify key gene-related terms, allocates documents based on the key gene-related terms, and utilizes GO to automatically generate a corpus-related gene ontology. The experimental results show that GOClonto is able to identify key gene-related terms. For a test biomedical document collection, GOClonto shows better performance than other clustering algorithms in terms of F-measure. Moreover, the ontology generated by GOClonto shows a significant informative conceptual structure.</p></td></tr><tr><td>1079</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_17">Identifying Key Concepts in an Ontology, through the Integration of Cognitive Principles with Statistical and Topological Measures</a></td></tr><tr><td colspan=3><p>In this paper we address the issue of identifying the concepts in an ontology, which best summarize what the ontology is about. Our approach combines a number of criteria, drawn from cognitive science, network topology, and lexical statistics. In the paper we show two versions of our algorithm, which have been evaluated against the results produced by human experts. We report that the latest version of the algorithm performs very well, exhibiting an excellent degree of correlation with the choices of the experts. While the generation of automatic methods for ontology summarization is an interesting research issue in itself, the work described here also provides a basis for novel approaches to a variety of ontology engineering tasks, including ontology matching, automatic classification, ontology modularization, and ontology evaluation.</p></td></tr><tr><td>1080</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_22">Predicting Category Additions in a Topic Hierarchy</a></td></tr><tr><td colspan=3><p>This paper discusses the problem of predicting the structural changes in an ontology. It addresses ontologies that contain instances in addition to concepts. The focus is on an ontology where the instances are textual documents, but the approach presented in this document is general enough to also work with other kinds of instances, as long as a similarity measure can be defined over them. We examine the changes in the Open Directory Project ontology of Web pages over a period of several years and analyze the most common types of structural changes that took place during that time. We then present an approach for predicting one of the more common types of structural changes, namely the addition of a new concept that becomes the subconcept of an existing parent concept and adopts a few instances of this existing parent concept. We describe how this task can be formulated as a machine-learning problem and present an experimental evaluation of this approach that shows promising results of the proposed approach.</p></td></tr><tr><td>1081</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_18">The Art of Tagging: Measuring the Quality of Tags</a></td></tr><tr><td colspan=3><p>Collaborative tagging, supported by many social networking websites, is currently enjoying an increasing popularity. The usefulness of this largely available tag data has been explored in many applications including web resources categorization,deriving emergent semantics, web search etc. However, since tags are supplied by users </p></td></tr><tr><td>1082</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_28">Refining Instance Coreferencing Results Using Belief Propagation</a></td></tr><tr><td colspan=3><p>The problem of coreference resolution (finding individuals, which describe the same entity but have different URIs) is crucial when dealing with semantic data coming from different sources. Specific features of Semantic Web data (ontological constraints, data sparseness, varying quality of sources) are all significant for coreference resolution and must be exploited. In this paper we present a framework, which uses Dempster-Shafer belief propagation to capture these features and refine coreference resolution results produced by simpler string similarity techniques.</p></td></tr><tr><td>1083</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_16">An Editorial Workflow Approach For Collaborative Ontology Development</a></td></tr><tr><td colspan=3><p>The widespread use of ontologies in the last years has raised new challenges for their development and maintenance. Ontology development has transformed from a process normally performed by one ontology engineer into a process performed collaboratively by a team of ontology engineers, who may be geographically distributed and play different roles. For example, editors may propose changes, while authoritative users approve or reject them following a well defined process. This process, however, has only been partially addressed by existing ontology development methods, methodologies, and tool support. Furthermore, in a distributed environment where ontology editors may be working on local copies of the same ontology, strategies should be in place to ensure that changes in one copy are reflected in all of them. In this paper, we propose a workflow-based model for the collaborative development of ontologies in distributed environments and describe the components required to support them. We illustrate our model with a test case in the fishery domain from the United Nations Food and Agriculture Organisation (FAO).</p></td></tr><tr><td>1084</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_21">An Integrated Approach for Automatic Construction of Bilingual Chinese-English WordNet</a></td></tr><tr><td colspan=3><p>This paper compares various approaches for constructing Chinese-English bilingual WordNet. First, we implement three independent approaches that translate English WordNet to Chinese WordNet automatically, including Minimum Distance (MDA), Intersection (IA) and Words Co-occurrence (WCA). Minimum Distance compares the gloss of synset with the explanations of words from dictionaries. Intersection chooses the intersection part of Chinese in a synset. Words Co-occurrence counts the results of Chinese and English words from Google. Then, we integrate these three approaches into an integrated one, which is named MIWA. Experimental results show that the integrated approach MIWA has better performance: F-measure reaches 0.615, which is higher than that of each independent one.</p></td></tr><tr><td>1085</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_15">Bounded Ontological Consistency for Scalable Dynamic Knowledge Infrastructures</a></td></tr><tr><td colspan=3><p>Both semantic web applications and individuals are in need of knowledge infrastructures that can be used in dynamic and distributed environments where different autonomous entities create knowledge and build their own view of a domain. Our framework represents this using evolving simple contextual ontologies and mappings between them, at the same time as incremental logical coherence is maintained. The definition of semantic autonomy includes these aspects. Our earlier research has shown that a knowledge infrastructure can have semantic autonomy that maintains global consistency, if the knowledge representation is kept simple. We generalize that research by investigating what happens if the consistency of a knowledge infrastructure is bounded 1) within certain regions called spheres of consistency, and 2) by allowing a limited variable degree of inconsistency. Our experiments show that a phase transition can occur in this kind of system, beyond which constant-time and constant-memory complexity is approached.</p></td></tr><tr><td>1086</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_12">A Pattern Based Approach for Re-engineering Non-Ontological Resources into Ontologies</a></td></tr><tr><td colspan=3><p>With the goal of speeding up the ontology development process, ontology engineers are starting to reuse as much as possible available ontologies and non-ontological resources such as classification schemes, thesauri, lexicons and folksonomies, that already have some degree of consensus. The reuse of such non-ontological resources necessarily involves their re-engineering into ontologies. Non-ontological resources are highly heterogeneous in their data model and contents: they encode different types of knowledge, and they can be modeled and implemented in different ways. In this paper we present (1) a typology for non-ontological resources, (2) a pattern based approach for re-engineering non-ontological resources into ontologies, and (3) a use case of the proposed approach.</p></td></tr><tr><td>1087</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_11">ROC: A Method for Proto-ontology Construction by Domain Experts</a></td></tr><tr><td colspan=3><p>Ontology construction is a labour-intensive and costly process. Even though many formal and semi-formal vocabularies are available, creating an ontology for a specific application is hindered in a number of ways. Firstly, the process of elicitating concepts is a time consuming and strenuous process. Secondly, it is difficult to keep focus. Thirdly, technical modelling constructs are hard to understand for the uninitiated. We propose ROC as a method to cope with these problems. ROC builds on well-known approaches for ontology construction. However, we reuse existing sources to generate a repository of proposed associations. ROC assists in efficiently putting forward all relevant concepts and relations by providing a large set of potential candidate associations. Secondly, rather than using intermediate representations of formal constructs we confront the domain expert with ‘natural-language-like’ statements generated from RDF-based triples. Moreover, we strictly separate the roles of problem owner, domain expert and knowledge engineer, each having his own responsibilities and skills. The domain expert and problem owner keep focus by monitoring a well-defined application purpose. We have implemented an initial set of tools to support ROC. This paper describes the ROC method and two application cases in which we evaluate the overall approach.</p></td></tr><tr><td>1088</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_13">Efficient Index Maintenance for Frequently Updated Semantic Data</a></td></tr><tr><td colspan=3><p>Nowadays, the demand on querying and searching the Semantic Web is increasing. Some systems have adopted IR (Information Retrieval) approaches to index and search the Semantic Web data due to its capability to handle the Web-scale data and efficiency on query answering. Additionally, the huge volumes of data on the Semantic Web are frequently updated. Thus, it further requires effective update mechanisms for these systems to handle the data change. However, the existing update approaches only focus on document. It still remains a big challenge to update IR index specially designed for semantic data in the form of finer grained structured objects rather than unstructured documents. In this paper, we present a well-designed update mechanism on the IR index for triples. Our approach provides a flexible and effective update mechanism by dividing the index into blocks. It reduces the number of update operations during the insertion of triples. At the same time, it preserves the efficiency on query processing and the capability to handle large scale semantic data. Experimental results show that the index update time is a fraction of that by complete reconstruction w.r.t. the portion of the inserted triples. Moreover, the query response time is not notably affected. Thus, it is capable to make newly arrived semantic data immediately searchable for users.</p></td></tr><tr><td>1089</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_9">Deriving Concept Mappings through Instance Mappings</a></td></tr><tr><td colspan=3><p>Ontology matching is a promising step towards the solution to the interoperability problem of the Semantic Web. Instance-based methods have the advantage of focusing on the most active parts of the ontologies and reflect concept semantics as they are actually being used. Previous instance-based mapping techniques were only applicable to cases where a substantial set of instances shared by both ontologies. In this paper, we propose to use a lexical search engine to map instances from different ontologies. By exchanging concept classification information between these mapped instances, an artificial set of common instances is built, on which existing instance-based methods can apply. Our experiment results demonstrate the effectiveness and applicability of this method in broad thesaurus mapping context.</p></td></tr><tr><td>1090</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_14">Towards a Component-Based Framework for Developing Semantic Web Applications</a></td></tr><tr><td colspan=3><p>For those outside the research community, to develop Semantic Web applications entails real difficulty. This difficulty is due in part to the lack of usable approaches for planning Semantic Web solutions, even though Semantic Web tools have already reached industrial maturity. We propose here the Semantic Web Framework, a component-based framework for analysing rapidly the required components, the dependencies between them, and selecting existing solutions. This approach has been tested with a number of industrial partners, which justifies the effort made in this direction.</p></td></tr><tr><td>1091</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_10">Deep Semantic Mapping between Functional Taxonomies for Interoperable Semantic Search</a></td></tr><tr><td colspan=3><p>This paper discusses ontology mapping between two taxonomies of functions of artifacts for the engineering knowledge management. The mapping is of two ways and has been manually established with deep semantic analysis based on a reference ontology of function for bridging the ontological gaps between the taxonomies. We report on the successful results thanks to such deep analysis not at the lexical level but at the ontological level. Using the mapping knowledge, we developed a semantic search system which can provide engineers with interoperable access to technical documents by searching for functional metadata based on either of functional taxonomies.</p></td></tr><tr><td>1092</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_6">SAOR: Authoritative Reasoning for the Web</a></td></tr><tr><td colspan=3><p>In this paper we discuss the challenges of performing reasoning on large scale RDF datasets from the Web. We discuss issues and practical solutions relating to reasoning over web data using a rule-based approach to forward-chaining; in particular, we identify the problem of ontology hijacking: new ontologies published on the Web re-defining the semantics of existing concepts resident in other ontologies. Our solution introduces consideration of authoritative sources. Our system is designed to scale, comprising of file-scans and selected lightweight on-disk indices. We evaluate our methods on a dataset in the order of a hundred million statements collected from real-world Web sources.</p></td></tr><tr><td>1093</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_7">Scalable Distributed Ontology Reasoning Using DHT-Based Partitioning</a></td></tr><tr><td colspan=3><p>Ontology reasoning is an indispensable step to fully exploit the implicit semantics of Semantic Web data. The inherent distribution characteristic of the Semantic Web and huge amount of ontology instance data necessitates efficient and scalable distributed ontology reasoning. Current researches on distributed ontology reasoning mainly focus on dealing with the heterogeneity of different ontologies but pay little attention to the performance of distributed reasoning and have not presented practical approaches and systems. Our goal is to propose an efficient and scalable distributed ontology reasoning approach, making it practical in real semantic applications. We propose an approach in this paper, in which Description Logic reasoners for TBox reasoning are combined with rule engines for ABox reasoning to support both expressive ontologies and large amount of instance data. The published data from each node is distributed using a DHT-based partitioning and stored in well-designed relational databases to support convenient and efficient reasoning through cooperation of the distributed nodes. A practical distributed ontology reasoning and querying system called DORS is developed based on our proposed approach. Our experiments both in LANs and on PlanetLab using University Ontology Benchmark show high efficiency of DORS compared with the centralized OWL ontology reasoning system Minerva as well as good scalability with respect to the number of nodes and volume of data in the system.</p></td></tr><tr><td>1094</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_5">A Tableau Algorithm for Possibilistic Description Logic </a></td></tr><tr><td colspan=3><p>Uncertainty reasoning and inconsistency handling are two important problems that often occur in the applications of the Semantic Web. Possibilistic description logics provide a flexible framework for representing and reasoning with ontologies where uncertain and/or inconsistent information is available. Although possibilistic logic has become a popular logical framework for uncertainty reasoning and inconsistency handling, its role in the Semantic Web is underestimated. One of the challenging problems is to provide a practical algorithm for reasoning in possibilistic description logics. In this paper, we propose a tableau algorithm for possibilistic description logic </p></td></tr><tr><td>1095</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_3">Temporal Ontology Language for Representing and Reasoning Interval-Based Temporal Knowledge</a></td></tr><tr><td colspan=3><p>W3C Web Ontology working group has recently developed OWL as an ontology language for the Semantic Web. However, because OWL does not have the full-fledged semantics for temporal information, it cannot perform reasoning about temporal knowledge. Entities in the real world are changing according to the passage of time and new facts are occurring due to events. If knowledge in the KBs does not have the temporal information, it becomes incomplete and incorrect. Therefore, we in this paper propose an ontology language TL-OWL, which extends OWL to have the temporal semantics in order to represent and reason the temporal information in the Semantic Web.</p></td></tr><tr><td>1096</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_2"> − </a></td></tr><tr><td colspan=3><p>We give a classification of the complexity of </p></td></tr><tr><td>1097</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_28">IP-Explorer: A Semantic Web Based Intellectual Property Knowledge Base and Trading Platform</a></td></tr><tr><td colspan=3><p>In this paper, we demonstrate IP-Explorer, a semantic web based IP knowledge base and trading platform with the following characteristics: First, it is based on the semantic web technology; Second, it is built by mining the China High-Tech Fair(CHTF) database and the State Intellectual Property Office of PRC (SIPO) database; Third, "Smart Pushing" proactively pushes useful information based on user profiles, browsing and searching records, and activities of peers; Fourth, "IP family" collects IP’s from different industry sectors and integrate them as a "family", helping the user to broaden his horizon; Fifth, "Technology Roadmap" provides a visualization tool for decision makers to observe technology history and trends.</p></td></tr><tr><td>1098</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_27">SHARE: A Semantic Web Query Engine for Bioinformatics</a></td></tr><tr><td colspan=3><p>Driven by the goal of automating data analyses in the field of bioinformatics, SHARE (Semantic Health and Research Environment) is a specialized SPARQL engine that resolves queries against Web Services and SPARQL endpoints. Developed in conjunction with SHARE, SADI (Semantic Automated Discovery and Integration) is a standard for native-RDF services that facilitates the automated assembly of services into workflows, thereby eliminating the need for ad hoc scripting in the construction of a bioinformatics analysis pipeline.</p></td></tr><tr><td>1099</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_4">A Formal Semantics-Preserving Translation from Fuzzy Relational Database Schema to Fuzzy OWL DL Ontology</a></td></tr><tr><td colspan=3><p>How to construct Web ontologies has become a key technology to enable the Semantic Web, especially how to construct ontologies by extracting domain knowledge from database models such as the relational database model. But in real-world applications, information is often imprecise and uncertain, thus the formal approach to translation from Fuzzy Relational Database Schema (FRDBS) to fuzzy ontology is helpful for extracting domain knowledge from database, which can profitably support fuzzy ontology development and developing data-intensive Semantic Web applications. In this paper, we first give the formal definition of FRDBS. Then, the formal definition and Model-Theoretic semantics of a kind of new fuzzy OWL DL ontology are given in more detail. What’s more, we realize the formal translation from FRDBS to fuzzy OWL DL ontology by means of reverse engineering technique. Of course, the correctness of translation is also proved. With an example, it shows that the translation method is semantics-preserving and effective. Finally, the reasoning problem of satisfiability, subsumption, and redundancy of FRDBS may reason automatically through reasoning mechanism of the corresponding fuzzy description logic f-SHOIN(D) of fuzzy OWL DL ontology is also investigated, which can further contribute to constructing fuzzy OWL DL ontologies exactly that meet application’s needs well.</p></td></tr><tr><td>1100</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_26">RepOSE: An Environment for Repairing Missing Ontological Structure</a></td></tr><tr><td colspan=3><p>Developing ontologies is not an easy task and often the resulting ontologies are not consistent or complete. Such ontologies, although often useful, lead to problems when used in semantically-enabled applications.Wrong conclusions may be derived or valid conclusions may be missed. Defects in ontologies can take different forms. Syntactic defects are usually easy to find and to resolve. Defects regarding style include such things as unintended redundancy. More severe defects are the modeling defects which require domain knowledge to detect and resolve, and semantic defects such as unsatisfiable concepts and inconsistent ontologies.</p></td></tr><tr><td>1101</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_25">Semantic Rules on Drug Discovery Data</a></td></tr><tr><td colspan=3><p>Aggregating and presenting a wide variety of information pertinent to the biological and pharmacological effects of chemical compounds will be a critical part of 21st century drug discovery. However there is currently a lack of tools for effectively integrating and aggregating information about chemical compound. In this paper we tackle this problem using Semantic Web Technologies, particularly OWL ontologies,compound-centric RDF networks, and RDF inference to detect relationships between compounds and biological affects, genes,and diseases, and to present information to a user clustered by disease area.</p></td></tr><tr><td>1102</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_1">A Modularization-Based Approach to Finding All Justifications for OWL DL Entailments</a></td></tr><tr><td colspan=3><p>Finding the justifications for an entailment (i.e., minimal sets of axioms responsible for it) is a prominent reasoning service in ontology engineering, as justifications facilitate important tasks like debugging inconsistencies or undesired subsumption. Though several algorithms for finding all justifications exist, issues concerning efficiency and scalability remain a challenge due to the sheer size of real-life ontologies. In this paper, we propose a novel method for finding all justifications in OWL DL ontologies by limiting the search space to smaller modules. To this end, we show that so-called locality-based modules cover all axioms in the justifications. We present empirical results that demonstrate an improvement of several orders of magnitude in efficiency and scalability of finding all justifications in OWL DL ontologies.</p></td></tr><tr><td>1103</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_23">Overcoming Schema Heterogeneity between Linked Semantic Repositories to Improve Coreference Resolution</a></td></tr><tr><td colspan=3><p>Schema heterogeneity issues often represent an obstacle for discovering coreference links between individuals in semantic data repositories. In this paper we present an approach, which performs ontology schema matching in order to improve instance coreference resolution performance. A novel feature of the approach is its use of existing instance-level coreference links defined in third-party repositories as background knowledge for schema matching techniques. In our tests of this approach we obtained encouraging results, in particular, a substantial increase in recall in comparison with existing sets of coreference links.</p></td></tr><tr><td>1104</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_24">Social Semantic Rule Sharing and Querying in Wellness Communities</a></td></tr><tr><td colspan=3><p>In this paper we describe the Web 3.0 case study WellnessRules, where ontology-structured rules (including facts) about wellness opportunities are created by participants in rule languages such as Prolog and N3, and translated for interchange within a wellness community using RuleML/XML. The wellness rules are centered around participants, as profiles, encoding knowledge about their activities, nutrition, etc. conditional on the season, the time-of-day, the weather, etc. This distributed knowledge base extends fact-only FOAF profiles with a vocabulary and rules about wellness group networking. The communication between participants is organized through Rule Responder, permitting translator-based reuse of wellness profiles and their distributed querying across engines. WellnessRules interoperates between rules and queries in the relational (Datalog) paradigm of the pure-Prolog subset of POSL and in the frame (F-logic) paradigm of N3. These derivation rule languages are implemented in the engines OO jDREW and Euler, and connected via Rule Responder to support wellness communities. An evaluation of Rule Responder instantiated for WellnessRules found acceptable Web response times.</p></td></tr><tr><td>1105</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_20">Querying the Web of Data: A Formal Approach</a></td></tr><tr><td colspan=3><p>The increasing amount of interlinked RDF data has finally made available the necessary building blocks for the web of data. This in turns makes it possible (and interesting) to query such a collection of graphs as an open and decentralized knowledge base. However, despite the fact that there are already implementations of query answering algorithms for the web of data, there is no formal characterization of what a satisfactory answer is expected to be. In this paper, we propose a preliminary model for such an open collection of graphs which goes beyond the standard single-graph RDF semantics, describes three different ways in which a query can be answered, and characterizes them semantically in terms of three incremental restrictions on the relation between the domain of interpretation of each single component graph.</p></td></tr><tr><td>1106</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_22">Entropy-Based Metrics for Evaluating Schema Reuse</a></td></tr><tr><td colspan=3><p>Schemas, which provide a way to give structure to information, are becoming more and more important for information integration. The model described here provides concrete metrics of the momentary “health” of an application and its evolution over time, as well as a means of comparing one application with another. Building upon the basic notions of actors, concepts, and instances, the presented technique defines and measures the information entropy of a number of simple relationships among these objects. The technique itself is evaluated against data sets drawn from the Freebase collaborative database, the Swoogle search engine, and an instance of Semantic MediaWiki.</p></td></tr><tr><td>1107</td><td>2008</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-540-89704-0_8">Versatile Semantic Modeling of Frame Logic Programs under Answer Set Semantics</a></td></tr><tr><td colspan=3><p>This work introduces the framework of Frame Answer Set programs (</p></td></tr><tr><td>1108</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_19">Improving Folksonomies Using Formal Knowledge: A Case Study on Search</a></td></tr><tr><td colspan=3><p>Search in folksonomies is impeded by lack of machine understandable descriptions for the meaning of tags and their relations. One approach to addressing this problem is the use of formal knowledge resources (KS) to assign meaning to the tags, most notably WordNet and (online) ontologies. However, there is no insight of how the different characteristics of such KS can contribute to improving search in folksonomies. In this work we compare the two KS in the context of folksonomy search, first by evaluating the enriched structures and then by performing a user study on searching the folksonomy content through these structures. We also compare them to cluster-based folksonomy search. We show that the diversity of ontologies leads to more satisfactory results compared to WordNet although the latter provides richer structures. We also conclude that the idiosyncrasies of folksonomies can not be addressed by only using formal KS.</p></td></tr><tr><td>1109</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_21">A Relevance-Directed Algorithm for Finding Justifications of DL Entailments</a></td></tr><tr><td colspan=3><p>Finding the justifications of an entailment, i.e. minimal sets of axioms responsible for the entailment, is an important problem in ontology engineering and thus has become a key reasoning task for Description Logic-based ontologies. Although practical techniques to find all possible justifications exist, efficiency is still a problem. Furthermore, in the worst case the number of justifications for a subsumption entailment is exponential in the size of the ontology. Therefore, it is not always desirable to compute all justifications. In this paper, we propose a novel black-box algorithm that iteratively constructs a set of justifications of an entailment using a relevance-based selection function. Each justification returned by our algorithm is attached with a weight denoting its relevance degree w.r.t. the entailment. Finally, we implement the algorithm and present evaluation results over real-life ontologies that show the benefits of the selection function.</p></td></tr><tr><td>1110</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_17">Two-Fold Service Matchmaking – Applying Ontology Mapping for Semantic Web Service Discovery</a></td></tr><tr><td colspan=3><p>Semantic Web Services (SWS) aim at the automated discovery and orchestration of Web services on the basis of comprehensive, machine-interpretable semantic descriptions. Since SWS annotations usually are created by distinct SWS providers, semantic-level mediation, i.e. mediation between concurrent semantic representations, is a key requirement for SWS discovery. Since semantic-level mediation aims at enabling interoperability across heterogeneous semantic representations, it can be perceived as a particular instantiation of the ontology mapping problem. While recent SWS matchmakers usually rely on manual alignments or subscription to a common ontology, we propose a two-fold SWS matchmaking approach, consisting of (a) a general-purpose semantic-level mediator and (b) comparison and matchmaking of SWS capabilities. Our semantic-level mediation approach enables the implicit representation of similarities across distinct SWS by grounding service descriptions in so-called Mediation Spaces (MS). Given a set of SWS and their respective grounding, a SWS matchmaker automatically computes instance similarities across distinct SWS ontologies and matches the request to the most suitable SWS. A prototypical application illustrates our approach.</p></td></tr><tr><td>1111</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_15">Utilising Task-Patterns in Organisational Process Knowledge Sharing</a></td></tr><tr><td colspan=3><p>Pattern based task management has been proposed as a promising approach to work experience reuse in knowledge intensive work environments. This paper inspects the need of organisational work experience sharing and reuse in the context of a real-life scenario based on use case studies. We developed a task pattern management system that supports process knowledge externalisation-internalisation. The system brings together task management related concepts and semantic technologies that materialise the former through a variety of semantic enhanced measures. Case studies were carried out for evaluating the proposed approach and also for drawing inspiration for future development.</p></td></tr><tr><td>1112</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_8">An Effective Similarity Propagation Method for Matching Ontologies without Sufficient or Regular Linguistic Information</a></td></tr><tr><td colspan=3><p>Most existing ontology matching methods are based on the linguistic information. However, some ontologies have not sufficient or regular linguistic information such as natural words and comments, so the linguistic-based methods can not work. Structure-based methods are more practical for this situation. Similarity propagation is a feasible idea to realize the structure-based matching. But traditional propagation does not take into consideration the ontology features and will be faced with effectiveness and performance problems. This paper analyzes the classical similarity propagation algorithm </p></td></tr><tr><td>1113</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_7">Entity Resolution in Texts Using Statistical Learning and Ontologies</a></td></tr><tr><td colspan=3><p>Ambiguities, which are inherently present in natural languages represent a challenge of determining the actual identities of entities mentioned in a document (e.g., </p></td></tr><tr><td>1114</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_6">Repairing the Missing is-a Structure of Ontologies</a></td></tr><tr><td colspan=3><p>Developing ontologies is not an easy task and often the resulting ontologies are not consistent or complete. Such ontologies, although often useful, also lead to problems when used in semantically-enabled applications. Wrong conclusions may be derived or valid conclusions may be missed. To deal with this problem we may want to repair the ontologies. Up to date most work has been performed on finding and repairing the semantic defects such as unsatisfiable concepts and inconsistent ontologies. In this paper we tackle the problem of repairing modeling defects and in particular, the repairing of structural relations (is-a hierarchy) in the ontologies. We study the case where missing is-a relations are given. We define the notion of a structural repair and develop algorithms to compute repairing actions that would allow deriving the missing is-a relations in the repaired ontology. Further, we define preferences between repairs. We also look at how we can use external knowledge to recommend repairing actions to a domain expert. Further, we discuss an implemented prototype and its use as well as an experiment using the ontologies of the Anatomy track of the Ontology Alignment Evaluation Initiative.</p></td></tr><tr><td>1115</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_3">Supporting the Development of Data Wrapping Ontologies</a></td></tr><tr><td colspan=3><p>We consider the problem of designing data wrapping ontologies whose purpose is to describe relational data sources and to provide a semantically enriched access to the underlying data. Since such ontologies must be close to the data they wrap, the new terms that they introduce must be “supported” by data from the relational sources; i.e. when queried, they should return nonempty answers. In order to ensure non-emptiness, those wrapping ontologies are usually carefully handcrafted by taking into account the query answering mechanism. In this paper we address the problem of supporting an ontology engineer in this task. We provide an algorithm for verifying emptiness of a term in the data wrapping ontology w.r.t.the data sources. We also show how this algorithm can be used to guide the ontology engineer in fixing potential terms unsupported by the data. Finally, we present an implemented tool and an empirical study showing benefits of our approach.</p></td></tr><tr><td>1116</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_2">Modeling Common Real-Word Relations Using Triples Extracted from n-Grams</a></td></tr><tr><td colspan=3><p>In this paper, we present an approach providing generalized relations for automatic ontology building based on frequent word n-grams. Using publicly available Google </p></td></tr><tr><td>1117</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_18">An Approach to Analyzing Dynamic Trustworthy Service Composition</a></td></tr><tr><td colspan=3><p>Service composition and related technologies have provided favorable means for building complex Web software systems. It may span multiple organizational units requires particular considerations on trustworthy issues. However, the distributive and heterogeneous characteristics of services make it hard to guarantee trustworthiness of service composition. This paper presents a method for analyzing dynamic trustworthy service composition according to the characteristics and requirements of service composition. Petri nets are used to precisely describe the composition process in order to describe the logic relation between different components. Based on this, the concept of trust matrix is given to represent the relationships between states. A trustworthy service composition strategy and its enforcement method are proposed. A case study of Travel Service demonstrates the feasibility of proposed method.</p></td></tr><tr><td>1118</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_16">Reasoning about Partially Ordered Web Service Activities in PSL</a></td></tr><tr><td colspan=3><p>Many tasks within semantic web service discovery can be formalized as reasoning problems related to the partial ordering of subactivity occurrences in a complex activity. We show how the first-order ontology of the Process Specification Language (PSL) can be used to represent both the queries and the process descriptions that constitute the underlying theory for the reasoning problems. We also identify extensions of the PSL Ontology for which these problems are NP-complete and then explicitly axiomatize classes of activities for which the various reasoning problems are tractable.</p></td></tr><tr><td>1119</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_5">What Makes a Good Ontology? A Case-Study in Fine-Grained Knowledge Reuse</a></td></tr><tr><td colspan=3><p>Understanding which ontology characteristics can predict a “good” quality ontology, is a core and ongoing task in the Semantic Web. In this paper, we provide our findings on which structural ontology characteristics are usually observed in high-quality ontologies. We obtain these findings through a task-based evaluation, where the task is the assessment of the correctness of semantic relations. This task is of increasing importance for a set of novel Semantic Web tools, which perform fine-grained knowledge reuse (i.e., they reuse only appropriate parts of a given ontology instead of the entire ontology). We conclude that, while structural ontology characteristics do not provide statistically significant information to ensure that an ontology is reliable (“good”), in general, richly populated ontologies, with higher depth and breadth variance are more likely to provide reliable semantic content.</p></td></tr><tr><td>1120</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_4">A Conceptual Model for a Web-Scale Entity Name System</a></td></tr><tr><td colspan=3><p>The problem of identity and reference is receiving increasing attention in the (semantic) web community and is emerging as one of the key features which distinguish traditional knowledge representation from knowledge representation </p></td></tr><tr><td>1121</td><td>2009</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-642-10871-6_1">Cross-Lingual Ontology Mapping – An Investigation of the Impact of Machine Translation</a></td></tr><tr><td colspan=3><p>Ontologies are at the heart of knowledge management and make use of information that is not only written in English but also in many other natural languages. In order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. This paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in cross-lingual ontology mapping scenarios. In particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in English and Chinese are presented. Based on findings derived from these studies, limitations of this generic approach are discussed. It is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in cross-lingual ontology mapping. Finally, to address the identified challenges, a semantic-oriented cross-lingual ontology mapping (SOCOM) framework is proposed and discussed.</p></td></tr></table>
      </body>
    </html>.
    