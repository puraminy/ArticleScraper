
    <html>
      <head><title>HTML Pandas Dataframe with CSS</title></head>
      <link rel="stylesheet" type="text/css" href="df_style.css"/>
      <body>
        
    <table border="1" class="dataframe mystyle">
    <tr>
        <td colspan=2>Conference</td><td><a href="https://link.springer.com/conference/semweb>ISWC</a>"></td>
    </tr>
    <tr>
        <td colspan=2>Search terms</ts><td>none</td>
    </tr>
    <tr>
        <td colspan=2>Percentage</td><td>341 of 341 = 100.0%</td>
    </tr>
    <tr>
        <td colspan=2>From</td><td>all</td>
    </tr>
    <tr>
    <td colspan=3>Papers</td>
    </tr>
    <tr><td>0</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_32">Road Traffic Question Answering System Using Ontology</a></td></tr><tr><td colspan=3>Many people use social media to report and receive road traffic information, e.g., car accidents and congestions. We have implemented a Twitter-based traffic-related information reposting (retweeting) system, which users usually referred to as @traffy. To improve on our works, we propose an ontology-based Thai-language question answering system that gathers real-time traffic data from Twitter. The data collected are converted into traffic incident knowledge of </td></tr><tr><td>1</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_23">Classification of News by Topic Using Location Data</a></td></tr><tr><td colspan=3>In this work, we will consider news articles to determine geo-localization of their information and classify their topics on the basis of an available open data source: OpenStreetMap (OSM). We propose a knowledge-based conceptual and computational approach that disambiguates place names (i.e., geo-objects and regions) mentioned in news articles in terms of geographic coordinates. The geo-located news articles are analyzed to identify local topics: we found that the mentioned geo-objects are a good proxy to classify news topics.</td></tr><tr><td>2</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_31">Ontology Based Suggestion Distribution System </a></td></tr><tr><td colspan=3>The digitization of modern cities has brought cities to a new level. There are still many new areas yet to be discovered in this new ecosystem. Today, there is an urgent need for smarter cities to support the growing population. One particular problem is citizens do not know which city department to give their suggestions to. This paper presents a system for distributing suggestions from citizens to the right city officials based on ontology knowledge base. We use data from official websites to construct our ontology and do experiments with actual suggestions from citizens. The experiments show some promising results.</td></tr><tr><td>3</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_24">Monitoring and Automating Factories Using Semantic Models</a></td></tr><tr><td colspan=3>Keeping factories running at any time is a critical task for every manufacturing enterprise. Optimizing the flows of goods and services inside and between factories is a challenge that attracts much attention in research and business. The idea to fully describe a factory in a digital form to improve decision making is called a virtual factory. While promising virtual factory frameworks have been proposed, their semantic models lack depth and suffer from limited expressiveness. We propose an enhanced semantic model of a factory, which enables views spanning from the high level of supply chains to the low level of machines on the shop floor. The model includes a mapping to relational production databases to support federated queries on different legacy systems in use. We evaluate the model in a production line use case, demonstrating that it can be used for typical factory tasks, such as assembly line identification or machine availability checks.</td></tr><tr><td>4</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_23">Ontology Development for Interoperable Database to Share Data in Service Fields</a></td></tr><tr><td colspan=3>Sharing data from various systems such as sensor networks, time and motion study data, and text data is important. To process and manage the collected data, the authors have proposed a database framework called the COTO database. As described in this paper, the authors propose a COTO-ontology as the basis of the COTO database. The COTO-ontology was developed based on the data collected by DANCE, which is an evaluation tool of quality of care, from the real settings. It will be applied as a platform for the evaluation of robotic devices used for nursing care.
</td></tr><tr><td>5</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_22">Enabling Spatial OLAP Over Environmental and Farming Data with QB4SOLAP</a></td></tr><tr><td colspan=3>Governmental organizations and agencies have been making large amounts of spatial data available on the Semantic Web (SW). However, we still lack efficient techniques for analyzing such large amounts of data as we know them from relational database systems, e.g., multidimensional (MD) data warehouses and On-line Analytical Processing (OLAP). A basic prerequisite to enable such advanced analytics is a well-defined schema, which can be defined using the QB4SOLAP vocabulary that provides sufficient context for spatial OLAP (SOLAP). In this paper, we address the challenging problem of MD querying with SOLAP operations on the SW by applying QB4SOLAP to a non-trivial spatial use case based on real-world open governmental data sets across various spatial domains. We describe the process of combining, interpreting, and publishing disparate spatial data sets as a spatial data cube on the SW and show how to query it with SOLAP operators.</td></tr><tr><td>6</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_24">Efficiently Finding Paths Between Classes to Build a SPARQL Query for Life-Science Databases</a></td></tr><tr><td colspan=3>Many databases in life science are provided in Resource Description Framework (RDF) model with SPARQL Protocol and RDF Query Language (SPARQL) endpoints. However, it may be difficult for users who are not familiar with Semantic Web technologies to write a SPARQL query. Therefore, assisting users to build SPARQL queries is important task to expand the range of users of RDF databases. We developed a web application called SPARQL Builder (</td></tr><tr><td>7</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_29">Implementing LOD Surfer as a Search System for the Annotation of Multiple Protein Sequence Alignment</a></td></tr><tr><td colspan=3>Many life science databases have been provided as Linked Open Data (LOD). To promote the utilization of these databases, we had developed a method that can be referred to as LOD Surfer, that employed federated query search along a path of class–class relationships. In this study, we developed a specified version of the LOD Surfer for the annotation of multiple protein sequence alignment. The system comprised a web application programming interface (API) and a client system for the API. The web API provides a list of classes, and a list of paths between the classes that are specified by a user. The client presents the list of classes and the list of paths obtained from the API and assists a user in selecting classes and paths to acquire the required annotation of proteins. Additionally, the client system generates SPARQL queries to execute a federated query search for a selected path. During the development of the system, we can observe that (1) the client system should display some instances with human readable information because class selection is not an easy task for biological researchers, and (2) it is preferable that the client system stores paths that are selected by a user for reuse by other users because path selection may be time consuming at times and because the selected paths may be valuable for other researchers.</td></tr><tr><td>8</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_28">Integrated Semantic Model for Complex Disease Network</a></td></tr><tr><td colspan=3>To understand biological phenomena, biologists have identified the interactions between biological molecules in vivo. Until recently, all of the unique and interactive information of such molecules has been built into a database and made available online. Among them, there was an effort to understand the relationship of molecules based on biological pathways, and a standard model called BioPAX was made to enable interchange and operation of data. In particular, Pathway Commons integrates other biological data besides biological pathways using BioPAX. We are interested in identifying the molecular mechanisms of disease and recommending drugs for treatment. In addition to data provided by Pathway Commons, additional disease and drug data was added to be used in various analysis. We extended the model to express the data that BioPAX could not cover and converted all the data to RDF based on the model. We integrate and present diverse biological data using semantic technologies from the perspective of representing disease networks. We hope that this information will aid in a deeper understanding of disease and drug recommendations.</td></tr><tr><td>9</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_27">Semantic Navigation of Disease-Specific Pathways: The Case of Non-small Cell Lung Cancer (NSCLC)</a></td></tr><tr><td colspan=3>By studying the cancer genome, scientists can discover what base changes are causing a cell to become a cancer cell. In addition, cancers and diseases are affected by a series of complex interactions between a multitude of entities such as genes and proteins. Biological pathway analysis became necessary to understand these entities within diverse contexts. In this paper, we propose a framework for researchers to navigate disease-specific pathways. The basic structure of analysis data is BioPAX which is described in RDF and is produced by the Reactome database (biological pathway database). For this framework, we utilize a large scale of biological sources such as Pathway Commons, clinical data, dbSNP, and ClinVar. Especially, we choose non-small cell lung cancer (NSCLC) for case study to demonstrate components of semantic navigation. Furthermore, we generate and analyze non-small cell lung cancer (NSCLC) specific pathways. Our proposed system will help researchers find a point at which they begin their interests. For instance, it can help discover which protein or gene most affect a specific disease or it can aid in integrating different sources of biological information. Moreover, plenty of biological data extended by our system suggests a new perspective for scientists to find a direction of research.</td></tr><tr><td>10</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_24">Erratum to: Refinement-Based OWL Class Induction with Convex Measures</a></td></tr><tr><td colspan=3>nan</td></tr><tr><td>11</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_23">Linked Urban Open Data Including Social Problems’ Causality and Their Costs</a></td></tr><tr><td colspan=3>There are various urban problems, such as suburban crime, dead shopping street, and littering. However, various factors are socially intertwined; thus, structural management of the related data is required for visualizing and solving such problems. Moreover, in order to implement the action plans, local governments first need to grasp the cost-effectiveness. Therefore, this paper aims to construct Linked Open Data (LOD) that include causal relations of urban problems and the related cost information in the budget. We first designed a data schema that represents the urban problems’ causality and extended the schema to include budget information based on QB4OLAP. Next, we semi-automatically enriched instances according to the schema using natural language processing and crowdsourcing. Finally, as use cases of the resulting LOD, we provided example queries to extract the relationships between several problems and the particular cost information. We found several causes that lead to the vicious circle of urban problems and for the solutions of those problems, we suggest to a local government which actions should be addressed.</td></tr><tr><td>12</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_22">Semantically Enhanced Case Adaptation for Dietary Menu Recommendation of Diabetic Patients</a></td></tr><tr><td colspan=3>
Dietary menu planning for diabetic patients is a complicated tasks involving specific and common-sense knowledge. Case-based approach has been used to provide recommendation in the case where ratings were not easily available for domains such as menu planning. Among the important but yet difficult tasks in the case-based approach is case adaptation. To successfully support case adaptation, the constraint-based approach and food composition ontology were employed. Constraints knowledge were represented as production rules and exploits the food ontology to support adaptation. An ontological approach is also proposed to perform the inference process to satisfy the multiple design constraints.
</td></tr><tr><td>13</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_21">Semantic IoT: Intelligent Water Management for Efficient Urban Outdoor Water Conservation</a></td></tr><tr><td colspan=3>Water depletion is critical in the dry tropics due to drought, increased development and demographic or economic shifts. Although educational initiatives have improved urban indoor water-use, excessive outdoor wastage still occurs because in most urban areas residential users only have a biannual reading of quantity available to make informed or educated decisions on necessary or unnecessary consumption. For example, the average consumer will water lawns during a designated non-restricted time. The amount of water they use is determined arbitrarily (i.e., either by sight or by blocks of time). In many cases, water is wasted due to over saturation, automated sprinklers that cannot sense precipitation, poor placement of sprinkler direction, etc. Outdoor water use efficiency could be maximized if water flow was shut off when an area of lawn has had sufficient water based on a more intelligent monitoring system. This paper describes the development of an intelligent water management and information system that integrates real-time sensed data (soil moisture, etc.) and Web-available information to make dynamic decisions on water release for lawns and fruit trees. The initial pilot-prototype combines Semantic Technologies with Internet of Things to decrease urban outdoor water-use and educate residents on best water usage strategies.</td></tr><tr><td>14</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_20">User Participatory Construction of Open Hazard Data for Preventing Bicycle Accidents</a></td></tr><tr><td colspan=3>Recently, bicycle-related accidents, e.g., collision accidents at intersection increase and account for approximately 20% of all traffic accidents in Japan; thus, it is regarded as one of the serious social problems. However, the Traffic Accident Occurrence Map released by the Japanese Metropolitan Police Department is currently based on accident information records, and thus there are a number of near-miss events, which are overlooked in the map but will be useful for preventing the possible accidents. Therefore, we detect locations with high possibility of bicycle accidents using user participatory sensing and offer them drivers and government officials as Open Hazard Data (OHD) to prevent future bicycle accident. This paper uses smartphone sensors to obtain data for acceleration, location, and handle rotation information. Then, by classifying those data with convolutional neural networks, it was confirmed that the locations, where sudden braking occurred can be detected with an accuracy of 80%. In addition, we defined an RDF model for OHD that is currently publicly available. In future, we plan to develop applications using OHD, e.g., notifying alerts when users are approaching locations where near-miss events have occurred.</td></tr><tr><td>15</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_19">Development of Semantic Web-Based Imaging Database for Biological Morphome</a></td></tr><tr><td colspan=3>We introduce the RIKEN Microstructural Imaging Metadatabase, a semantic web-based imaging database in which image metadata are described using the Resource Description Framework (RDF) and detailed biological properties observed in the images can be represented as Linked Open Data. The metadata are used to develop a large-scale imaging viewer that provides a straightforward graphical user interface to visualise a large microstructural tiling image at the gigabyte level. We applied the database to accumulate comprehensive microstructural imaging data produced by automated scanning electron microscopy. As a result, we have successfully managed vast numbers of images and their metadata, including the interpretation of morphological phenotypes occurring in sub-cellular components and biosamples captured in the images. We also discuss advanced utilisation of morphological imaging data that can be promoted by this database.</td></tr><tr><td>16</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_17">KG-Buddhism: The Chinese Knowledge Graph on Buddhism</a></td></tr><tr><td colspan=3>One of the most important elements in human society is religion, which provides moralities to help regulate human behaviours. However, the Web lacks a specialized knowledge graph on religion. To facilitate religious knowledge sharing, we aim to build </td></tr><tr><td>17</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_16">Cross-Lingual Taxonomy Alignment with Bilingual Knowledge Graph Embeddings</a></td></tr><tr><td colspan=3>Recently, different knowledge graphs have become the essential components of many intelligent applications, but no research has explored the use of knowledge graphs to cross-lingual taxonomy alignment (CLTA), which is the task of mapping each category in the source taxonomy of one language onto a ranked list of most relevant categories in the target taxonomy of another language. In this paper, we study how to perform CLTA with a multilingual knowledge graph. Firstly, we identify the candidate matched categories in the target taxonomy for each category in the source taxonomy. Secondly, we find the relevant knowledge denoted as triples for each category in the given taxonomies. Then, we propose two different bilingual knowledge graph embedding models called BTransE and BTransR to encode triples of different languages into the same vector space. Finally, we perform CLTA based on the vector representations of the relevant RDF triples for each category. Preliminary experimental results show that our approach is comparable and complementary to the state-of-the-art method.</td></tr><tr><td>18</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_18">Semantic Graph Analysis for Federated LOD Surfing in Life Sciences</a></td></tr><tr><td colspan=3>Currently, Linked Open Data (LOD) is increasingly used when publishing life science databases. To facilitate flexible use of such databases, we employ a method that uses federated query search along a path of class–class relationships. However, an effective method for federated query search requires analysis of the structure the relationships form for LOD datasets. Therefore, we constructed a graph of class–class relationships among 43 SPARQL endpoints and analyzed the connectivity of the graph. As a result, we found that (1) the sizes of connected components follow a power law; thus we should deal with the classes separately according to the size of connected components, (2) only the largest and second largest connected components have paths among classes from two or more SPARQL endpoints, and the datasets of each of the two connected components share ontologies, and (3) key classes that connect SPARQL endpoints are primarily upper-level concepts in the biological domain.</td></tr><tr><td>19</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_15">Enhancing Knowledge Graph Embedding from a Logical Perspective</a></td></tr><tr><td colspan=3>Knowledge graph embedding aims to represent entities and relations in a knowledge graph as low-dimensional real-value vectors. Most existing studies exploit only structural information to learn these vectors. This paper studies how logical information expressed as RBox axioms in OWL 2 is used for embedding. The involvement of RBox axioms could prevent existing methods from learning predictive vectors. For example, the symmetric, reflexive or transitive relations can be declared by RBox axioms, but popular translation-based methods are unable to learn distinguishable vectors for multiple these relations in the ideal case. To overcome these limitations introduced by the involvement of RBox axioms, this paper proposes to enhance existing translation-based methods by logical pre-completion and bi-directional projection of entities. Experimental results demonstrate that these enhancements improve the predictive performance in link prediction and triple classification.</td></tr><tr><td>20</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_11">Missing RDF Triples Detection and Correction in Knowledge Graphs</a></td></tr><tr><td colspan=3>Knowledge graphs (KGs) have become a powerful asset in information science and technology. To foster enhancing search, information retrieval and question answering domains KGs offer effective structured information. KGs represent real-world entities and their relationships in Resource Description Framework (RDF) triples format. Despite the large amount of knowledge, there are still missing and incorrect knowledge in the KGs. We study the graph patterns of interlinked entities to discover missing and incorrect RDF triples in two KGs - DBpedia and YAGO. We apply graph-based approach to map similar object properties and apply similarity based approach to map similar datatype properties. Our propose methods can utilize those similar ontology properties and efficiently discover missing and incorrect RDF triples in DBpedia and YAGO.</td></tr><tr><td>21</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_14">Mining Inverse and Symmetric Axioms in Linked Data</a></td></tr><tr><td colspan=3>In the context of Linked Open Data, substantial progress has been made in mining of property subsumption and equivalence axioms. However, little progress has been made in determining if a predicate is symmetric or if its inverse exists within the data. Our study of popular linked datasets such as DBpedia, YAGO and their associated ontologies has shown that they contain very few inverse and symmetric property axioms. The state-of-the-art approach ignores the open-world nature of linked data and involves a time-consuming step of preparing the input for the rule-miner. To overcome these shortcomings, we propose a schema-agnostic unsupervised method to discover inverse and symmetric axioms from linked datasets. For mining inverse property axioms, we find that other than support and confidence scores, a new factor called predicate-preference factor (</td></tr><tr><td>22</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_12">A New Sentiment and Topic Model for Short Texts on Social Media</a></td></tr><tr><td colspan=3>Nowadays plenty of user-generated posts, e.g., tweets and sina weibos, are published on social media and the posts imply the public’s opinions towards various topics. Joint sentiment/topic models are widely applied in detecting sentiment-aware topics on the lengthy documents. However, the characteristics of posts, i.e., short texts, on social media pose new challenges: (1) context sparsity problem of posts makes traditional sentiment-topic models inapplicable; (2) conventional sentiment-topic models are designed for flat documents without structure information, while publishing users, publishing timeslices and hashtags of posts provide rich structure information for these posts. In this paper, we firstly devise a method to mine potential hashtags, based on explicit hashtags, to further enrich structure information for posts, then we propose a novel Sentiment Topic Model for Posts (STMP) which aggregates posts with the structure information, i.e., timeslices, users and hashtags, to alleviate the context sparsity problem. Experiments on Sentiment140 and Twitter7 show STMP outperforms previous models both in sentiment classification and sentiment-aware topic extraction.</td></tr><tr><td>23</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_13">Semi-supervised Stance-Topic Model for Stance Classification on Social Media</a></td></tr><tr><td colspan=3>Stance detection aims to automatically determine from text whether the author of the text is in favor of, against, or neutral towards a issue. Social media, such as Sina Weibo, reflects the general public’s stances towards different issues. Detecting and summarizing stances towards specific issues from social media is an important and challenging task. Although stance detection on social media has been studied before, previous work, most of which are based on supervised learning, may not work well because they suffer from its heavy dependence on training data. Other weakly supervised method also use some heuristic rules to select the posts with specific stances as training data, but these selected posts often concentrate on a few subtopics of the specific issue, these weakly supervised method can only train a biased stance classifier. To better detect stances toward specific issues, we consider to detect stances with a small number of labeled training data and a mass of unlabeled data. To integrate the supervised information into our model, we combine a discriminative maximum entropy (Max-Ent) component with the generative component. The Max-Ent component leverages hand-crafted features from labeled data to separate different stances. In this paper, we propose a semi-supervised topic model, Semi-Supervised Stance Topic Model (SSTM), that model stances and topics of the posts on social media. Since the posts on social media are short texts, we also incorporate the structural information of the posts, i.e., gender information, location information and time information, to aggregate posts for alleviating the context sparsity of the posts. The model has been evaluated on the selected posts on sina weibo, which talk about “the verbal battle of Han han and Fang zhouzi”, to classify the stance of each posts. Preliminary experiments have shown promising results achieved by SSTM. Moreover, we also analyze the common difficulties in stance detection on social media. Finally, we also visualize the subtopics of the given issue generated by SSTM.</td></tr><tr><td>24</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_10">Publishing E-RDF Linked Data for Many Agents by Single Third-Party Server</a></td></tr><tr><td colspan=3>
Linked data is one of the most successful practices in semantic web, which has led to the opening and interlinking of data. Though many agents (mostly academic organizations and government) have published a large amount of linked data, numerous agents such as private companies and industries either do not have the ability or do not want to make an additional effort to publish linked data. Thus, for agents who are willing to open part of their data but do not want to make an effort, the task can be undertaken by a professional third-party server (together with professional experts) that publishes linked data for these agents. Consequently, when a single third-party server is on behalf of multiple agents, it is also responsible to organize these multiple-source URIs (data) in a systematic way to make them referable, satisfying the 4-star data principles, as well as protect the confidential data of these agents. In this paper, we propose a framework to leverage these challenges and design a URI standard based on our proposed E-RDF, which extends and optimizes the existing 5-star linked data principles. Also, we introduce a customized data filtering mechanism to protect the confidential data. For validation, we implement a prototype system as a third-party server that publishes linked data for a number of agents. It demonstrates well-organized 5-star linked data plus E-RDF and shows the additional advantages of data integration and interlinking among agents.
</td></tr><tr><td>25</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_7">The Identity Problem in Description Logic Ontologies and Its Application to View-Based Information Hiding</a></td></tr><tr><td colspan=3>The work in this paper is motivated by a privacy scenario in which the identity of certain persons (represented as anonymous individuals) should be hidden. We assume that factual information about known individuals (i.e., individuals whose identity is known) and anonymous individuals is stored in an ABox and general background information is expressed in a TBox, where both the TBox and the ABox are publicly accessible. The identity problem then asks whether one can deduce from the TBox and the ABox that a given anonymous individual is equal to a known one. Since this would reveal the identity of the anonymous individual, such a situation needs to be avoided. We first observe that not all Description Logics (DLs) are able to derive any such equalities between individuals, and thus the identity problem is trivial in these DLs. We then consider DLs with nominals, number restrictions, or function dependencies, in which the identity problem is non-trivial. We show that in these DLs the identity problem has the same complexity as the instance problem. Finally, we consider an extended scenario in which users with different rôles can access different parts of the TBox and ABox, and we want to check whether, by a sequence of rôle changes and queries asked in each rôle, one can deduce the identity of an anonymous individual.</td></tr><tr><td>26</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_9">Entity Linking in Queries Using Word, Mention and Entity Joint Embedding</a></td></tr><tr><td colspan=3>Entity linking in queries is an important task for connecting search engines and knowledge bases. This task is very challenging because queries are usually very short and there is very limited context information for entity disambiguation. This paper proposes a new accurate and efficient entity linking approach for search queries. The proposed approach first jointly learns word, mention and entity embeddings in a unified space, and then computes a set of features for entity disambiguation based on the learned embeddings. The entity linking problem is solved as a ranking problem in our approach, a ranking SVM is trained to accurately predict entity links. Experiments on real data show that our proposed approach achieves better performance than comparison approaches.</td></tr><tr><td>27</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_8">Resolving Range Violations in DBpedia</a></td></tr><tr><td colspan=3>DBpedia, a large-scale multi-disciplinary knowledge graph extracted from structured data in Wikipedia, is an essential part of the Linked Open Data (LOD). However, several previous works report many types of errors existing in DBpedia. The crucial one is </td></tr><tr><td>28</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_5">Reasoning on Context-Dependent Domain Models</a></td></tr><tr><td colspan=3>Modelling context-dependent domains is hard, as capturing multiple context-dependent concepts and constraints easily leads to inconsistent models or unintended restrictions. However, current semantic technologies not yet support reasoning on context-dependent domains. To remedy this, we introduced ConDL, a set of novel description logics tailored to reason on contextual knowledge, as well as JConHT, a dedicated reasoner for ConDL ontologies. ConDL enables reasoning on the consistency and satisfiability of context-dependent domain models, e.g., Compartment Role Object Models (CROM). We evaluate the suitability and efficiency of our approach by reasoning on a modelled banking application and measuring the performance on randomly generated models.</td></tr><tr><td>29</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_2">Data Structuring for Launching Web Services Triggered by Media Content</a></td></tr><tr><td colspan=3>There are some efforts to inspire the viewers to buy something or visit somewhere when they are viewing such objects or places on the TV programs or web sites. It is required to link some specific services, which can be run on smartphones or tablets, to content on TV or a web site. However, simple combination of content and possible services still requires some operations taken by the viewers such as typing search words in the applications or the pages of the services. Such required actions may decline their motivations to use the services. Therefore, the authors propose the data model that allows to seek matching of an entity within content with various services, and to generate service launch information dynamically changed by combination of an entity and a service. The data model is based on combination of ontology class structure and reasoning. In this paper, effectiveness of the data model is shown by the prototype applications which call various web services to inspire viewers for subsequent actions in accordance with media content.
</td></tr><tr><td>30</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_4">Refinement-Based OWL Class Induction with Convex Measures</a></td></tr><tr><td colspan=3>Beam-search may be used to iteratively explore and evaluate </td></tr><tr><td>31</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_6">Energy-Efficiency of OWL Reasoners—Frequency Matters</a></td></tr><tr><td colspan=3>While running times of ontology reasoners have been studied extensively, studies on energy-consumption of reasoning are scarce, and the energy-efficiency of ontology reasoning is not fully understood yet. Earlier empirical studies on the energy-consumption of ontology reasoners focused on reasoning on smart phones and used measurement methods prone to noise and side-effects. This paper presents an evaluation of the energy-efficiency of five state-of-the-art OWL reasoners on an ARM single-board computer that has built-in sensors to measure the energy consumption of CPUs and memory precisely. Using such a machine gives full control over installed and running software, active clusters and CPU frequencies, allowing for a more precise and detailed picture of the energy consumption of ontology reasoning. Besides evaluating the energy consumption of reasoning, our study further explores the relationship between computation power of the CPU, reasoning time, and energy consumption.</td></tr><tr><td>32</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_3">Refined JST Thesaurus Extended with Data from Other Open Life Science Data Sources</a></td></tr><tr><td colspan=3>We are developing a refined Japan Science and Technology (JST) thesaurus with thirty-five relations to enable description of rigorous relationships among concepts. In this study, we prepared an environment for performing SPARQL queries and evaluated the JST thesaurus in the life sciences by comparing query results with the originals. Based on the results of the investigation, we constructed a fibrinolysis network from the thesaurus as a collection of concepts connected with fibrinolysis within three steps, and we discovered that fibrinolysis was associated with fifty-four concepts, including sixteen diseases and twelve physiological phenomena. Subsequently, using the sub-classified relations, we divided the sixteen diseases into two diseases that developed after fibrinolysis progressed, seven diseases that shared common molecules in the development mechanism with fibrinolysis, and other associated conditions. Furthermore, we mapped concepts between the JST thesaurus, ChEBI, and Gene Ontology by matching the labels and synonyms. As a result, we could integrate the fibrinolysis network with thirty-seven chemicals, including four antifibrinolytic agents and twenty-seven human gene products that can regulate fibrinolysis. Thus, we were able to handle the information relating to a series of molecules, molecular-level biological phenomena, and diseases by integrating the refined JST thesaurus with information regarding chemicals and gene products from other resources.
</td></tr><tr><td>33</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-70682-5_1">Building Wikipedia Ontology with More Semi-structured Information Resources</a></td></tr><tr><td colspan=3>Wikipedia has been recently drawing attention as a semi-structured information resource for the automatic building of ontology. This paper describes a method of building general-purpose “lightweight ontology” by semi-automatically extracting the Is-a relation (rdfs:subClassOf), class-instance relation (rdf:type), concepts such as Triple, and a relation between concepts from information that includes category trees, define statements, lists and Wikipedia infoboxes. Also, we evaluate the built ontology by comparing it with other Wikipedia ontologies, such as YAGO and DBpedia.</td></tr><tr><td>34</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_26">Inference of Functions, Roles, and Applications of Chemicals Using Linked Open Data and Ontologies</a></td></tr><tr><td colspan=3>A simple method to efficiently collect reliable chemical information was studied for developing an ontological foundation. Even ChEBI, a major chemical ontology, which consists of approximately 90,000 chemicals and information about 1,000 biological and chemical roles, and applications, lacks information regarding the roles of most of the chemicals. NikkajiRDF, linked open data which provide information of approximately 3.5 million chemicals and 694 application examples, is also being developed. NikkajiRDF was integrated with Interlinking Ontology for Biological Concepts (IOBC), which includes 80,000 concepts, including information on a number of diseases and drugs. As a result, it was possible to infer new information on at least one of the 432 biological and chemical functions, applications and involvements with biological phenomena, including diseases to 5,038 chemicals using IOBC’s ontological structure. Furthermore, seven chemicals and drugs, which would be involved in 16 diseases, were discovered using knowledge graphs that were developed from IOBC.</td></tr><tr><td>35</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_24">A Conceptual Framework for Linking Open Government Data Based-On Geolocation: A Case of Thailand</a></td></tr><tr><td colspan=3>Over the past decade, most governments have steadily progressed towards a policy for more openness, more accountability and more transparency. Such a strategy to publish open data, which are meaningful and valuable, has made available open government data (OGD) that are publicly accessible to everyone. To promote OGD usage, most OGD datasets are published in a tabular form or a CSV spreadsheet format, which can be easily browsed and downloaded by a human user. However, applications of OGD often require data from different datasets to be integrated. This is a challenging and cumbersome task which usually demand huge human effort, especially if metadata as well as data representation and encoding standards are not well defined. With a thorough analysis into Thailand’s OGD (ThOGD) having over thousand datasets, we found that OGD datasets often involve data related to geolocation, places or administrative division. Therefore, using such geodata as potential linking nodes is very attractive. However, this is not an easy task due to data heterogeneity issues. For example, a location might be represented using a geographic coordination system (e.g., latitude and longitude) or an administrative division which could be in a different level from highest to lowest division such as regions, provinces, districts, municipalities, etc.). Moreover, in Thailand geographical regions can be divided differently by different division schemes depending on the application domains, e.g., meteorology, tourism and statistics. To tackle this challenge, in this paper, we propose a conceptual framework for mapping and linking OGD datasets using geolocation data which could increase OGD usage and promote the development of new services or applications.</td></tr><tr><td>36</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_25">Construction and Reuse of Linked Agriculture Data: An Experience of Taiwan Government Open Data</a></td></tr><tr><td colspan=3>This paper describes our experiences on dealing with the transformation from Traceable Agriculture Product (TAP) records to Linked Open Government Data. By using existing ontologies and vocabularies, TAP Ontology is developed for clarifying the semantics of TAP. To increase the reusability of TAP, the crops and operational processing details of TAP are mapped to Common Agricultural Vocabulary (CAVOC). There are four SPARQL endpoints developed for supporting queries to TAP. To demonstrate the reuse of Linked TAP, we develop a Chrome extension LinkedFood to offer TAP information via reading ingredients in recipe websites.</td></tr><tr><td>37</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_23">IMI: A Common Vocabulary Framework for Open Government Data</a></td></tr><tr><td colspan=3>Making meanings of terms used in government systems exchangeable is important to enhance semantic interoperability between systems and promote use of open government data. IMI is an interoperability framework for digital government and open government data in Japan. The IMI common vocabulary framework which is a part of IMI since 2013 aims to provide a mechanism for sharing meanings of terms and relationships between terms to enhance interoperability. This paper describes the current status of the IMI common vocabulary framework to share our experiences and knowledge from the development in five years. At first we illustrate the IMI common vocabulary and its core vocabulary which includes a basic set of terms used in data or referred from existing data to share meanings. Then we describe specifications of components in the common vocabulary framework like data exchange formats, notations and a package system. As the IMI common vocabulary has been already used in various areas, we also introduce its deployment support and real use cases of the common vocabulary framework.</td></tr><tr><td>38</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_22">Building the Core Vocabulary of Crop Names to Integrate the Vocabularies by Multiple Government Agencies</a></td></tr><tr><td colspan=3>Since agriculture is the oldest industry in our society and the basis of our life and economics, knowledge of crops as product of agriculture is also old and spread all over the society. As a result, the names of crops are messy and sometimes inconsistent. It is problematic in the digital and Internet era since interoperability is not assured. In this paper, we proposes Crop Vocabulary (CVO) as the core vocabulary of crop names to solve interoperability and machine-readability on crop names. There are many vocabularies about crops by various food chain stakeholders. Here we picked up three vocabularies issued by Japanese government with respect to food security, namely the Agricultural chemical use reference, the Agricultural chemical residue reference, and the Food composition database, since food security is the primary concern of all food chain stakeholders including farmers and consumers. As the result of comparative analysis of these vocabularies, we defined the concept of crop as the botanical information such as the scientific name of species with additional information such as edible parts, cultivation methods and usage. According to the definition, we investigated these three vocabularies and identified 1,249 crops with unique names. The element of CVO contains the information about the crop itself such as synonym, English name and scientific name as well as links to names in the above-mentioned vocabularies and other external vocabularies such as Wikipedia, AGROVOC and NCBI Taxonomy. We develop web-based API for CVO and an application for the farm management as an example. The value of CVO as a core vocabulary in the field of agriculture is identified by compatibility with existing vocabularies and its usefulness is demonstrated by web services applications developed based on CVO.</td></tr><tr><td>39</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_21">Publication of Statistical Linked Open Data in Japan</a></td></tr><tr><td colspan=3>The Japanese Statistics Center began publishing a statistical linked open data (LOD) site in 2016. The data currently consists of approximately 1.3 billion triples. The publication of statistical data as LOD enables datasets and categorizations to be clarified. This allows users not only to search objective data easily, but also to combine the data with other domestic or international data. This paper first introduces a design policy for LOD and a method for representing geographic areas. Then, it explains the method used to query the LOD by using SPARQL or GeoSPARQL, and provides one example application.</td></tr><tr><td>40</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_20">Making Complex Ontologies End User Accessible via Ontology Projections</a></td></tr><tr><td colspan=3>Ontologies are a powerful mechanism to structure domains of interest. They have successfully been applied in medical domain, industry and other important areas. Despite the simplicity of ontological vocabularies that consist of classes and properties, ontologies can relate elements of the vocabulary with the help of axioms in a very non-trivial way. Thus, the relationship between classes and properties can become hardly accessible by end users thus affecting the practical value of ontologies. Indeed, it is essential for end users to be able to navigate or browse through an ontology, to get a big picture of what classes there are and what they have in common in terms of other related classes and properties. This helps end users in effectively performing various knowledge engineering tasks such as querying and domain exploration. To this end, in this short paper, we describe an approach to project OWL 2 ontologies into graphs and show how to leverage this approach in practical systems for visual query formulation and faceted search that we tested in various scenarios.</td></tr><tr><td>41</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_19">Semantic Diagnostics of Smart Factories</a></td></tr><tr><td colspan=3>Smart factories are one of the biggest trends in modern manufacturing, also known as Industry 4.0. They reach a new level of process automation and make heavy use of sensors in manufactoring equipment, which brings new challenges to monitoring and diagnostics at smart factories. We propose to address the challenges with a novel rule-based monitoring and diagnostics language that relies on ontologies and reasoning and allows one to write diagnostic tasks at a high level of abstraction. We show that our approach speeds up the diagnostic routine of engineers at Siemens: they can formulate and deploy diagnostic tasks in factories faster than with existing Siemens data-driven solutions. Moreover we show that our diagnostic language, despite the built-in reasoning, allows for efficient execution of diagnostic tasks over large volumes of industrial data. Finally, we implemented our ideas in a prototypical diagnostic system for smart factories.</td></tr><tr><td>42</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_18">Automatic Ontology Development from Semi-structured Data in Web-Portal: Towards Ontology of Thai Rice Knowledge</a></td></tr><tr><td colspan=3>Heavyweight ontology is difficult to develop even for experienced ontology engineer, but it is required for semantic based computer software as core knowledge. Most of existing automated ontology development methods however focuses on lightweight ontology, taxonomy-instance extraction. This work presents a method to automatically construct relation-heavy ontology from semi-structured web content providing deep knowledge in specific domain. Classes, instances and hierarchical relation are derived from the category content from the web. Relations are extracted based on frequent expression details. Templates of relation and its range are extracted from common content with partial difference. Similar contexts are grouped with similarity and form as relation to attach to ontology classes. The case study of this work is Thai rice knowledge including rice variety, disease, weed and pest provided in website from responsible government. The complete ontology is used as core knowledge for personalised web service. The service assists in filter content in summary that matched to users’ information. Courtesy to the generated relation-heavy ontology, it is able to recommend relevant chained concepts to users based on semantic relation. From evaluation from an expert, the generated ontology obtained about 97% accuracy.</td></tr><tr><td>43</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_17">A Graph-Based Method for Interactive Mapping Revision</a></td></tr><tr><td colspan=3>Discovering semantic relations between heterogeneous ontologies is one of the key research topics in the Semantic Web. As the matching strategies adopted are largely heuristic, wrong mappings often exist in alignments generated by ontology matching systems. The mainstream methods for mapping revision deal with logical inconsistencies, so erroneous mappings not causing an inconsistency may be left out. Therefore, manual validations from domain experts are required. In this paper, we propose a graph-based method for interactive mapping revision with the purpose of reducing manual efforts as much as possible. Source ontologies are encoded into an integrated graph, where its mapping arcs are obtained by transforming mappings and will be evaluated by the expert. We specify the decision space for mapping revision and the corresponding operations that can be applied in the graph. After a human decision is made in each interaction, the mappings entailed by the manually confirmed ones are automatically approved. Conversely, those that would entail the rejected mappings or make the graph incoherent are declined. The whole update process modeled in the decision space can be done in polynomial time. Moreover, we define an impact function based on the integrated graph to identify the most influential mappings that will be displayed to the expert. In this way, the efforts of manual evaluation could be reduced further. The experiment on real-world ontology alignments shows that our method can save more decisions made by the expert than other revisions in most cases.</td></tr><tr><td>44</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_16">DeFind: A Protege Plugin for Computing Concept Definitions in </a></td></tr><tr><td colspan=3>We introduce an extension to the Protégé ontology editor, which allows for discovering concept definitions, which are not explicitly present in axioms, but are logically implied by an ontology. The plugin supports ontologies formulated in the Description Logic </td></tr><tr><td>45</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_15">A Quantitative Evaluation of Natural Language Question Interpretation for Question Answering Systems</a></td></tr><tr><td colspan=3>Systematic benchmark evaluation plays an important role in the process of improving technologies for Question Answering (QA) systems. While currently there are a number of existing evaluation methods for natural language (NL) QA systems, most of them consider only the final answers, limiting their utility within a black box style evaluation. Herein, we propose a subdivided evaluation approach to enable finer-grained evaluation of QA systems, and present an evaluation tool which targets the NL question (NLQ) interpretation step, an initial step of a QA pipeline. The results of experiments using two public benchmark datasets suggest that we can get a deeper insight about the performance of a QA system using the proposed approach, which should provide a better guidance for improving the systems, than using black box style approaches.</td></tr><tr><td>46</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_14">A Methodology for a Criminal Law and Procedure Ontology for Legal Question Answering</a></td></tr><tr><td colspan=3>The Internet and the development of the semantic web have created the opportunity to provide structured legal data on the web. However, most legal information is in text. It is difficult to automatically determine the right natural language answer about the law to a given natural language question. One approach is to develop systems of legal ontologies and rules. Our example ontology represents semantic information about USA criminal law and procedure as well as the applicable legal rules. The purpose of the ontology is to provide reasoning support to an legal question answering tool that determines entailment between a pair of texts, one known as the Background information (Bg) and the other Question statement (Q), whether Bg entails Q based on the application of the law. The key contribution of this paper is a clear and well-structured methodology that serves to develop such criminal law ontologies and rules (CLOR).</td></tr><tr><td>47</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_13">Leveraging Part-of-Speech Tagging for Sentiment Analysis in Short Texts and Regular Texts</a></td></tr><tr><td colspan=3>Sentiment analysis has been approached from a spectrum of methodologies, including statistical learning using labelled corpus and rule-based approach where rules may be constructed based on the observations on the lexicons as well as the output from natural language processing tools. In this paper, the experiments to transform labelled datasets by using NLP tools and subsequently performing sentiment analysis via statistical learning algorithms are detailed. In addition to the common data pre-processing prior to sentiment analysis, we represent the tokens in the datasets using Part-Of-Speech (POS) tags. The aim of the experiments is to investigate the impact of POS tags on sentiment analysis, particularly on both short texts and regular texts. The experimental results on short texts show that the combination of adjective and adverb predicts the sentiment of short texts the best. While noun is generally deemed to be neutral in sentiment polarity, the experimental results show that it helps to increase the accuracy of sentiment analysis on regular texts. Besides, the role of negation analysis in the datasets has also been investigated and reported based on the experimental results obtained.</td></tr><tr><td>48</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_9">EmbNum: Semantic Labeling for Numerical Values with Deep Metric Learning</a></td></tr><tr><td colspan=3>Semantic labeling for numerical values is a task of assigning semantic labels to unknown numerical attributes. The semantic labels could be numerical properties in ontologies, instances in knowledge bases, or labeled data that are manually annotated by domain experts. In this paper, we refer to semantic labeling as a retrieval setting where the label of an unknown attribute is assigned by the label of the most relevant attribute in labeled data. One of the greatest challenges is that an unknown attribute rarely has the same set of values with the similar one in the labeled data. To overcome the issue, statistical interpretation of value distribution is taken into account. However, the existing studies assume a specific form of distribution. It is not appropriate in particular to apply open data where there is no knowledge of data in advance. To address these problems, we propose a neural numerical embedding model (</td></tr><tr><td>49</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_12">A Marker Passing Approach to Winograd Schemas</a></td></tr><tr><td colspan=3>This paper approaches a solution of Winograd Schemas with a marker passing algorithm which operates on an automatically generated semantic graph. The semantic graph contains common sense facts from data sources form the semantic web like domain ontologies e.g. from Linked Open Data (LOD), WordNet, Wikidata, and ConceptNet. Out of those facts, a semantic decomposition algorithm selects relevant facts for the concepts used in the Winograd Schema and adds them to the semantic graph. Markers are propagated through the graph and used to identify an answer to the Winograd Schema. Depending on the encoded knowledge in the graph (connectionist view of world knowledge) and the information encoded on the marker (for symbolic reasoning) our approach selects the answers. With this selection, the marker passing approach is able to beat the state-of-the-art approach by about 12%.</td></tr><tr><td>50</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_11">Predicate Invention Based RDF Data Compression</a></td></tr><tr><td colspan=3>RDF is a data representation format for schema-free structured information that is gaining speed in the context of semantic web, life science, and vice versa. With the continuing proliferation of structured data, demand for RDF compression is becoming increasingly important. In this study, we introduce a novel lossless compression technique for RDF datasets (triples), called PIC (Predicate Invention based Compression). By generating informative predicates and constructing effective mapping to original predicates, PIC only needs to store dramatically reduced number of triples with the newly created predicates, and restoring the original triples efficiently using the mapping. These predicates are automatically generated by a decomposable forward-backward procedure, which consequently supports very fast parallel bit computation. As a semantic compression method for structured data, besides the reduction of syntactic verbosity and data redundancy, we also invoke semantics in the RDF datasets. Experiments on various datasets show competitive results in terms of compression ratio.</td></tr><tr><td>51</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_10">SPARQ</a></td></tr><tr><td colspan=3>With more and more applications providing semantic data to improve interoperability, the amount of available RDF datasets is constantly increasing. The SPARQL query language is a W3C recommendation to provide query capabilities on such RDF datasets. Data integration from different RDF sources is up to now mostly task of RDF consuming clients. However, from a functional perspective, data integration boils down to a function application that consumes input data as parameters, and based on these, produces a new set of data as output. Following this notion, we introduce SPARQ</td></tr><tr><td>52</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_8">Unified Access to Heterogeneous Data Sources Using an Ontology</a></td></tr><tr><td colspan=3>The rise of cloud computing started a transition for software applications from local to remote infrastructures. This migration created an opportunity to aggregate and consolidate analogous data content. However, this data content usually come with very different data structures and data terminologies and is usually tightly coupled to one or more applications. With these disparities and restrictions, the analogous data ends up both centrally stored but spread over several disconnected heterogeneous data sources. In this article, we present an approach to aggregate data sources using live data consolidation. The approach preserves the original data sources; and by doing so, prevents associated applications from having to migrate to a new data source. The approach uses an ontology at its core to serve as a common semantic ground between data sources and leverage its stored knowledge to expand query capabilities.</td></tr><tr><td>53</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_7">Ontology-Based Semantic Representation of Silk Road’s Caravanserais: Conceptualization of Multifaceted Links</a></td></tr><tr><td colspan=3>Knowledge representation and reasoning has gained relevance during the last years to improve historic architecture understanding and comparisons by developing innovative systems. This article presents research results about semantic representation of a sub set of Silk Road heritages, caravanserai. The core of the information system is an ontology-based schema to capture general and domain-based features of caravanserai by conceptualizing multifaceted links. Lexical links which are mapped from upper level sources are defined to give meaning, quotation and derivation to terms. Upper level links are proposed to give parent-child relations, part-whole relations or associative relations to building components or divisions represented as entities in terminology schema. The major contribution of the research is to conceptualize domain based links for architectural heritage. After studying different thesauruses or standards related to architectural classification or spatial reasoning, three schemas were defined as construction, services and spatial configuration. They acquire qualitative relations between building elements or divisions of a selected corpus of caravanserais. The paper concludes with technical and domain-based assessment of the ontology by publishing the ontology online in Web protégé and using the knowledge to classify 140 cases of the corpus of desert on route caravanserais of Safavid Period. Future work is to publish the RDF ontology as Linked Data.</td></tr><tr><td>54</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_6">On Enhancing Visual Query Building over KGs Using Query Logs</a></td></tr><tr><td colspan=3>Knowledge Graphs have recently gained a lot of attention and have been successfully applied in both academia and industry. Since KGs may be very large: they may contain millions of entities and triples relating them to each other, to classes, and assigning them data values, it is important to provide endusers with effective tools to explore information encapsulated in KGs. In this work we present a visual query system that allows users to explore KGs by intuitively constructing tree-shaped conjunctive queries. It is known that systems of this kind suffer from the problem of information overflow: when constructing a query the users have to iteratively choose from a potentially very long list of options, sich as, entities, classes, and data values, where each such choice corresponds to an extension of the query new filters. In order to address this problem we propose an approach to substantially reduce such lists with the help of ranking and by eliminating the so-called deadends, options that yield queries with no answers over a given KG.</td></tr><tr><td>55</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_5">Incorporating Text into the Triple Context for Knowledge Graph Embedding</a></td></tr><tr><td colspan=3>Knowledge graph embedding, aiming to represent entities and relations in a knowledge graph as low-dimensional real-value vectors, has attracted the attention of a large number of researchers. However, most of the embedding methods ignore the incompleteness of the knowledge graphs and they focus on the triples themselves in the knowledge graphs. In this paper, we try to introduce the information of texts to enhance the performances based on contextual model for knowledge graph embedding. Based on the assumption of the distant supervision, the sentences in texts contains abundant semantic information of the triples in knowledge graph, so that these semantic information can be utilized to relief the incompleteness of knowledge graphs and enhance the performances of knowledge graph embedding. Compared with state-of-the-art systems, preliminary evaluation results show that our proposed method obtains the better results in Hits@10.</td></tr><tr><td>56</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_2">More Is Better: Sequential Combinations of Knowledge Graph Embedding Approaches</a></td></tr><tr><td colspan=3>Constructing and maintaining large-scale good quality knowledge graphs present many challenges. Knowledge graph completion has been regarded a promising direction in the knowledge graph community. The majority of current work for knowledge graph completion approaches do not take the schema of a target knowledge graph as input. As a result, the triples generated by these approaches are not necessarily consistent with the schema of the target knowledge graph. This paper proposes to improve the correctness of knowledge graph completion based on Schema Aware Triple Classification (SATC), which enables sequential combinations of knowledge graph embedding approaches. Extensive experiments show that our proposed approaches can significantly improve the correctness of the new triples produced by knowledge graph embedding methods.</td></tr><tr><td>57</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_3">Knowledge Graph-Based Core Concept Identification in Learning Resources</a></td></tr><tr><td colspan=3>The automatic identification of core concepts addressed by a learning resource is an important task in favor of organizing content for educational purposes and for the next generation of learner support systems. We present a set of strategies for core concept identification on the basis of a semantic representation built using the open and available knowledge in the so-called Knowledge Graphs (KGs). Different unsupervised weighting strategies, as well as a supervised method that operates on the semantic representation, were implemented for core concept identification. In order to test the effectiveness of the proposed strategies, a human-expert annotated dataset of 96 learning resources extracted from MOOCs was built. In our experiments, we show the capacity of the semantic representation for the core-concept identification task as well as the superiority of the supervised method.</td></tr><tr><td>58</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_4">Ranking Diagnoses for Inconsistent Knowledge Graphs by Representation Learning</a></td></tr><tr><td colspan=3>When a knowledge graph (KG) is growing e.g. by knowledge graph completion, it might become inconsistent with the logical theory which formalizing the schema of the KG. A common approach to restoring consistency is removing a minimal set of triples from the KG, called a diagnosis of the KB. However, there can be a large number of diagnoses. It is hard to manually select the best one among these diagnoses to restore consistency. To alleviate the selection burden, this paper studies automatic methods for ranking diagnoses so that people can merely focus on top diagnoses when seeking the best one. An approach to ranking diagnoses through representation learning aka knowledge graph embedding is proposed. Given a set of diagnoses, the approach first learns the embedding of the complement set of the union of all diagnoses, then for every diagnosis, incrementally learns an embedding of the complement set of the diagnosis and employs the embedding to estimate the removal cost of the diagnosis, and finally ranks diagnoses by removal costs. To evaluate the approach, four knowledge graphs with logical theories are constructed from the four great classical masterpieces of Chinese literature. Experimental results on these datasets show that the proposed approach is significantly more effective than classical random methods in ranking the best diagnoses at top places.</td></tr><tr><td>59</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-04284-4_1">Knowledge Driven Intelligent Survey Systems for Linguists</a></td></tr><tr><td colspan=3>In this paper, we propose Knowledge Graph (KG), an articulated underlying semantic structure, to be a semantic bridge between human and systems. To illustrate our proposal, we focus on KG based intelligent survey systems. In state of the art systems, knowledge is hard-coded or implicit in these systems, making it hard for researchers to reuse, customise, link, or transmit the structured knowledge. Furthermore, such systems do not facilitate dynamic interaction based on the semantic structure. We design and implement a knowledge-driven intelligent survey system which is based on knowledge graph, a widely used technology that facilitates sharing and querying hypotheses, survey content, results, and analyses. The approach is developed, implemented, and tested in the field of Linguistics. Syntacticians and morphologists develop theories of grammar of natural languages. To evaluate theories, they seek intuitive grammaticality (well-formedness) judgments from native speakers, which either support a theory or provide counter-evidence. Our preliminary experiments show that a knowledge graph based linguistic survey can provide more nuanced results than traditional document-based grammaticality judgment surveys by allowing for tagging and manipulation of specific linguistic variables.</td></tr><tr><td>60</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_22">Towards an Enterprise Entity Hub: Integration of General and Enterprise Knowledge</a></td></tr><tr><td colspan=3>Today’s enterprises possess vast quantities of data generated by their own applications and services across different organisational systems. However, the data tends to be stored in many data-silos with redundant, duplicated, and outdated information. This means that finding the right information and obtaining valuable insights is difficult. To address this problem, we develop the enterprise entity hub that enables users to search, analyse, share, and filter information across large-scale data sources. One of its core components, the base knowledge is used to organise and interlink the organisation’s knowledge model, we then construct a base knowledge through the integration of several data sources, and expand this knowledge by interlinking a set of enterprise content.</td></tr><tr><td>61</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_20">Distance-Based Ranking of Negative Answers</a></td></tr><tr><td colspan=3>Suggesting negative answers to users is a solution to the problem of insufficiently many answers in conjunctive query answering over description logic (DL) ontologies. Negative answers are complementary to certain answers and have explanations in the given ontology. Before being suggested to users, negative answers need to be ranked. However, there is no method for ranking negative answers by now. To fill this gap, this paper studies ranking methods that require only information on the given query and the given ontology. Three distance-based methods are proposed. Experimental results demonstrate that these methods can effectively rank negative answers to those conjunctive queries that have certain answers in the given DL ontology.</td></tr><tr><td>62</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_18">PROSE: A Plugin-Based Paraconsistent OWL Reasoner</a></td></tr><tr><td colspan=3>The study of paraconsistent reasoning with ontologies is especially important for the Semantic Web since knowledge is not always perfect within it. Quasi-classical semantics is proven to rationally draw more meaningful conclusions even from an inconsistent ontology with the stronger inference power of paraconsistent reasoning. In our previous work, we have conceived a quasi-classical framework called </td></tr><tr><td>63</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_21">Contrasting RDF Stream Processing Semantics</a></td></tr><tr><td colspan=3>The increasing popularity of RDF Stream Processing (RSP) has led to developments of data models and processing engines which diverge in several aspects, ranging from the representation of RDF streams to semantics. Benchmarking systems such as LSBench, SRBench, CSRBench, and YABench were introduced as attempts to compare different approaches, focusing mainly on the operational aspects. The recent logic-based LARS framework provides a theoretical underpinning to analyze stream processing/reasoning semantics. In this work, we use LARS to compare the semantics of two typical RSP engines, namely C-SPARQL and CQELS, identify conditions when they agree on the output, and discuss situations where they disagree. The findings give insights that might prove to be useful for the RSP community in developing a common core for RSP.</td></tr><tr><td>64</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_17">Answer Type Identification for Question Answering</a></td></tr><tr><td colspan=3>Question Answering research has long recognised that the identification of the type of answer being requested is a fundamental step in the interpretation of a question as a whole. Previous strategies have ranged from trivial keyword matches, to statistical analyses, to well-defined algorithms based on shallow syntactic parses with user-interaction for ambiguity resolution. A novel strategy combining deep NLP on both syntactic and dependency parses with supervised learning is introduced and results that improve on extant alternatives reported. The impact of the strategy on QALD is also evaluated with a proprietary Question Answering system and its positive results analysed.</td></tr><tr><td>65</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_15">A Contrastive Study on Semantic Prosodies of Minimal Degree Adverbs in Chinese and English</a></td></tr><tr><td colspan=3>From the perspective of cross-language, this paper, by using Chinese and English corpus, examines the semantic prosody similarities and differences between </td></tr><tr><td>66</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_16">A Graph Traversal Based Approach to Answer Non-Aggregation Questions over DBpedia</a></td></tr><tr><td colspan=3>We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems.</td></tr><tr><td>67</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_13">Computing the Semantic Similarity of Resources in DBpedia for Recommendation Purposes</a></td></tr><tr><td colspan=3>The Linked Open Data cloud has been increasing in popularity, with DBpedia as a first-class citizen in this cloud that has been widely adopted across many applications. Measuring similarity between resources and identifying their relatedness could be used for various applications such as item-based recommender systems. To this end, several similarity measures such as </td></tr><tr><td>68</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_19">Meta-Level Properties for Reasoning on Dynamic Data</a></td></tr><tr><td colspan=3>Dynamic features are important for data processing when dealing with real applications. In this paper we introduce a methodology for validating the construction of ontological knowledge base and optimising the query answering with such ontologies. In this paper, we firstly introduce some meta-properties of dynamic for ontologies. These meta-properties impose several constraints on the taxonomic structure of an ontology. We then investigate how to build up a meta-ontology with the constrains on these meta-properties. The goal of our methodology is </td></tr><tr><td>69</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_14">Identifying an Agent’s Preferences Toward Similarity Measures in Description Logics</a></td></tr><tr><td colspan=3>In Description Logics (DLs), concept similarity measures (CSMs) aim at identifying a degree of commonality between two given concepts and are often regarded as a generalization of the classical reasoning problem of equivalence. That is, any two concepts are equivalent if their similarity degree is one, and vice versa. When two concepts are not quite equivalent but similar, nevertheless, a problem may arise as to which aspects of commonality should play more important role than others. This work presents the so-called </td></tr><tr><td>70</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_10">Improving Knowledge Base Completion by Incorporating Implicit Information</a></td></tr><tr><td colspan=3>Over the past few years, many large Knowledge Bases (KBs) have been constructed through relation extraction technology but they are still often incomplete. As a supplement to training a more powerful extractor, Knowledge Base Completion which aims at learning new facts based on existing ones has recently attracted much attention. Most of the existing methods, however, are only utilizing the explicit facts in a single KB. By analyzing the data, we find that some implicit information should also been captured for a more comprehensive consideration during completion process. These information include the intrinsic properties of KBs (e.g. relational constraints) and potential synergies between various KBs (i.e. semantic similarity). For the former, we distinguish the missing data by using relational constraints to reduce the data sparsity. For the later, we incorporate two semantical regularizations into the learning model to encode the semantic similarity. Experimental results show that our approach is better than the methods that consider only explicit facts or only a single knowledge base, and achieves significant accuracy improvements in binary relation prediction.</td></tr><tr><td>71</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_9">Leveraging Chinese Encyclopedia for Weakly Supervised Relation Extraction</a></td></tr><tr><td colspan=3>In the research of named-entity relation extraction based on supervision, selecting relation features for traditional methods are usually finished by people, and it’s hard to implement these methods for large-scale corpus. On the other hand, fixing relation types is the premise, so the practicabilities of these methods are not so ideal. This paper presents a weakly supervised method for Chinese named-entity relation extraction without man-made annotations, and the relation types in this method are not chosen artificially. The method collects entity relation types from the structured knowledge in encyclopedia pages, and then automatically annotates the relation instances existing in the texts based on these relation types. Simultaneously, the syntactic and semantic features of entity relations will be considered in this method, then the machine learning data will be completed, finally we use Support Vector Machine (SVM) model to train relation classifiers from training data, and these classifiers could try to extract entity relations from testing data. We carry out the experiment with the data from Chinese Baidu Encyclopedia pages, and the results show the effectiveness of this method, the overall F1 value reaches to 83.12 %. In order to probe the universality of this method, we also use the acquired relation classifiers to extract entity relations from news texts, and the results manifest that this method owns certain universality.</td></tr><tr><td>72</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_6">ERA-RJN: A SPARQL-Rank Based Top-k Join Query Optimization</a></td></tr><tr><td colspan=3>With the wide use of RDF data, searching and ranking semantic data with SPARQL has become a research hot-spot. While there is no much research work on top-k join queries optimization in RDF native stores. This paper proposes a new rank-join operator algorithm ERA-RJN on the basis of SPARQL-RANK algebra, making use of the advantage of random access availability in RDF native storage. This paper implements the ERA-RJN operator on the ARQ-RANK platform, and performs experiments, verifies the high efficiency of ERA-RJN algorithm dealing with SPARQL top-k join query in RDF native storage</td></tr><tr><td>73</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_7">CNME: A System for Chinese News Meta-Data Extraction</a></td></tr><tr><td colspan=3>News mining has gained increasing attention because of the overwhelming news produced everyday. Lots of news portals such as Sina (</td></tr><tr><td>74</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_11">Automatic Generation of Semantic Data for Event-Related Medical Guidelines</a></td></tr><tr><td colspan=3>Medical Guidelines pay an important role in medical decision making systems. Medical guidelines are usually involved with event-related actions or procedure. However, little research has been done on how event-related medical guidelines can be converted into its semantic representation such as RDF/OWL data. This paper proposes an approach of automatic generation of semantic data for event-related medical guidelines. This generation is achieved by using the logic programming language Prolog with the support of medical ontologies such as SNOMED CT. We will report the experiments with the automatic generation of the semantic data for event-related Chinese medical guidelines, and show the relevant results.
</td></tr><tr><td>75</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_5">Alignment Aware Linked Data Compression</a></td></tr><tr><td colspan=3>The success of linked data has resulted in a large amount of data being generated in a standard RDF format. Various techniques have been explored to generate a compressed version of RDF datasets for archival and transmission purpose. However, these compression techniques are designed to compress a given dataset without using any external knowledge, either through a compact representation or removal of semantic redundancies present in the dataset. In this paper, we introduce a novel approach to compress RDF datasets by exploiting alignments present across various datasets at both instance and schema level. Our system generates lossy compression based on the confidence value of relation between the terms. We also present a comprehensive evaluation of the approach by using reference alignment from OAEI.</td></tr><tr><td>76</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_12">Evaluating and Comparing Web-Scale Extracted Knowledge Bases in Chinese and English</a></td></tr><tr><td colspan=3>DBpedia and YAGO are the two main data sources serving as the hub of Linking Open Data (LOD), and they both contain Chinese data. Zhishi.me and SSCO extract Chinese knowledge from Wikipedia and other Chinese Encyclopedic Web sites like Baidu-Baike and Hudong-Baike. The quality of these Knowledge Bases (KBs) are not well investigated while their qualities are key to smart applications. In this paper, we evaluate three large Chinese KBs including DBpedia Chinese, zhishi.me and SSCO, and further compare them with English KBs. Since traditional methods on evaluating Web ontology can not be easily adapted to web-scale extracted KBs, we design two metric sets considering Richness and Correctness based on a quasi-formal conceptual representation to measure and compare these KBs. We also design a novel metric set on overlapped instances of different KBs to make the metric results comparable. Finally, we employ random sampling to reduce human efforts for assessing the correctness. The findings in these KBs give a detailed status report of the current situation of extracted KBs in both Chinese and English.</td></tr><tr><td>77</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_8">Bootstrapping Yahoo! Finance by Wikipedia for Competitor Mining</a></td></tr><tr><td colspan=3>Competitive intelligence, one of the key factors of enterprise risk management and decision support, depends on knowledge bases that contain a large amount of competitive information. A variety of finance websites have collected competitive information manually, which can be used as knowledge bases. Yahoo! Finance is one of the largest and most successful finance websites among them. However, they have problems of incompleteness, lack of competitive domain, and not-in-time updating. Wikipedia, which was built with collective wisdom and contains plenty of useful information in various forms, can solve the above-mentioned problems effectively, thus helping build a more comprehensive knowledge base. In this paper, we propose a novel semi-supervised approach to identify competitor information and competitive domain from Wikipedia based on a multi-strategy learning algorithm. More precisely, we leverage seeds of competition between companies and competition between products to distantly supervise the learning process to find text patterns in free texts. Considering that competitive information can be inferred from events, we design a learning-based method to determine event description sentences. The whole process is iteratively performed. The experimental results show the effectiveness of our approach. Moreover, the results extracted from Wikipedia supplement 14,000 competitor pairs and 8,000 competitive domains between rival companies to Yahoo! Finance.</td></tr><tr><td>78</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_4">Heuristic-Based Configuration Learning for Linked Data Instance Matching</a></td></tr><tr><td colspan=3>Instance matching in linked data has become increasingly important because of the rapid development of linked data. The goal of instance matching is to detect co-referent instances that refer to the same real-world objects. In order to realize such instances, instance matching systems use a configuration, which specifies the matching properties, similarity measures, and other settings of the matching process. For different repositories, the configuration is varied to adapt with the particular characteristics of the input. Therefore, the automation of configuration creation is very important. In this paper, we propose </td></tr><tr><td>79</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_3">Linked Open Vocabulary Recommendation Based on Ranking and Linked Open Data</a></td></tr><tr><td colspan=3>The vocabulary space of the Semantic Web includes more than 500 vocabularies according to the Linked Open Vocabularies (LOV) initiative that maintains the directory list and provides search functionality on top of the curated data. Domain experts and researchers have populated it to facilitate the interpretation and exchange of information in the Web of Data. The abundance of vocabularies and terms available in the LOV space, on one hand aims to cover the major knowledge management needs, but on the other hand it could be cumbersome for a non-expert or even a vocabulary expert to find the correct way through the collection. To address this problem, we present an approach that helps to identify the most appropriate set of LOV vocabulary terms for a given Web content context by leveraging the existing dynamics within the LOV graph and the usage patterns in the LOD cloud. The paper describes the framework architecture that enables the discovery of vocabularies; it focuses on the corresponding metrics and algorithm, and discusses the outcomes of the applied experiments.</td></tr><tr><td>80</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_1">Modeling and Querying Spatial Data Warehouses on the Semantic Web</a></td></tr><tr><td colspan=3>The Semantic Web (SW) has drawn the attention of data enthusiasts, and also inspired the exploitation and design of multidimensional data warehouses, in an unconventional way. Traditional data warehouses (DW) operate over static data. However multidimensional (MD) data modeling approach can be dynamically extended by defining both the schema and instances of MD data as RDF graphs. The importance and applicability of MD data warehouses over RDF is widely studied yet none of the works support a spatially enhanced MD model on the SW. Spatial support in DWs is a desirable feature for enhanced analysis, since adding encoded spatial information of the data allows to query with spatial functions. In this paper we propose to empower the spatial dimension of data warehouses by adding spatial data types and topological relationships to the existing QB4OLAP vocabulary, which already supports the representation of the constructs of the MD models in RDF. With QB4SOLAP, spatial constructs of the MD models can be also published in RDF, which allows to implement spatial and metric analysis on spatial members along with OLAP operations. In our contribution, we describe a set of spatial OLAP (SOLAP) operations, demonstrate a spatially extended metamodel as, QB4SOLAP, and apply it on a use case scenario. Finally, we show how these SOLAP queries can be expressed in SPARQL.</td></tr><tr><td>81</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_21">Hypercat RDF: Semantic Enrichment for IoT</a></td></tr><tr><td colspan=3>The rapid growth of sensor networks and smart devices has led to the generation of an increasing amount of information. Such information typically originates from various sources and is published in different formats. One of the key prerequisites for the Internet of Things (IoT) is interoperability. The Hypercat specification defines a lightweight JSON-based hypermedia catalogue, and is tailored towards the existing needs of industry. In this work, we propose a semantic enrichment of Hypercat, defining an RDF-based catalogue. We propose an ontology that captures the core of the Hypercat RDF specification and provides a mapping mechanism between existing JSON and proposed RDF properties. Finally, we propose a new type of search, called </td></tr><tr><td>82</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-31676-5_2">RDF Graph Visualization by Interpreting Linked Data as Knowledge</a></td></tr><tr><td colspan=3>It is known that Semantic Web and Linked Open Data (LOD) are powerful technologies for knowledge management, and explicit knowledge is expected to be presented by RDF format (Resource Description Framework), but normal users are far from RDF due to technical skills required. As we learn, a concept-map or a node-link diagram can enhance the learning ability of learners from beginner to advanced user level, so RDF graph visualization can be a suitable tool for making users be familiar with Semantic technology. However, an RDF graph generated from the whole query result is not suitable for reading, because it is highly connected like a hairball and less organized. To make a graph presenting knowledge be more proper to read, this research introduces an approach to sparsify a graph using the combination of three main functions: graph simplification, triple ranking, and property selection. These functions are mostly initiated based on the interpretation of RDF data as knowledge units together with statistical analysis in order to deliver an easily-readable graph to users. A prototype is implemented to demonstrate the suitability and feasibility of the approach. It shows that the simple and flexible graph visualization is easy to read, and it creates the impression of users. In addition, the attractive tool helps to inspire users to realize the advantageous role of linked data in knowledge management.
</td></tr><tr><td>83</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_18">Entity Linking in Web Tables with Multiple Linked Knowledge Bases</a></td></tr><tr><td colspan=3>The World-Wide Web contains a large scale of valuable relational data, which are embedded in HTML tables (i.e. Web tables). To extract machine-readable knowledge from Web tables, some work tries to annotate the contents of Web tables as RDF triples. One critical step of the annotation is entity linking (EL), which aims to map the string mentions in table cells to their referent entities in a knowledge base (KB). In this paper, we present a new approach for EL in Web tables. Different from previous work, the proposed approach replaces a single KB with multiple linked KBs as the sources of entities to improve the quality of EL. In our approach, we first apply a general graph-based algorithm to EL in Web tables with each single KB. Then, we leverage the existing and newly learned “</td></tr><tr><td>84</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_20">Linking Named Entity in a Question with DBpedia Knowledge Base</a></td></tr><tr><td colspan=3>The emerging Linked Open Data provides an opportunity to answer the natural language question based on knowledge bases (KB). One challenge of the question answering (QA) problem is to link the entity mention in the question with the entity in the existing knowledge base. This study proposes an approach to link entity mention with a DBpedia entity. We propose an entity-centric indexing model to help search candidate entities in KB. After obtaining the candidate entities, we expand the context of the entity mention with WordNet and ConceptNet, we compute the context similarity between the expanded context and the property value of the candidate entity and the popularity of the candidate entity. Finally, we rerank the candidate entities by leveraging these features. Evaluations are performed on DBpedia version 2015, the evaluation tests show that our approach is promising in dealing with linking named entity in DBpedia.</td></tr><tr><td>85</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_15">Data Analysis of Hierarchical Data for RDF Term Identification</a></td></tr><tr><td colspan=3>Generating Linked Data based on existing data sources requires the modeling of their information structure. This modeling needs the identification of potential entities, their attributes and the relationships between them and among entities. For databases this identification is not required, because a data schema is always available. However, for other data formats, such as hierarchical data, this is not always the case. Therefore, analysis of the data is required to support </td></tr><tr><td>86</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_17">Non-hierarchical Relation Extraction of Chinese Text Based on Scalable Corpus</a></td></tr><tr><td colspan=3>As for ontology construction from Chinese text, the non-hierarchical relation extraction is harder than the concept extraction and its extraction effect is still not satisfactory. In this paper, we put forward a scalable corpus model, which uses Tongyici Cilin and word2vec to calculate terms’ similarity and add the qualified candidate terms to the corpora. In this way we can expand the scalable corpus while extracting non-hierarchical relations. In turn, the scalable corpus that has been expanded with the new terms will facilitate the non-hierarchical relation extraction further. We carry out the experiment with Chinese texts in the domain of Computer, whose results show that with expansion of the corpus, the extraction effect will be better and better.</td></tr><tr><td>87</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_14">Towards Answering Provenance-Enabled SPARQL Queries Over RDF Data Cubes</a></td></tr><tr><td colspan=3>The SPARQL 1.1 standard has made it possible to formulate analytical queries in SPARQL. While some approaches have become available for processing analytical queries on RDF data cubes, little attention has been paid to answering provenance-enabled queries over such data. Yet, considering provenance is a prerequisite to being able to validate if a query result is trustworthy. The main challenge for existing triple stores is the way provenance can be encoded in standard triple stores based on context values (named graphs). Hence, in this paper we analyze the suitability of existing triple stores for answering provenance-enabled queries on RDF data cubes, identify their shortcomings, and propose an index to handle the high number of context values that provenance encoding typically entails. Our experimental results using the Star Schema Benchmark show the feasibility and scalability of our index and query evaluation strategies.</td></tr><tr><td>88</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_16">PIWD: A Plugin-Based Framework for Well-Designed SPARQL</a></td></tr><tr><td colspan=3>In the real world datasets (e.g., DBpedia query log), queries built on well-designed patterns containing only AND and OPT operators (for short, WDAO-patterns) account for a large proportion among all SPARQL queries. In this paper, we present a plugin-based framework for all SELECT queries built on WDAO-patterns, named PIWD. The framework is based on a parse tree called </td></tr><tr><td>89</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_13">ASPG: Generating OLAP Queries for SPARQL Benchmarking</a></td></tr><tr><td colspan=3>The increasing use of data analytics on Linked Data leads to the requirement for SPARQL engines to efficiently execute Online Analytical Processing (OLAP) queries. While SPARQL 1.1 provides basic constructs, further development on optimising OLAP queries lacks benchmarks that mimic the data distributions found in Link Data. Existing work on OLAP benchmarking for SPARQL has usually adopted queries and data from relational databases, which may not well represent Linked Data. We propose an approach that maps typical OLAP operations to SPARQL and a tool named ASPG to automatically generate OLAP queries from real-world Linked Data. We evaluate ASPG by constructing a benchmark called DBOBfrom the online DBpedia endpoint, and use DBOB to measure the performance of the Virtuoso engine.</td></tr><tr><td>90</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_12">Estimation of Spatio-Temporal Missing Data for Expanding Urban LOD</a></td></tr><tr><td colspan=3>The illegal parking of bicycles has been an urban problem in Tokyo and other urban areas. We have sustainably built a Linked Open Data (LOD) relating to the illegal parking of bicycles (IPBLOD) to support the problem solving by raising social awareness. Also, we have estimated and complemented the temporally missing data to enrich the IPBLOD, which consisted of intermittent social-sensor data. However, there are also spatial missing data where a bicycle might be illegally parked, and it is necessary to estimate those data in order to expand the areas. Thus, we propose and evaluate a method for estimating spatially missing data. Specifically, we find stagnation points using computational fluid dynamics (CFD), and we filter the stagnation points based on popularity stakes that are calculated using Linked Data. As a result, a significant difference in between the baseline and our approach was represented using the chi-square test.</td></tr><tr><td>91</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_9">A Preliminary Investigation Towards Improving Linked Data Quality Using Distance-Based Outlier Detection</a></td></tr><tr><td colspan=3>With more and more data being published on the Web as Linked Data, Web Data quality is becoming increasingly important. While quite some work has been done with regard to quality assessment of Linked Data, only few works have addressed quality improvement. In this article, we present a preliminary an approach for identifying potentially incorrect RDF statements using distance-based outlier detection. Our method follows a three stage approach, which automates the whole process of finding potentially incorrect statements for a certain property. Our preliminary evaluation shows that a high precision is maintained with different settings.</td></tr><tr><td>92</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_19">Towards Multi-target Search of Semantic Association</a></td></tr><tr><td colspan=3>Semantic association represents group relationship among objects in linked data. Searching semantic associations is complicated, which involves the search of multiple objects and the search of their group relationships simultaneously. In this paper, we propose this kind of search as a multi-target search, and we compare it to traditional search tasks, which we classify as single-target search. A novel search model is introduced, and the notion of virtual document is used to extract linguistic information of semantic associations. Multi-target search is finally fulfilled by a PageRank-like ranking scheme and a top-</td></tr><tr><td>93</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_7">A MapReduce-Based Approach for Prefix-Based Labeling of Large XML Data</a></td></tr><tr><td colspan=3>A massive amount of XML (Extensible Markup Language) data is available on the web, which can be viewed as tree data. One of the fundamental building blocks of information retrieval from tree data is answering structural queries. Various labeling schemes have been suggested for rapid structural query processing. We focus on the prefix-based labeling scheme that labels each node with a concatenation of its parent’s label and its child order. This scheme has been adapted in RDF (Resource Description Framework) data management systems that index RDF data in tree by grouping subjects. Recently, a MapReduce-based algorithm for the prefix-based labeling scheme was suggested. We observe that this algorithm fails to keep label size minimized, which makes the prefix-based labeling scheme difficult for massive real-world XML datasets. To address this issue, we propose a MapReduce-based algorithm for prefix-based labeling of XML data that reduces label size by adjusting the order of label assignments based on the structural information of the XML data. Experiments with real-world XML datasets show that the proposed approach is more effective than previous works.</td></tr><tr><td>94</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_10">Linked Data Collection and Analysis Platform for Music Information Retrieval</a></td></tr><tr><td colspan=3>There has been extensive research on music information retrieval (MIR), such as signal processing, pattern mining, and information retrieval. In such studies, audio features extracted from music are commonly used, but there is no open platform for data collection and analysis of audio features. Therefore, we build the platform for the data collection and analysis for MIR research. On the platform, we represent the music data with Linked Data, which are in a format suitable for computer processing, and also link data fragments to each other. By adopting the Linked Data, the music data will become easier to publish and share, and there is an advantage that complex music analysis will be facilitated. In this paper, we first investigate the frequency of the audio features used in previous studies on MIR for designing the Linked Data schema. Then, we build a platform, that automatically extracts the audio features and music metadata from YouTube URIs designated by users, and adds them to our Linked Data DB. Finally, the sample queries for music analysis and the current record of music registrations in the DB are presented.</td></tr><tr><td>95</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_11">Semantic Data Acquisition by Traversing Class–Class Relationships Over Linked Open Data</a></td></tr><tr><td colspan=3>Linked Open Data (LOD), a powerful mechanism for linking different datasets published on the World Wide Web, is expected to increase the value of data through mashups of various datasets on the Web. One of the important requirements for LOD is to be able to find a path of resources connecting two given classes. Because each class contains many instances, inspecting all of the paths or combinations of the instances results in an explosive increase of computational complexity. To solve this problem, we have proposed an efficient method that obtains and prioritizes a comprehensive set of connections over resources by traversing class–class relationships of interest. To put our method into practice, we have been developing a tool for LOD exploration. In this paper, we introduce the technologies used in the tool, focusing especially on the development of a measure for predicting whether a path of class–class relationships has connected triples or not. Because paths without connected triples can be predicted and removed, using the prediction measure enables us to display more paths from which users can obtain data that interests them.</td></tr><tr><td>96</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_3">Designing of Ontology for Domain Vocabulary on Agriculture Activity Ontology (AAO) and a Lesson Learned</a></td></tr><tr><td colspan=3>This paper proposes Agriculture Activity Ontology (AAO) as a basis of the core vocabulary of agricultural activity. Since concepts of agriculture activities are formed by the various context such as purpose, means, crop, and field, we organize the agriculture activity ontology as a hierarchy of concepts discriminated by various properties such as purpose, means, crop and field. The vocabulary of agricultural activity is then defined as the subset of the ontology. Since the ontology is consistent, extendable, and capable of some inferences thanks to Description Logics, so the vocabulary inherits these features. The vocabulary is also linked to existing vocabularies such as AGROVOC. It is expected to use in the data format in the agricultural IT system. The vocabulary is adopted as the part of “the guideline for agriculture activity names for agriculture IT systems” issued by Ministry of Agriculture, Forestry and Fisheries (MAFF), Japan. Also we investigated the usefulness of the ontology as the method for defining the domain vocabulary.</td></tr><tr><td>97</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_8">RIKEN MetaDatabase: A Database Platform as a Microcosm of Linked Open Data Cloud in the Life Sciences</a></td></tr><tr><td colspan=3>The amount and heterogeneity of life-science datasets published on the Web have considerably increased recently. However, biomedical scientists face numerous serious difficulties in finding, using and publishing useful databases. In order to solve these issues, we developed a Resource Description Framework-based database platform, called RIKEN MetaDatabase, which allows biologists to easily develop, publish and integrate databases. The platform manages metadata of both research data and individual data described with standardised vocabularies and ontologies, and has a simple browser-based graphical user interface for viewing data including tabular and graphical views. The platform was released in April 2015, and 110 databases including mammalian, plant, bioresource and image databases with 21 ontologies have been published through this platform as of July 2016. This paper describes the technical knowledge obtained through the development and operation of RIKEN MetaDatabase as a challenge for accelerating life-science data distribution promotion.</td></tr><tr><td>98</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_6">An Empirical Study on Property Clustering in Linked Data</a></td></tr><tr><td colspan=3>Properties are used to describe entities, a part of which are likely to be clustered together to constitute an aspect. For example, </td></tr><tr><td>99</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_5">Compression Algorithms for Log-Based Recovery in Main-Memory Data Management</a></td></tr><tr><td colspan=3>
With the dramatic increases in performance requirement of computer hardware and decreases in its cost in recent years, the relevant research in main-memory database is becoming more and more popular and has a prosperous future. Log-based recovery, which is one of its most important research directions, is a set of problems accompanied by volatile memory. Its problem of stagnation in memory/CPU resulted from the slow I/O speed of non-volatile storage now needs to be addressed urgently. However, there is no specific platform for log-based recovery research. So the study aims to address this issue.
</td></tr><tr><td>100</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_4">SQuaRE: A Visual Approach for Ontology-Based Data Access</a></td></tr><tr><td colspan=3>We present the SPARQL Query and R2RML mappings Environment (SQuaRE) which provides a visual interface for creating mappings expressed in R2RML. SQuaRE is a web-based tool with easy to use interface that can be applied in the ontology-based data access applications. We describe SQuaRE’s main features, its architecture as well as implementation details. We compare SQuaRE with other similar tools and describe our future development plans.</td></tr><tr><td>101</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_29">A Semantic Keyword Search Based on the Bidirectional Fix Root Query Graph Construction Algorithm</a></td></tr><tr><td colspan=3>In the last few years, the large amount of personal information in RDF format is widely deployed. To access the semantic information, it needs a semantic formal query (e.g. SPARQL query). However, this kind of query requires users to know the ontology structure and master its syntax. This paper proposes the X-SKengine, the semantic keyword search engine for specific expert discovery domain. The X-SKengine transforms the user keywords to the SPARQL query using a bidirectional fix root query graph construction algorithm which is able to compute the query graphs without limitation of the directions of relationships in ontologies. The experiment was conducted to compare the capability of SPARQL query construction between X-SKengine and the previous version. The results show that X-SKengine can automatically construct SPARQL queries relevant to meaning of user keywords for various ontology structures.</td></tr><tr><td>102</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_30">An Ontology-Based Intelligent Speed Adaptation System for Autonomous Cars</a></td></tr><tr><td colspan=3> </td></tr><tr><td>103</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_2">Inquiry into RDF and OWL Semantics</a></td></tr><tr><td colspan=3>The purpose of this paper is to present the higher order formalization of RDF and OWL with setting up ontological meta-modeling criteria through the discussion of Russell’s </td></tr><tr><td>104</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_27">A Keyword Exploration for Retrieval from Biomimetics Databases</a></td></tr><tr><td colspan=3>Biomimetics contributes to innovative engineering by imitating the models, systems, and elements of nature. Biomimetics research requires the development of a biomimetics database including widely varied knowledge across different domains. Interoperability of knowledge among those domains is necessary to create such a database. Ontologies clarify concepts that appear in target domains and help to improve interoperability. Furthermore, linked data technologies are very effective for integrating a database with existing biological diversity databases. In this paper, we propose a keyword exploration technique to find appropriate keywords for retrieving meaningful knowledge from various biomimetics databases. Such a technique could support idea creation by users based on a biomimetics ontology. This paper shows a prototype of the biomimetics ontology and keyword exploration tool.</td></tr><tr><td>105</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_26">Template-Driven Semantic Parsing for Focused Web Crawler</a></td></tr><tr><td colspan=3>We present Template-Driven Semantic Parser (TDSP) capable to represent, at least to some degree, the semantics of Web pages being processed. Data extraction process realized by means of TDSP is driven by a set of instructions stored in an easily modifiable XML-based template. In order to enhance the precision of Web page data extraction, the TDSP template format allows to use a specialized Expression Language (EL). The template may be easily created and modified using a tool called Visual Template Designer. TDSP provides an output document containing an RDF graph composed of triples that represent the website resources under exploration. In accordance to the Semantic Web paradigm, each resource has its semantics assigned and is connected to other resources by means of one or many relations. The semantic types of the resources and the relations between them are predefined in an ontology of Web artifacts.</td></tr><tr><td>106</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-50112-3_1">How Can Reasoner Performance of ABox Intensive Ontologies Be Predicted?</a></td></tr><tr><td colspan=3>Reasoner performance prediction of ontologies in OWL 2 language has been studied so far from different dimensions. One key aspect of these studies has been the prediction of how much time a particular task for a given ontology will consume. Several approaches have adopted different machine learning techniques to predict time consumption of ontologies already. However, these studies focused on capturing general aspects of the ontologies (i.e., mainly the complexity of their TBoxes), while paying little attention to ABox intensive ontologies. To address this issue, in this paper, we propose to improve the representativeness of ontology metrics by developing new metrics which focus on the ABox features of ontologies. Our experiments show that the proposed metrics contribute to overall prediction accuracy for all ontologies in general without causing side-effects.</td></tr><tr><td>107</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_28">Choosing Related Concepts for Intelligent Exploration</a></td></tr><tr><td colspan=3>How to explore semantic data such as linked data, knowledge graph, ontologies etc. and get appropriate information from them are very important techniques for intelligent application systems based on them. In this article, we focus on intelligent exploration of ontologies since ontologies provide systematized knowledge to understand target domain and contribute to deep understanding of semantic data. The authors propose a novel conceptual search method called “Multistep Expansion based Concept Search” to get appropriate concepts from ontologies according to the user’s intentions and purpose.</td></tr><tr><td>108</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_24">Personalized Model Combination for News Recommendation in Microblogs</a></td></tr><tr><td colspan=3>Facing large amount of accessible data everyday on the Web, it is difficult for people to find relevant news articles, hence the importance of news recommendation. Focused on the information to be used and the way to model it, each of the existing models proposes its own algorithm to recommend different news to different users. For these models, personalization is only done at the recommendation level. But if the user chooses a model that is not appropriate for him, the recommendation may fail to work accurately. Therefore, personalization should also be done at the model level. In our proposed model, the first level is defining four atomic recommendation models that make fully use of the social and content information of users and the second level is adapting to each user that atomic models effectively used. Experiments conducted on two real datasets built from </td></tr><tr><td>109</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_25">A Model for Enriching Multilingual Wikipedias Using Infobox and Wikidata Property Alignment</a></td></tr><tr><td colspan=3>Wikipedia supports a large converged data with millions of contributions in more than 287 languages currently. Its content changes rapidly and continuously every hour with thousands of edits which trigger many challenges for Wikipedia in controlling, associating and balancing article content among language editions. This paper provides some processes to enrich Wikipedia content, which will retrieve semantic relations based on alignment between infobox properties and Wikidata properties in various languages. Then, the outcomes mainly contribute these semantic relations back to Wikidata and Wikipedias, especially ones are based on the Latin alphabet. The case study will offer a specific case about aligning biological infoboxes and detecting missing interwiki links of biological species at Vietnamese Wikipedia and English Wikipedia.</td></tr><tr><td>110</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_22">Development of the Belief Culture Ontology and Its Application: Case Study of the GreaterMekong Subregion</a></td></tr><tr><td colspan=3>In this paper, the development of an ontology that represents the knowledge of belief culture in the Greater Mekong subregion(GMS) is presented. The ontology was carefully designed to specify the concepts relevant to intangible and tangible cultural heritage and the relations among them. The knowledge domain in this work focuses on the cultural context and implicit attributes of the GMS as an initial case study. To further illustrate the potential of the developed ontology, a semantic search application was implemented and then evaluated by experts. On the evaluation processes, several complicated queries were designed in order to fully utilize the relations among ontological classes, and the results were returned accurately. The evaluation proved that the ontology was welldefined in aspects of its hierarchical structure and relations from intermediate concept layers.</td></tr><tr><td>111</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_23">Representations of Psychological Functions of Peer Support Services for Diabetic Patients</a></td></tr><tr><td colspan=3>One of the functions of peer support services for diabetic patients is psychological changes through communications among patients. Medical professionals in the practice of this research request peer support services through a web system. The design of the psychological functions requires tailoring depending on contexts. An important thing for the adaptive design is to discuss the needed psychological functions among designers such as medical professionals, patients, and researchers. However, since the designers tend to set intentions of psychological functions by taking a seat-of-the-pants approach, even for the designers, describing their intentions of psychological functions is not easy. In this paper, we propose a framework to represent psychological functions for the designers to share and discuss intentions of psychological functions in web systems.</td></tr><tr><td>112</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_20">RDFa Parser for Hybrid Resources</a></td></tr><tr><td colspan=3>In this paper, we propose several mash-up design methodologies that are intended for enhancing human-readable aspects of semantically annotated data resources such as LOD. Proposals are based on the features provided by RDFa format which is an extension of RDF format. So called “semantic parser” have been developed which is an application software to provide data-conversion/transformation service through web API. The parser provides several hybrid features for both human-readable (HR) and machine-readable (MR) resources by using RDFa format.</td></tr><tr><td>113</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_19">Optimizing SPARQL Query Processing on Dynamic and Static Data Based on Query Time/Freshness Requirements Using Materialization</a></td></tr><tr><td colspan=3>To integrate various Linked Datasets, data warehousing and live query processing provide two extremes for optimized response time and quality respectively. The first approach provides very fast responses but with low-quality because changes of original data are not immediately reflected on materialized data. The second approach provides accurate responses but it is notorious for long response times. A hybrid SPARQL query processor provides a middle ground between two specified extremes by splitting the triple patterns of SPARQL query between live and local processors based on a predetermined coherence threshold specified by the administrator. Considering quality requirements while splitting the SPARQL query, enables the processor to eliminate the unnecessary live execution and releases resources for other queries. This requires estimating the quality of response provided with current materialized data, compare it with user requirements and determine the most selective sub-queries which can boost the response quality up to the specified level with least computational complexity. In this work, we propose solutions for estimating the freshness of materialized data, as one dimension of the quality, by extending cardinality estimation techniques. Experimental results show that we can estimate the freshness of materialized data with a low error rate.</td></tr><tr><td>114</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_21">Ontology Construction and Schema Merging Using an Application-Independent Ontology for Humanitarian Aid in Disaster Management</a></td></tr><tr><td colspan=3>Humanitarian aid information, e.g., information on the occurrences of disaster situations, victims, shelters, resources, and facilities, is usually rapidly dynamic, ambiguous, and huge. A system of humanitarian aid often involves data items from multidisciplinary environments, some of which have similar meanings but appear structurally different in various data sources. To achieve semantic interoperability among humanitarian aid information systems to be exchanged meaningful information, this paper contributes a methodology for construction of an application-independent ontology and proposes a guideline for merging information from different databases through the application-independent ontology that helps people to integrate systems with minimal modification. We demonstrate how to develop an ontology for </td></tr><tr><td>115</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_18">Graph Pattern Based RDF Data Compression</a></td></tr><tr><td colspan=3>nan</td></tr><tr><td>116</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_17">G-Diff: A Grouping Algorithm for RDF Change Detection on MapReduce</a></td></tr><tr><td colspan=3>Linked Data is a collection of RDF data that can grow exponentially and change over time. Detecting changes in RDF data is important to support Linked Data consuming applications with version management. Traditional approaches for change detection are not scalable. This has led researchers to devise algorithms on the MapReduce framework. Most works simply take a URI as a Map key. We observed that it is not efficient to handle RDF data with a large number of distinct URIs since many Reduce tasks have to be created. Even though the Reduce tasks are scheduled to run simultaneously, too many small Reduce tasks would increase the overall running time. In this paper, we propose G-Diff, an efficient MapReduce algorithm for RDF change detection. G-Diff groups triples by URIs during Map phase and sends the triples to a particular Reduce task rather than multiple Reduce tasks. Experiments on real datasets showed that the proposed approach takes less running time than previous works.</td></tr><tr><td>117</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_16">Mining Type Information from Chinese Online Encyclopedias</a></td></tr><tr><td colspan=3>Recently, there is an increasing interest in extracting or mining type information from Web sources. Type information stating that an instance is of a certain type is an important component of knowledge bases. Although there has been some work on obtaining type information, most of current techniques are either language-dependent or to generate one or more general types for a given instance because of type sparseness. In this paper, we present a novel approach for mining type information from Chinese online encyclopedias. More precisely, we mine type information from abstracts, infoboxes and categories of article pages in Chinese encyclopedia Web sites. In particular, most of the generated Chinese type information is inferred from categories of article pages through an attribute propagation algorithm and a graph-based random walk method. We conduct experiments over Chinese encyclopedia Web sites: Baidu Baike, Hudong Baike and Chinese Wikipedia. Experimental results show that our approach can generate large scale and high-quality Chinese type information with types of appropriate granularity.</td></tr><tr><td>118</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_13">Publishing Danish Agricultural Government Data as Semantic Web Data</a></td></tr><tr><td colspan=3>Recent advances in Semantic Web technologies have led to a growing popularity of the Linked Open Data movement. Only recently, the Danish government has joined the movement and published several datasets as Open Data. These raw datasets are difficult to process automatically and combine with other data sources on the Web. Hence, our goal is to convert such data into RDF and make it available to a broader range of users and applications as Linked Open Data. In this paper, we discuss our experiences based on the particularly interesting use case of agricultural data as agriculture is one of the most important industries in Denmark. We describe the process of converting the data and discuss the particular problems that we encountered with respect to the considered datasets. We additionally evaluate our result based on several queries that could not be answered based on existing sources before.</td></tr><tr><td>119</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_15">A Multi-strategy Learning Approach to Competitor Identification</a></td></tr><tr><td colspan=3>Competitor identification tries to find competitors of some entity in a given field, which is the key to the success of market intelligence. Manually collecting competitors is labor-intensive and time consuming. So automatic approaches are proposed for this purpose. However, these approaches suffer from the following two main challenges. Competitor information might not only be contained in semi-structured sources like lists or tables, but also be mentioned in free texts. The diversity of its sources make competitor identification quite difficult. Also, these competitors might not always occur in form of their full names. The occurrences of name variants further increase the diversity, and make the task more challenging. In this paper, we propose a novel unsupervised approach to identify competitors from prospectuses based on a multi-strategy learning algorithm. More precisely, we first extract competitors from lists using some predefined heuristic rules. By leveraging redundancies among competitor information in lists, tables, and texts, these competitors are fed as seeds to distantly supervise the learning process to find table columns and text patterns containing competitors. The whole process is iteratively performed. In each iteration, the newly discovered competitors of high confidence from various sources are treated as new seeds for bootstrapping. The experimental results show the effectiveness of our approach without human intentions and external knowledge bases. Moreover, the approach significantly outperforms traditional named entity recognition approaches.</td></tr><tr><td>120</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_12">inteSearch: An Intelligent Linked Data Information Access Framework</a></td></tr><tr><td colspan=3>Information access over linked data requires to determine subgraph(s), in linked data’s underlying graph, that correspond to the required information need. Usually, an information access framework is able to retrieve richer information by checking of a large number of possible subgraphs. However, on the fly checking of a large number of possible subgraphs increases information access complexity. This makes an information access frameworks less effective. A large number of contemporary linked data information access frameworks reduce the complexity by introducing different heuristics but they suffer on retrieving richer information. Or, some frameworks do not care about the complexity. However, a practically usable framework should retrieve richer information with lower complexity. In linked data information access, we hypothesize that pre-processed data statistics of linked data can be used to efficiently check a large number of possible subgraphs. This will help to retrieve comparatively richer information with lower data access complexity. Preliminary evaluation of our proposed hypothesis shows promising performance.</td></tr><tr><td>121</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_7">An Information Literacy Ontology and Its Use for Guidance Plan Design – An Example on Problem Solving</a></td></tr><tr><td colspan=3>Recently, it is very important to educate about information literacy since information techniques are rapidly developed. However, common view and definition on information literacy are not established enough. Therefore, it is required to systematize concepts related to information literacy. This article discusses an experimental development of Information Literacy Ontology and its use for guidance plan design.</td></tr><tr><td>122</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_8">A Roadmap for Navigating the Life Sciences Linked Open Data Cloud</a></td></tr><tr><td colspan=3>Multiple datasets that add high value to biomedical research have been exposed on the web as a part of the Life Sciences Linked Open Data (LSLOD) Cloud. The ability to easily navigate through these datasets is crucial for personalized medicine and the improvement of drug discovery process. However, navigating these multiple datasets is not trivial as most of these are only available as isolated SPARQL endpoints with very little vocabulary reuse. The content that is indexed through these endpoints is scarce, making the indexed dataset opaque for users. In this paper, we propose an approach for the creation of an active Linked Life Sciences Data Roadmap, a set of configurable rules which can be used to discover links (roads) between biological entities (cities) in the LSLOD cloud. We have catalogued and linked concepts and properties from 137 public SPARQL endpoints. Our Roadmap is primarily used to dynamically assemble queries retrieving data from multiple SPARQL endpoints simultaneously. We also demonstrate its use in conjunction with other tools for selective SPARQL querying, semantic annotation of experimental datasets and the visualization of the LSLOD cloud. We have evaluated the performance of our approach in terms of the time taken and entity capture. Our approach, if generalized to encompass other domains, can be used for road-mapping the entire LOD cloud.</td></tr><tr><td>123</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_11">Building of Industrial Parts LOD for EDI - A Case Study -</a></td></tr><tr><td colspan=3>A wide variety of mechanical parts are used as products in the area of manufacturing. The code systems of product information are necessary for realizing Electronic Data Interchange (EDI) of business-to-business. However, each code systems are designed and maintained by different industry associations. Thus, we built an industrial parts Linked Open Data (LOD), which we called “N-ken LOD” based on a screw product code system (N-ken Code) maintained by Osaka fasteners cooperative association (Daibyokyo). In this paper, we first describe building of N-ken LOD, then how we linked it to external datasets like DBpedia, and built product supplier relations in order to support the EDI.</td></tr><tr><td>124</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_10">CURIOS Mobile: Linked Data Exploitation for Tourist Mobile Apps in Rural Areas</a></td></tr><tr><td colspan=3>As mobile devices proliferate and their computational power has increased rapidly over recent years, mobile applications have become a popular choice for visitors to enhance their travelling experience. However, most tourist mobile apps currently use narratives generated specifically for the app and often require a reliable Internet connection to download data from the cloud. These requirements are difficult to achieve in rural settings where many interesting cultural heritage sites are located. Although Linked Data has become a very popular format to preserve historical and cultural archives, it has not been applied to a great extent in tourist sector. In this paper we describe an approach to using Linked Data technology for enhancing visitors’ experience in rural settings. In particular, we present CURIOS Mobile, the implementation of our approach and an initial evaluation from a case study conducted in the Western Isles of Scotland.</td></tr><tr><td>125</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_9">Link Prediction in Linked Data of Interspecies Interactions Using Hybrid Recommendation Approach</a></td></tr><tr><td colspan=3>Linked Open Data for ACademia (LODAC) together with National Museum of Nature and Science have started collecting linked data of interspecies interaction and making link prediction for future observations. The initial data is very sparse and disconnected, making it very difficult to predict potential missing links using only one prediction model alone. In this paper, we introduce Link Prediction in Interspecies Interaction network (LPII) to solve this problem using hybrid recommendation approach. Our prediction model is a combination of three scoring functions, and takes into account collaborative filtering, community structure, and biological classification. We have found our approach, LPII, to be more accurate than other combinations of scoring functions. Using significance testing, we confirm that these three scoring functions are significant for LPII and they play different roles depending on the conditions of linked data. This shows that LPII can be applied to deal with other real-world situations of link prediction.
</td></tr><tr><td>126</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_5">Employing </a></td></tr><tr><td colspan=3>Fuzzy Description Logics generalize crisp ones by providing membership degree semantics for concepts and roles by fuzzy sets. Recently, answering of conjunctive queries has been investigated and implemented in optimized reasoner systems based on the rewriting approach for crisp DLs. In this paper we investigate how to employ such existing implementations for crisp query answering in </td></tr><tr><td>127</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_6">Ontology Based Inferences Engine for Veterinary Diagnosis</a></td></tr><tr><td colspan=3>Motivated on knowledge representation and veterinary domain this project aims at using semantic technologies to develop a tool which supports veterinary diagnosis. For this purpose an ontology based inference engine was developed following the diagnosis task definition provided by CommonKADS methodology. OWL was the language used for representing the ontologies, they were built using Protégé and processed using the Jena API. The inference engine was tested with two different ontologies. This shows the versatility of the developed tool that can easily be used to diagnose different types of diseases. This is an example of the application of CommonKADS diagnosis template using ontologies. We are currently working to make diagnoses in other domains of knowledge.</td></tr><tr><td>128</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_14">A Lightweight Treatment of Inexact Dates</a></td></tr><tr><td colspan=3>This paper presents a </td></tr><tr><td>129</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_4">Reasoning for </a></td></tr><tr><td colspan=3>This works is motivated by a real-world case study where it is necessary to integrate and relate existing ontologies through </td></tr><tr><td>130</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_2">On Desirable Properties of the Structural Subsumption-Based Similarity Measure</a></td></tr><tr><td colspan=3>Checking for subsumption relation is the main reasoning service readily available in classical DL reasoners. With their binary response stating whether two given concepts are in the subsumption relation, it is adequate for many applications relied on the service. However, in several specific applications, there often exists the case that requires an investigation for concepts that are not directly in a subclass-superclass relation but shared some commonality. In this case, providing merely a crisp response is apparently insufficient. To achieve this, the similarity measure for DL </td></tr><tr><td>131</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_3">A Graph-Based Approach to Ontology Debugging in DL-Lite</a></td></tr><tr><td colspan=3>Ontology debugging is an important nonstandard reasoning task in ontology engineering which provides the explanations of the causes of incoherence in an ontology. In this paper, we propose a graph-based algorithm to calculate minimal incoherence-preserving subterminology (MIPS) of an ontology in a light-weight ontology language, DL-Lite. We first encode a DL-Lite ontology to a graph, then calculate all the MIPS of an ontology by backtracking some pairs of nodes in the graph. We implement the algorithm and conduct experiments over some real ontologies. The experimental results show that our debugging system is efficient and outperforms the existing systems.</td></tr><tr><td>132</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-15615-6_1">Revisiting Default Description Logics – and Their Role in Aligning Ontologies</a></td></tr><tr><td colspan=3>We present a new approach to extend the Web Ontology Language (OWL) with the capabilities to reason with defaults. This work improves upon the previously established results on integrating defaults with description logics (DLs), which were shown to be decidable only when the application of defaults is restricted to named individuals in the knowledge base. We demonstrate that the application of defaults (integrated with DLs) does not have to be restricted to named individuals to retain decidability and elaborate on the application of defaults in the context of ontology alignment and ontology-based systems.</td></tr><tr><td>133</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_17">Effective Online Knowledge Graph Fusion</a></td></tr><tr><td colspan=3>Recently, Web search engines have empowered their search with knowledge graphs to satisfy increasing demands of complex information needs about entities. Each engine offers an online knowledge graph service to display highly relevant information about the query entity in form of a structured summary called </td></tr><tr><td>134</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_9">Entity Typing Using Distributional Semantics and DBpedia</a></td></tr><tr><td colspan=3>Recognising entities in a text and linking them to an external resource is a vital step in creating a structured resource (e.g. a knowledge base) from text. This allows semantic querying over a dataset, for example selecting all politicians or football players. However, traditional named entity recognition systems only distinguish a limited number of entity types (such as Person, Organisation and Location) and entity linking has the limitation that often not all entities found in a text can be linked to a knowledge base. This creates a gap in coverage between what is in the text and what can be annotated with fine grained types.</td></tr><tr><td>135</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_38">Combining Truth Discovery and RDF Knowledge Bases to Their Mutual Advantage</a></td></tr><tr><td colspan=3>This study exploits knowledge expressed in RDF Knowledge Bases (KBs) to enhance Truth Discovery (TD) performances. TD aims to identify </td></tr><tr><td>136</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_43">Encoding Category Correlations into Bilingual Topic Modeling for Cross-Lingual Taxonomy Alignment</a></td></tr><tr><td colspan=3>Cross-lingual taxonomy alignment (CLTA) refers to mapping each category in the source taxonomy of one language onto a ranked list of most relevant categories in the target taxonomy of another language. Recently, vector similarities depending on bilingual topic models have achieved the state-of-the-art performance on CLTA. However, these models only model the textual context of categories, but ignore explicit category correlations, such as correlations between the categories and their co-occurring words in text or correlations among the categories of ancestor-descendant relationships in a taxonomy. In this paper, we propose a unified solution to encode category correlations into bilingual topic modeling for CLTA, which brings two novel category correlation based bilingual topic models, called </td></tr><tr><td>137</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_38">How Semantic Technologies Can Enhance Data Access at Siemens Energy</a></td></tr><tr><td colspan=3>We present a description and analysis of the data access challenge in the Siemens Energy. We advocate for Ontology Based Data Access (OBDA) as a suitable Semantic Web driven technology to address the challenge. We derive requirements for applying OBDA in Siemens, review existing OBDA systems and discuss their limitations with respect to the Siemens requirements. We then introduce the Optique platform as a suitable OBDA solution for Siemens. Finally, we describe our preliminary installation and evaluation of the platform in Siemens.</td></tr><tr><td>138</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_44">Cross-Lingual Infobox Alignment in Wikipedia Using Entity-Attribute Factor Graph</a></td></tr><tr><td colspan=3>Wikipedia infoboxes contain information about article entities in the form of attribute-value pairs, and are thus a very rich source of structured knowledge. However, as the different language versions of Wikipedia evolve independently, it is a promising but challenging problem to find correspondences between infobox attributes in different language editions. In this paper, we propose 8 effective features for cross lingual infobox attribute matching containing categories, templates, attribute labels and values. We propose entity-attribute factor graph to consider not only individual features but also the correlations among attribute pairs. Experiments on the two Wikipedia data sets of English-Chinese and English-French show that proposed approach can achieve high F1-measure: 85.5% and 85.4% respectively on the two data sets. Our proposed approach finds 23,923 new infobox attribute mappings between English and Chinese Wikipedia, and 31,576 between English and French based on no more than six thousand existing matched infobox attributes. We conduct an infobox completion experiment on English-Chinese Wikipedia and complement 76,498 (more than 30% of EN-ZH Wikipedia existing cross-lingual links) pairs of corresponding articles with more than one attribute-value pairs.</td></tr><tr><td>139</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_29">Automated Fine-Grained Trust Assessment in Federated Knowledge Bases</a></td></tr><tr><td colspan=3>The federation of different data sources gained increasing attention due to the continuously growing amount of data. But the more data are available from heterogeneous sources, the higher the risk is of inconsistency. To tackle this challenge in federated knowledge bases we propose a fully automated approach for computing trust values at different levels of granularity. Gathering both the conflict graph and statistical evidence generated by inconsistency detection and resolution, we create a Markov network to facilitate the application of Gibbs sampling to compute a probability for each conflicting assertion. Based on which, trust values for each integrated data source and its respective signature elements are computed. We evaluate our approach on a large distributed dataset from the domain of library science.</td></tr><tr><td>140</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_39">Content Based Fake News Detection Using Knowledge Graphs</a></td></tr><tr><td colspan=3>This paper addresses the problem of fake news detection. There are many works already in this space; however, most of them are for social media and not using news content for the decision making. In this paper, we propose some novel approaches, including the B-TransE model, to detecting fake news based on news content using knowledge graphs. In our solutions, we need to address a few technical challenges. Firstly, computational-oriented fact checking is not comprehensive enough to cover all the relations needed for fake news detection. Secondly, it is challenging to validate the correctness of the extracted triples from news articles. Our approaches are evaluated with the Kaggle’s ‘Getting Real about Fake News’ dataset and some true articles from main stream media. The evaluations show that some of our approaches have over 0.80 F1-scores.</td></tr><tr><td>141</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_37">Measuring Semantic Coherence of a Conversation</a></td></tr><tr><td colspan=3>Conversational systems have become increasingly popular as a way for humans to interact with computers. To be able to provide intelligent responses, conversational systems must correctly model the structure and semantics of a conversation. We introduce the task of measuring semantic (in)coherence in a conversation with respect to background knowledge, which relies on the identification of semantic relations between concepts introduced during a conversation. We propose and evaluate graph-based and machine learning-based approaches for measuring semantic coherence using knowledge graphs, their vector space embeddings and word embedding models, as sources of background knowledge. We demonstrate how these approaches are able to uncover different coherence patterns in conversations on the Ubuntu Dialogue Corpus.</td></tr><tr><td>142</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_36">Cross-Lingual Classification of Crisis Data</a></td></tr><tr><td colspan=3>Many citizens nowadays flock to social media during crises to share or acquire the latest information about the event. Due to the sheer volume of data typically circulated during such events, it is necessary to be able to efficiently filter out irrelevant posts, thus focusing attention on the posts that are truly relevant to the crisis. Current methods for classifying the relevance of posts to a crisis or set of crises typically struggle to deal with posts in different languages, and it is not viable during rapidly evolving crisis situations to train new models for each language. In this paper we test statistical and semantic classification approaches on cross-lingual datasets from 30 crisis events, consisting of posts written mainly in English, Spanish, and Italian. We experiment with scenarios where the model is trained on one language and tested on another, and where the data is translated to a single language. We show that the addition of semantic features extracted from external knowledge bases improve accuracy over a purely statistical model.</td></tr><tr><td>143</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_35">Canonicalisation of Monotone SPARQL Queries</a></td></tr><tr><td colspan=3>Caching in the context of expressive query languages such as SPARQL is complicated by the difficulty of detecting equivalent queries: deciding if two conjunctive queries are equivalent is NP-complete, where adding further query features makes the problem undecidable. Despite this complexity, in this paper we propose an algorithm that performs syntactic canonicalisation of SPARQL queries such that the answers for the canonicalised query will not change versus the original. We can guarantee that the canonicalisation of two queries within a core fragment of SPARQL (monotone queries with select, project, join and union) is equal if and only if the two queries are equivalent; we also support other SPARQL features but with a weaker soundness guarantee: that the (partially) canonicalised query is equivalent to the input query. Despite the fact that canonicalisation must be harder than the equivalence problem, we show the algorithm to be practical for real-world queries taken from SPARQL endpoint logs, and further show that it detects more equivalent queries than when compared with purely syntactic methods. We also present the results of experiments over synthetic queries designed to stress-test the canonicalisation method, highlighting difficult cases.</td></tr><tr><td>144</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_10">WC3: Analyzing the Style of Metadata Annotation Among Wikipedia Articles by Using Wikipedia Category and the DBpedia Metadata Database</a></td></tr><tr><td colspan=3>WC3 (Wikipedia Category Consistency Checker) is a system that supports the analysis of the metadata-annotation style in Wikipedia articles belonging to a particular Wikipedia category (the subcategory of “Categories by parameter”) by using the DBpedia metadata database. This system aims to construct an appropriate SPARQL query to represent the category and compares the retrieved results and articles that belong to the category. In this paper, we introduce WC3 and extend the algorithm to analyze efficiently additional varieties of Wikipedia category. We also discuss the metadata-annotation quality of the Wikipedia by using WC3. URL of WC3 is </td></tr><tr><td>145</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_31">Query-Based Linked Data Anonymization</a></td></tr><tr><td colspan=3>We introduce and develop a declarative framework for privacy-preserving Linked Data publishing in which privacy and utility policies are specified as SPARQL queries. Our approach is data-independent and leads to inspect only the privacy and utility policies in order to determine the sequence of anonymization operations applicable to any graph instance for satisfying the policies. We prove the soundness of our algorithms and gauge their performance through experiments.</td></tr><tr><td>146</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_34">WORQ: Workload-Driven RDF Query Processing</a></td></tr><tr><td colspan=3>Cloud-based systems provide a rich platform for managing large-scale RDF data. However, the distributed nature of these systems introduces several performance challenges, </td></tr><tr><td>147</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_33">Bash Datalog: Answering Datalog Queries with Unix Shell Commands</a></td></tr><tr><td colspan=3>Dealing with large tabular datasets often requires extensive preprocessing. This preprocessing happens only once, so that loading and indexing the data in a database or triple store may be an overkill. In this paper, we present an approach that allows preprocessing large tabular data in Datalog – without indexing the data. The Datalog query is translated to Unix Bash and can be executed in a shell. Our experiments show that, for the use case of data preprocessing, our approach is competitive with state-of-the-art systems in terms of scalability and speed, while at the same time requiring only a Bash shell on a Unix system.</td></tr><tr><td>148</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_32">Answering Provenance-Aware Queries on RDF Data Cubes Under Memory Budgets</a></td></tr><tr><td colspan=3>The steadily-growing popularity of semantic data on the Web and the support for aggregation queries in SPARQL 1.1 have propelled the interest in Online Analytical Processing (OLAP) and data cubes in RDF. Query processing in such settings is challenging because SPARQL OLAP queries usually contain many triple patterns with grouping and aggregation. Moreover, one important factor of query answering on Web data is its provenance, i.e., metadata about its origin. Some applications in data analytics and access control require to augment the data with provenance metadata and run queries that impose constraints on this provenance. This task is called provenance-aware query answering. In this paper, we investigate the benefit of caching some parts of an RDF cube augmented with provenance information when answering provenance-aware SPARQL queries. We propose provenance-aware caching (PAC), a caching approach based on a provenance-aware partitioning of RDF graphs, and a benefit model for RDF cubes and SPARQL queries with aggregation. Our results on real and synthetic data show that PAC outperforms significantly the LRU strategy (least recently used) and the Jena TDB native caching in terms of hit-rate and response time.</td></tr><tr><td>149</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_30">Towards Empty Answers in SPARQL: Approximating Querying with RDF Embedding</a></td></tr><tr><td colspan=3>The LOD cloud offers a plethora of RDF data sources where users discover items of interest by issuing SPARQL queries. A common query problem for users is to face with empty answers: given a SPARQL query that returns nothing, how to refine the query to obtain a non-empty set? In this paper, we propose an RDF graph embedding based framework to solve the SPARQL empty-answer problem in terms of a continuous vector space. We first project the RDF graph into a continuous vector space by an entity context preserving translational embedding model which is specially designed for SPARQL queries. Then, given a SPARQL query that returns an empty set, we partition it into several parts and compute approximate answers by leveraging RDF embeddings and the translation mechanism. We also generate alternative queries for returned answers, which helps users recognize their expectations and refine the original query finally. To validate the effectiveness and efficiency of our framework, we conduct extensive experiments on the real-world RDF dataset. The results show that our framework can significantly improve the quality of approximate answers and speed up the generation of alternative queries.</td></tr><tr><td>150</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_28">Practical Ontology Pattern Instantiation, Discovery, and Maintenance with Reasonable Ontology Templates</a></td></tr><tr><td colspan=3>Reasonable Ontology Templates (</td></tr><tr><td>151</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_29">Pragmatic Ontology Evolution: Reconciling User Requirements and Application Performance</a></td></tr><tr><td colspan=3>Increasingly, organizations are adopting ontologies to describe their large catalogues of items. These ontologies need to evolve regularly in response to changes in the domain and the emergence of new requirements. An important step of this process is the selection of candidate concepts to include in the new version of the ontology. This operation needs to take into account a variety of factors and in particular reconcile user requirements and application performance. Current ontology evolution methods focus either on ranking concepts according to their relevance or on preserving compatibility with existing applications. However, they do not take in consideration the impact of the ontology evolution process on the performance of computational tasks – e.g., in this work we focus on instance tagging, similarity computation, generation of recommendations, and data clustering. In this paper, we propose the </td></tr><tr><td>152</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_27">A Novel Approach and Practical Algorithms for Ontology Integration</a></td></tr><tr><td colspan=3>Today a wealth of knowledge and data are distributed using Semantic Web standards. Especially in the (bio)medical domain several sources like SNOMED, NCI, FMA, and more are distributed in the form of OWL ontologies. These can be matched and integrated in order to create one large medical Knowledge Base. However, an important issue is that the structure of these ontologies may be profoundly different hence using the mappings as initially computed can lead to incoherences or changes in their original structure which may affect applications. In this paper we present a framework and novel approach for integrating independently developed ontologies. Starting from an initial seed ontology which may already be in use by an application, new sources are used to iteratively enrich and extend the seed one. To deal with structural incompatibilities we present a novel fine-grained approach which is based on mapping repair and alignment conservativity, formalise it and provide an exact as well as approximate but practical algorithms. Our framework has already been used to integrate a number of medical ontologies and support real-world healthcare services provided by Babylon Health. Finally, we also perform an experimental evaluation and compare with state-of-the-art ontology integration systems that take into account the structure and coherency of the integrated ontologies obtaining encouraging results.</td></tr><tr><td>153</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_26">Mapping Diverse Data to RDF in Practice</a></td></tr><tr><td colspan=3>Converting data from diverse data sources to custom RDF datasets often faces several practical challenges related with the need to restructure and transform the source data. Existing RDF mapping languages assume that the resulting datasets mostly preserve the structure of the original data. In this paper, we present real cases that highlight the limitations of existing languages, and describe D2RML, a transformation-oriented RDF mapping language which addresses such practical needs by incorporating a programming flavor in the mapping process.</td></tr><tr><td>154</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_24">: A Benchmark Generator for Spatial Link Discovery Tools</a></td></tr><tr><td colspan=3>A number of real and synthetic benchmarks have been proposed for evaluating the performance of link discovery systems. So far, only a limited number of </td></tr><tr><td>155</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_21">Efficient Handling of SPARQL OPTIONAL for OBDA</a></td></tr><tr><td colspan=3> is a key feature in SPARQL for dealing with missing information. While this operator is used extensively, it is also known for its complexity, which can make efficient evaluation of queries with </td></tr><tr><td>156</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_23">Detecting Erroneous Identity Links on the Web Using Network Metrics</a></td></tr><tr><td colspan=3>In the absence of a central naming authority on the Semantic Web, it is common for different datasets to refer to the same thing by different IRIs. Whenever multiple names are used to denote the same thing, </td></tr><tr><td>157</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_19">Semantics and Validation of Recursive SHACL</a></td></tr><tr><td colspan=3>With the popularity of RDF as an independent data model came the need for specifying constraints on RDF graphs, and for mechanisms to detect violations of such constraints. One of the most promising schema languages for RDF is SHACL, a recent W3C recommendation. Unfortunately, the specification of SHACL leaves open the problem of validation against recursive constraints. This omission is important because SHACL by design favors constraints that reference other ones, which in practice may easily yield reference cycles.</td></tr><tr><td>158</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_18">GraFa: Scalable Faceted Browsing for RDF Graphs</a></td></tr><tr><td colspan=3>Faceted browsing has become a popular paradigm for user interfaces on the Web and has also been investigated in the context of RDF graphs. However, current faceted browsers for RDF graphs encounter performance issues when faced with two challenges: scale, where large datasets generate many results, and heterogeneity, where large numbers of properties and classes generate many facets. To address these challenges, we propose </td></tr><tr><td>159</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_25">Specifying, Monitoring, and Executing Workflows in Linked Data Environments</a></td></tr><tr><td colspan=3>We present an ontology for representing workflows over components with Read-Write Linked Data interfaces and give an operational semantics to the ontology via a rule language. Workflow languages have been successfully applied for modelling behaviour in enterprise information systems, in which the data is often managed in a relational database. Linked Data interfaces have been widely deployed on the web to support data integration in very diverse domains, increasingly also in scenarios involving the Internet of Things, in which application behaviour is often specified using imperative programming languages. With our work we aim to combine workflow languages, which allow for the high-level specification of application behaviour by non-expert users, with Linked Data, which allows for decentralised data publication and integrated data access. We show that our ontology is expressive enough to cover the basic workflow patterns and demonstrate the applicability of our approach with a prototype system that observes pilots carrying out tasks in a virtual reality aircraft cockpit. On a synthetic benchmark from the building automation domain, the runtime scales linearly with the size of the number of Internet of Things devices.</td></tr><tr><td>160</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_17"> Question Answering Over CodeOntology</a></td></tr><tr><td colspan=3>We present an unsupervised approach to process natural language questions that cannot be answered by factual question answering nor advanced data querying, requiring instead ad-hoc code generation and execution. To address this challenging task, our system, </td></tr><tr><td>161</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_20">Certain Answers for SPARQL with Blank Nodes</a></td></tr><tr><td colspan=3>Blank nodes in RDF graphs can be used to represent values known to exist but whose identity remains unknown. A prominent example of such usage can be found in the Wikidata dataset where, e.g., the author of Beowulf is given as a blank node. However, while SPARQL considers blank nodes in a query as existentials, it treats blank nodes in RDF data more like constants. Running SPARQL queries over datasets with unknown values may thus lead to counter-intuitive results, which may make the standard SPARQL semantics unsuitable for datasets with existential blank nodes. We thus explore the feasibility of an alternative SPARQL semantics based on certain answers. In order to estimate the performance costs that would be associated with such a change in semantics for current implementations, we adapt and evaluate approximation techniques proposed in a relational database setting for a core fragment of SPARQL. To further understand the impact that such a change in semantics may have on query solutions, we analyse how this new semantics would affect the results of user queries over Wikidata.</td></tr><tr><td>162</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_22">Representativeness of Knowledge Bases with the Generalized Benford’s Law</a></td></tr><tr><td colspan=3>Knowledge bases (KBs) such as DBpedia, Wikidata, and YAGO contain a huge number of entities and facts. Several recent works induce rules or calculate statistics on these KBs. Most of these methods are based on the assumption that the data is a representative sample of the studied universe. Unfortunately, KBs are biased because they are built from crowdsourcing and opportunistic agglomeration of available databases. This paper aims at approximating the representativeness of a relation within a knowledge base. For this, we use the generalized Benford’s law, which indicates the distribution expected by the facts of a relation. We then compute the minimum number of facts that have to be added in order to make the KB representative of the real world. Experiments show that our unsupervised method applies to a large number of relations. For numerical relations where ground truths exist, the estimated representativeness proves to be a reliable indicator.</td></tr><tr><td>163</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_15">That’s Interesting, Tell Me More! Finding Descriptive Support Passages for Knowledge Graph Relationships</a></td></tr><tr><td colspan=3>We address the problem of finding descriptive explanations of facts stored in a knowledge graph. This is important in high-risk domains such as healthcare, intelligence, etc. where users need additional information for decision making and is especially crucial for applications that rely on automatically constructed knowledge graphs where machine-learned systems extract facts from an input corpus and working of the extractors is opaque to the end-user. We follow an approach inspired from information retrieval and propose a simple, yet effective and efficient solution that takes into account passage level as well as document level properties to produce a ranked list of passages describing a given input relation. We test our approach using Wikidata as the knowledge base and Wikipedia as the source corpus and report results of user studies conducted to study the effectiveness of our proposed model.</td></tr><tr><td>164</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_14">Structured Event Entity Resolution in Humanitarian Domains</a></td></tr><tr><td colspan=3>In domains such as humanitarian assistance and disaster relief (HADR), events, rather than named entities, are the primary focus of analysts and aid officials. An important problem that must be solved to provide situational awareness to aid providers is automatic clustering of sub-events that refer to the same underlying event. An effective solution to the problem requires judicious use of both domain-specific and semantic information, as well as statistical methods like deep neural embeddings. In this paper, we present an approach, AugSEER (Augmented feature sets for Structured Event Entity Resolution), that combines advances in deep neural embeddings both on text and graph data with minimally supervised inputs from domain experts. AugSEER can operate in both online and batch scenarios. On five real-world HADR datasets, AugSEER is found, on average, to outperform the next best baseline result by almost 15% on the cluster purity metric and by 3% on the F1-Measure metric. In contrast, text-based approaches are found to perform poorly, demonstrating the importance of semantic information in devising a good solution. We also use sub-event clustering visualizations to illustrate the qualitative potential of AugSEER.</td></tr><tr><td>165</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_16">Exploring RDFS KBs Using Summaries</a></td></tr><tr><td colspan=3>Ontology summarization aspires to produce an abridged version of the original data source highlighting its most important concepts. However, in an ideal scenario, the user should not be limited only to static summaries. Starting from the summary, s/he should be able to further explore the data source requesting more detailed information for a particular part of it. In this paper, we present a new approach enabling the dynamic exploration of summaries through two novel operations </td></tr><tr><td>166</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_12">QA4IE: A Question Answering Based Framework for Information Extraction</a></td></tr><tr><td colspan=3>Information Extraction (IE) refers to automatically extracting structured relation tuples from unstructured texts. Common IE solutions, including Relation Extraction (RE) and open IE systems, can hardly handle cross-sentence tuples, and are severely restricted by limited relation types as well as informal relation specifications (e.g., free-text based relation tuples). In order to overcome these weaknesses, we propose a novel IE framework named QA4IE, which leverages the flexible question answering (QA) approaches to produce high quality relation triples across sentences. Based on the framework, we develop a large IE benchmark with high quality human evaluation. This benchmark contains 293K documents, 2M golden relation triples, and 636 relation types. We compare our system with some IE baselines on our benchmark and the results show that our system achieves great improvements.</td></tr><tr><td>167</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_13">Constructing a Recipe Web from Historical Newspapers</a></td></tr><tr><td colspan=3>Historical newspapers provide a lens on customs and habits of the past. For example, recipes published in newspapers highlight what and how we ate and thought about food. The challenge here is that newspaper data is often unstructured and highly varied. Digitised historical newspapers add an additional challenge, namely that of fluctuations in OCR quality. Therefore, it is difficult to locate and extract recipes from them. We present our approach based on distant supervision and automatically extracted lexicons to identify recipes in digitised historical newspapers, to generate recipe tags, and to extract ingredient information. We provide OCR quality indicators and their impact on the extraction process. We enrich the recipes with links to information on the ingredients. Our research shows how natural language processing, machine learning, and semantic web can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture.</td></tr><tr><td>168</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_9">An Ontology-Driven Probabilistic Soft Logic Approach to Improve NLP Entity Annotations</a></td></tr><tr><td colspan=3>Many approaches for Knowledge Extraction and Ontology Population rely on well-known Natural Language Processing (NLP) tasks, such as Named Entity Recognition and Classification (NERC) and Entity Linking (EL), to identify and semantically characterize the entities mentioned in natural language text. Despite being intrinsically related, the analyses performed by these tasks differ, and combining their output may result in NLP annotations that are implausible or even conflicting considering common world knowledge about entities. In this paper we present a Probabilistic Soft Logic (PSL) model that leverages ontological entity classes to relate NLP annotations from different tasks insisting on the same entity mentions. The intuition behind the model is that an annotation likely implies some ontological classes on the entity identified by the mention, and annotations from different tasks on the same mention have to share more or less the same implied entity classes. In a setting with various NLP tools returning multiple, confidence-weighted, candidate annotations on a single mention, the model can be operationally applied to compare the different annotation combinations, and to possibly revise the tools’ best annotation choice. We experimented applying the model with the candidate annotations produced by two state-of-the-art tools for NERC and EL, on three different datasets. The results show that the joint “a posteriori” annotation revision suggested by our PSL model consistently improves the original scores of the two tools.</td></tr><tr><td>169</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_11">Enriching Knowledge Bases with Counting Quantifiers</a></td></tr><tr><td colspan=3>Information extraction traditionally focuses on extracting relations between identifiable entities, such as </td></tr><tr><td>170</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_10">Ontology Driven Extraction of Research Processes</a></td></tr><tr><td colspan=3>We address the automatic extraction from publications of two key concepts for representing research processes: the concept of research activity and the sequence relation between successive activities. These representations are driven by the Scholarly Ontology, specifically conceived for documenting research processes. Unlike usual named entity recognition and relation extraction tasks, we are facing textual descriptions of activities of widely variable length, while pairs of successive activities often span multiple sentences. We developed and experimented with several sliding window classifiers using Logistic Regression, SVMs, and Random Forests, as well as a two-stage pipeline classifier. Our classifiers employ task-specific features, as well as word, part-of-speech and dependency embeddings, engineered to exploit distinctive traits of research publications written in English. The extracted activities and sequences are associated with other relevant information from publication metadata and stored as RDF triples in a knowledge base. Evaluation on datasets from three disciplines, Digital Humanities, Bioinformatics, and Medicine, shows very promising performance.</td></tr><tr><td>171</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_8">TSE-NER: An Iterative Approach for Long-Tail Entity Extraction in Scientific Publications</a></td></tr><tr><td colspan=3>Named Entity Recognition and Typing (NER/NET) is a challenging task, especially with long-tail entities such as the ones found in scientific publications. These entities (e.g. “WebKB”,“StatSnowball”) are rare, often relevant only in specific knowledge domains, yet important for retrieval and exploration purposes. State-of-the-art NER approaches employ supervised machine learning models, trained on expensive type-labeled data laboriously produced by human annotators. A common workaround is the generation of labeled training data from knowledge bases; this approach is not suitable for long-tail entity types that are, by definition, scarcely represented in KBs. This paper presents an iterative approach for training NER and NET classifiers in scientific publications that relies on minimal human input, namely a small seed set of instances for the targeted entity type. We introduce different strategies for training data extraction, semantic expansion, and result entity filtering. We evaluate our approach on scientific publications, focusing on the long-tail entities types </td></tr><tr><td>172</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_6">A Novel Ensemble Method for Named Entity Recognition and Disambiguation Based on Neural Network</a></td></tr><tr><td colspan=3>Named entity recognition (NER) and disambiguation (NED) are subtasks of information extraction that aim to recognize named entities mentioned in text, to assign them pre-defined types, and to link them with their matching entities in a knowledge base. Many approaches, often exposed as web APIs, have been proposed to solve these tasks during the last years. These APIs classify entities using different taxonomies and disambiguate them with different knowledge bases. In this paper, we describe Ensemble Nerd, a framework that collects numerous extractors responses, normalizes them and combines them in order to produce a final entity list according to the pattern (surface form, type, link). The presented approach is based on representing the extractors responses as real-value vectors and on using them as input samples for two Deep Learning networks: ENNTR (Ensemble Neural Network for Type Recognition) and ENND (Ensemble Neural Network for Disambiguation). We train these networks using specific gold standards. We show that the models produced outperform each single extractor responses in terms of micro and macro F1 measures computed by the GERBIL framework.</td></tr><tr><td>173</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_4">Towards Encoding Time in Text-Based Entity Embeddings</a></td></tr><tr><td colspan=3>Knowledge Graphs (KG) are widely used abstractions to represent entity-centric knowledge. Approaches to embed entities, entity types and relations represented in the graph into vector spaces - often referred to as KG embeddings - have become increasingly popular for their ability to capture the similarity between entities and support other reasoning tasks. However, representation of time has received little attention in these approaches. In this work, we make a first step to encode time into vector-based entity representations using a text-based KG embedding model named Typed Entity Embeddings (TEEs). In TEEs, each entity is represented by a vector that represents the entity and its type, which is learned from entity mentions found in a text corpus. Inspired by evidence from cognitive sciences and application-oriented concerns, we propose an approach to encode representations of years into TEEs by aggregating the representations of the entities that occur in event-based descriptions of the years. These representations are used to define two time-aware similarity measures to control the implicit effect of time on entity similarity. Experimental results show that the linear order of years obtained using our model is highly correlated with natural time flow and the effectiveness of the time-aware similarity measure proposed to flatten the time effect on entity similarity.</td></tr><tr><td>174</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_7">EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs</a></td></tr><tr><td colspan=3>Many question answering systems over knowledge graphs rely on entity and relation linking components in order to connect the natural language input to the underlying knowledge graph. Traditionally, entity linking and relation linking have been performed either as dependent sequential tasks or as independent parallel tasks. In this paper, we propose a framework called EARL, which performs entity linking and relation linking as a joint task. EARL implements two different solution strategies for which we provide a comparative analysis in this paper: The first strategy is a formalisation of the joint entity and relation linking tasks as an instance of the Generalised Travelling Salesman Problem (GTSP). In order to be computationally feasible, we employ approximate GTSP solvers. The second strategy uses machine learning in order to exploit the connection density between nodes in the knowledge graph. It relies on three base features and re-ranking steps in order to predict entities and relations. We compare the strategies and evaluate them on a dataset with 5000 questions. Both strategies significantly outperform the current state-of-the-art approaches for entity and relation linking.</td></tr><tr><td>175</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_5">Rule Learning from Knowledge Graphs Guided by Embedding Models</a></td></tr><tr><td colspan=3>Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as confidence reflect rule quality well when the KG is reasonably complete; however, these measures might be misleading otherwise. So it is difficult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules could be generated. Therefore, the ranking and pruning of candidate rules are major problems. To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.</td></tr><tr><td>176</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_41">Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics</a></td></tr><tr><td colspan=3>Knowledge Graphs (KGs) effectively capture explicit relational knowledge about individual entities. However, visual attributes of those entities, like their shape and color and pragmatic aspects concerning their usage in natural language are not covered. Recent approaches encode such knowledge by learning latent representations (‘embeddings’) separately: In computer vision, visual object features are learned from large image collections and in computational linguistics, word embeddings are extracted from huge text corpora which capture their distributional semantics. We investigate the potential of complementing the relational knowledge captured in KG embeddings with knowledge from text documents and images by learning a shared latent representation that integrates information across those modalities. Our empirical results show that a joined concept representation provides measurable benefits for (i) semantic similarity benchmarks, since it shows a higher correlation with the human notion of similarity than uni- or bi-modal representations, and (ii) entity-type prediction tasks, since it clearly outperforms plain KG embeddings. These findings encourage further research towards capturing types of knowledge that go beyond today’s KGs.</td></tr><tr><td>177</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_2">Aligning Knowledge Base and Document Embedding Models Using Regularized Multi-Task Learning</a></td></tr><tr><td colspan=3>Knowledge Bases (KBs) and textual documents contain rich and complementary information about real-world objects, as well as relations among them. While text documents describe entities in freeform, KBs organizes such information in a structured way. This makes these two information representation forms hard to compare and integrate, limiting the possibility to use them jointly to improve predictive and analytical tasks. In this article, we study this problem, and we propose KADE, a solution based on a regularized multi-task learning of KB and document embeddings. KADE can potentially incorporate any KB and document embedding learning method. Our experiments on multiple datasets and methods show that KADE effectively aligns document and entities embeddings, while maintaining the characteristics of the embedding models.</td></tr><tr><td>178</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_3">Inducing Implicit Relations from Text Using Distantly Supervised Deep Nets</a></td></tr><tr><td colspan=3>Knowledge Base Population (KBP) is an important problem in Semantic Web research and a key requirement for successful adoption of semantic technologies in many applications. In this paper we present Socrates, a deep learning based solution for Automated Knowledge Base Population from Text. Socrates does not require manual annotations which would make the solution hard to adapt to a new domain. Instead, it exploits a partially populated knowledge base and a large corpus of text documents to train a set of deep neural network models. As a result of the training process, the system learns how to identify implicit relations between entities across a highly heterogeneous set of documents from various sources, making it suitable for large-scale knowledge extraction from Web documents. Main contributions of this paper include (a) a novel approach based on composite contexts to acquire implicit relations from Title Oriented Documents, and (b) an architecture for unifying relation extraction using binary, unary, and composite contexts. We provide an extensive evaluation of the system across three different benchmarks with different characteristics, showing that our unified framework can consistently outperform state of the art solutions. Remarkably, Socrates ranked first in both the knowledge base population and attribute validation track at the Semantic Web Challenge at ISWC 2017.</td></tr><tr><td>179</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_40">Ontolex JeuxDeMots and Its Alignment to the Linguistic Linked Open Data Cloud</a></td></tr><tr><td colspan=3>JeuxDeMots (JdM) is a rich collaborative lexical network in French, built on a crowdsourcing principle as a game with a purpose, represented in an ad-hoc tabular format. In the interest of reuse and interoperability, we propose a conversion algorithm for JdM following the Ontolex model, along with a word sense alignment algorithm, called JdMBabelizer, that anchors JdM sense-refinements to synsets in the </td></tr><tr><td>180</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_38">Blockchain Enabled Privacy Audit Logs</a></td></tr><tr><td colspan=3>Privacy audit logs are used to capture the actions of participants in a data sharing environment in order for auditors to check compliance with privacy policies. However, collusion may occur between the auditors and participants to obfuscate actions that should be recorded in the audit logs. In this paper, we propose a Linked Data based method of utilizing blockchain technology to create tamper-proof audit logs that provide proof of log manipulation and non-repudiation. We also provide experimental validation of the scalability of our solution using an existing Linked Data privacy audit log model.</td></tr><tr><td>181</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_39">VICKEY: Mining Conditional Keys on Knowledge Bases</a></td></tr><tr><td colspan=3>A conditional key is a key constraint that is valid in only a part of the data. In this paper, we show how such keys can be mined automatically on large knowledge bases (KBs). For this, we combine techniques from key mining with techniques from rule mining. We show that our method can scale to KBs of millions of facts. We also show that the conditional keys we mine can improve the quality of entity linking by up to 47% points.</td></tr><tr><td>182</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_42">An Extension of SPARQL for Expressing Qualitative Preferences</a></td></tr><tr><td colspan=3>In this paper we present SPREFQL, an extension of the SPARQL language that allows appending a </td></tr><tr><td>183</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_34">Mining Hypotheses from Data in OWL: Advanced Evaluation and Complete Construction</a></td></tr><tr><td colspan=3>Automated acquisition (learning) of ontologies from data has attracted research interest because it can complement manual, expensive construction of ontologies. We investigate the problem of General Terminology Induction in OWL, i.e. acquiring general, expressive TBox axioms (hypotheses) from an ABox (data). We define novel measures designed to rigorously evaluate the quality of hypotheses while respecting the standard semantics of OWL. We propose an informed, data-driven algorithm that constructs class expressions for hypotheses in OWL and guarantees completeness. We empirically evaluate the quality measures on two corpora of ontologies and run a case study with a domain expert to gain insight into applicability of the measures and acquired hypotheses. The results show that the measures capture different quality aspects and not only correct hypotheses can be interesting.</td></tr><tr><td>184</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_37">Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding</a></td></tr><tr><td colspan=3>Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent the same real-world object. When facing KBs in different natural languages, conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers. These approaches often suffer from the uneven quality of translations between languages. While recent embedding-based techniques encode entities and relationships in KBs and do not need machine translation for cross-lingual entity alignment, a significant number of attributes remain largely unexplored. In this paper, we propose a joint attribute-preserving embedding model for cross-lingual entity alignment. It jointly embeds the structures of two KBs into a unified vector space and further refines it by leveraging attribute correlations in the KBs. Our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation.</td></tr><tr><td>185</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_35">Semantic Faceted Search with Aggregation and Recursion</a></td></tr><tr><td colspan=3>Faceted search is the de facto approach for exploration of data in e-commerce: it allows users to construct queries in an intuitive way without a prior knowledge of formal query languages. This approach has been recently adapted to the context of RDF. Existing faceted search systems however do not allow users to construct queries with aggregation and recursion which poses limitations in practice. In this work we extend faceted search over RDF with these functionalities and study the corresponding query language. In particular, we investigate complexity of the query answering and query containment problems.</td></tr><tr><td>186</td><td>2018</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-030-00671-6_1">Fine-Grained Evaluation of Rule- and Embedding-Based Systems for Knowledge Graph Completion</a></td></tr><tr><td colspan=3>Over the recent years, embedding methods have attracted increasing focus as a means for knowledge graph completion. Similarly, rule-based systems have been studied for this task in the past. What is missing so far is a common evaluation that includes more than one type of method. We close this gap by comparing representatives of both types of systems in a frequently used evaluation protocol. Leveraging the explanatory qualities of rule-based systems, we present a fine-grained evaluation that gives insight into characteristics of the most popular datasets and points out the different strengths and shortcomings of the examined approaches. Our results show that models such as TransE, RESCAL or HolE have problems in solving certain types of completion tasks that can be solved by a rule-based approach with high precision. At the same time, there are other completion tasks that are difficult for rule-based systems. Motivated by these insights, we combine both families of approaches via ensemble learning. The results support our assumption that the two methods complement each other in a beneficial way.</td></tr><tr><td>187</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_36">Investigating Learnability, User Performance, and Preferences of the Path Query Language SemwidgQL Compared to SPARQL</a></td></tr><tr><td colspan=3>In this paper, we present an empirical comparison of user performance and perceived usability for </td></tr><tr><td>188</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_33">Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine</a></td></tr><tr><td colspan=3>Real-time processing of data streams emanating from sensors is becoming a common task in Internet of Things scenarios. The key implementation goal consists in efficiently handling massive incoming data streams and supporting advanced data analytics services like anomaly detection. In an on-going, industrial project, a 24 / 7 available stream processing engine usually faces dynamically changing data and workload characteristics. These changes impact the engine’s performance and reliability. We propose Strider, a hybrid adaptive distributed RDF Stream Processing engine that optimizes logical query plan according to the state of data streams. Strider has been designed to guarantee important industrial properties such as scalability, high availability, fault tolerance, high throughput and acceptable latency. These guarantees are obtained by designing the engine’s architecture with state-of-the-art Apache components such as Spark and Kafka. We highlight the efficiency (</td></tr><tr><td>189</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_31">Entity Comparison in RDF Graphs</a></td></tr><tr><td colspan=3>In many applications, there is an increasing need for the new types of RDF data analysis that are not covered by standard reasoning tasks such as SPARQL query answering. One such important analysis task is entity comparison, i.e., determining what are similarities and differences between two given entities in an RDF graph. For instance, in an RDF graph about drugs, we may want to compare Metamizole and Ibuprofen and automatically find out that they are similar in that they are both analgesics but, in contrast to Metamizole, Ibuprofen also has a considerable anti-inflammatory effect. Entity comparison is a widely used functionality available in many information systems, such as universities or product comparison websites. However, comparison is typically domain-specific and depends on a fixed set of aspects to compare. In this paper, we propose a formal framework for domain-independent entity comparison over RDF graphs. We model similarities and differences between entities as SPARQL queries satisfying certain additional properties, and propose algorithms for computing them.</td></tr><tr><td>190</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_30">Completeness-Aware Rule Learning from Knowledge Graphs</a></td></tr><tr><td colspan=3>Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts. They are widely used in entity recognition, structured search, question answering, and other important tasks. Rule mining is commonly applied to discover patterns in KGs. However, unlike in traditional association rule mining, KGs provide a setting with a high degree of </td></tr><tr><td>191</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_25">Attributed Description Logics: Ontologies for Knowledge Graphs</a></td></tr><tr><td colspan=3>In modelling real-world knowledge, there often arises a need to represent and reason with meta-knowledge. To equip description logics (DLs) for dealing with such ontologies, we enrich DL concepts and roles with finite sets of attribute–value pairs, called annotations, and allow concept inclusions to express constraints on annotations. We show that this may lead to increased complexity or even undecidability, and we identify cases where this increased expressivity can be achieved without incurring increased complexity of reasoning. In particular, we describe a tractable fragment based on the lightweight description logic </td></tr><tr><td>192</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_32">Provenance Information in a Collaborative Knowledge Graph: An Evaluation of Wikidata External References</a></td></tr><tr><td colspan=3>Wikidata is a collaboratively-edited knowledge graph; it expresses knowledge in the form of subject-property-value triples, which can be enhanced with references to add provenance information. Understanding the quality of Wikidata is key to its widespread adoption as a knowledge resource. We analyse one aspect of Wikidata quality, provenance, in terms of relevance and authoritativeness of its external references. We follow a two-staged approach. First, we perform a crowdsourced evaluation of references. Second, we use the judgements collected in the first stage to train a machine learning model to predict reference quality on a large-scale. The features chosen for the models were related to reference editing and the semantics of the triples they referred to. </td></tr><tr><td>193</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_21">Computing FO-Rewritings in </a></td></tr><tr><td colspan=3>A prominent approach to implementing ontology-mediated queries (OMQs) is to rewrite into a first-order query, which is then executed using a conventional SQL database system. We consider the case where the ontology is formulated in the description logic </td></tr><tr><td>194</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_22">A Formal Framework for Comparing Linked Data Fragments</a></td></tr><tr><td colspan=3>The Linked Data Fragment (LDF) framework has been proposed as a uniform view to explore the trade-offs of consuming Linked Data when servers provide (possibly many) different interfaces to access their data. Every such interface has its own particular properties regarding performance, bandwidth needs, caching, etc. Several practical challenges arise. For example, before exposing a new type of LDFs in some server, can we formally say something about how this new LDF interface compares to other interfaces previously implemented in the same server? From the client side, given a client with some restricted capabilities in terms of time constraints, network connection, or computational power, which is the best type of LDFs to complete a given task? Today there are only a few formal theoretical tools to help answer these and other practical questions, and researchers have embarked in solving them mainly by experimentation.</td></tr><tr><td>195</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_26">Reliable Granular References to Changing Linked Data</a></td></tr><tr><td colspan=3>Nanopublications are a concept to represent Linked Data in a granular and provenance-aware manner, which has been successfully applied to a number of scientific datasets. We demonstrated in previous work how we can establish reliable and verifiable identifiers for nanopublications and sets thereof. Further adoption of these techniques, however, was probably hindered by the fact that nanopublications can lead to an explosion in the number of triples due to auxiliary information about the structure of each nanopublication and repetitive provenance and metadata. We demonstrate here that this significant overhead disappears once we take the version history of nanopublication datasets into account, calculate incremental updates, and allow users to deal with the specific subsets they need. We show that the total size and overhead of evolving scientific datasets is reduced, and typical subsets that researchers use for their analyses can be referenced and retrieved efficiently with optimized precision, persistence, and reliability.</td></tr><tr><td>196</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_23">Language-Agnostic Relation Extraction from Wikipedia Abstracts</a></td></tr><tr><td colspan=3>Large-scale knowledge graphs, such as DBpedia, Wikidata, or YAGO, can be enhanced by relation extraction from text, using the data in the knowledge graph as training data, i.e., using </td></tr><tr><td>197</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_28">The </a></td></tr><tr><td colspan=3>Answering queries over a federation of SPARQL endpoints requires combining data from more than one data source. Optimizing queries in such scenarios is particularly challenging not only because of (i) the large variety of possible query execution plans that correctly answer the query but also because (ii) there is only limited access to statistics about schema and instance data of remote sources. To overcome these challenges, most federated query engines rely on heuristics to reduce the space of possible query execution plans or on dynamic programming strategies to produce optimal plans. Nevertheless, these plans may still exhibit a high number of intermediate results or high execution times because of heuristics and inaccurate cost estimations. In this paper, we present </td></tr><tr><td>198</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_27">Cost-Driven Ontology-Based Data Access</a></td></tr><tr><td colspan=3>SPARQL query answering in ontology-based data access (OBDA) is carried out by translating into SQL queries over the data source. Standard translation techniques try to transform the user query into a union of conjunctive queries (UCQ), following the heuristic argument that UCQs can be efficiently evaluated by modern relational database engines. In this work, we show that translating to UCQs is not always the best choice, and that, under certain conditions on the interplay between the ontology, the mappings, and the statistics of the data, alternative translations can be evaluated much more efficiently. To find the best translation, we devise a cost model together with a novel cardinality estimation that takes into account all such OBDA components. Our experiments confirm that </td></tr><tr><td>199</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_19">Challenges of Source Selection in the WoD</a></td></tr><tr><td colspan=3>Federated querying, the idea to execute queries over several distributed knowledge bases, lies at the core of the semantic web vision. To accommodate this vision, SPARQL provides the SERVICE keyword that allows one to allocate sub-queries to servers. In many cases, however, data may be available from multiple sources resulting in a combinatorially growing number of alternative allocations of subqueries to sources. Running a federated query on all possible sources might not be very lucrative from a user’s point of view if extensive execution times or fees are involved in accessing the sources’ data. To address this shortcoming, federated join-cardinality approximation techniques have been proposed to narrow down the number of possible allocations to a few most promising (or results-yielding) ones.</td></tr><tr><td>200</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_20">AMUSE: Multilingual Semantic Parsing for Question Answering over Linked Data</a></td></tr><tr><td colspan=3>The task of answering natural language questions over RDF data has received wide interest in recent years, in particular in the context of the series of QALD benchmarks. The task consists of mapping a natural language question to an executable form, e.g. SPARQL, so that answers from a given KB can be extracted. So far, most systems proposed are (i) monolingual and (ii) rely on a set of hard-coded rules to interpret questions and map them into a SPARQL query. We present the first multilingual QALD pipeline that induces a model from training data for mapping a natural language question into logical form as probabilistic inference. In particular, our approach learns to map universal syntactic dependency representations to a language-independent logical form based on DUDES (Dependency-based Underspecified Discourse Representation Structures) that are then mapped to a SPARQL query as a deterministic second step. Our model builds on factor graphs that rely on features extracted from the dependency graph and corresponding semantic representations. We rely on approximate inference techniques, Markov Chain Monte Carlo methods in particular, as well as Sample Rank to update parameters using a ranking objective. Our focus lies on developing methods that overcome the lexical gap and present a novel combination of machine translation and word embedding approaches for this purpose. As a proof of concept for our approach, we evaluate our approach on the QALD-6 datasets for English, German & Spanish.</td></tr><tr><td>201</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_18">Meta Structures in Knowledge Graphs</a></td></tr><tr><td colspan=3>This paper investigates meta structures, schema-level graphs that abstract connectivity information among a set of entities in a knowledge graph. Meta structures are useful in a variety of knowledge discovery tasks ranging from relatedness explanation to data retrieval. We formalize the meta structure computation problem and devise efficient automata-based algorithms. We introduce a meta structure-based relevance measure, which can retrieve entities related to those in input. We implemented our machineries in a visual tool called MEKoNG. We report on an extensive experimental evaluation, which confirms the suitability of our proposal from both the efficiency and effectiveness point of view.</td></tr><tr><td>202</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_17">Learning Commonalities in SPARQL</a></td></tr><tr><td colspan=3>Finding the commonalities between descriptions of data or knowledge is a foundational reasoning problem of Machine Learning. It was formalized in the early 70’s as computing a </td></tr><tr><td>203</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_24">Alignment Cubes: Towards Interactive Visual Exploration and Evaluation of Multiple Ontology Alignments</a></td></tr><tr><td colspan=3>Ontology alignment is an area of active research where many algorithms and approaches are being developed. Their performance is usually evaluated by comparing the produced alignments to a reference alignment in terms of precision, recall and F-measure. These measures, however, only provide an overall assessment of the quality of the alignments, but do not reveal differences and commonalities between alignments at a finer-grained level such as, e.g., regions or individual mappings. Furthermore, reference alignments are often unavailable, which makes the comparative exploration of alignments at different levels of granularity even more important. Making such comparisons efficient calls for a “human-in-the-loop” approach, best supported through interactive visual representations of alignments. Our approach extends a recent tool, Matrix Cubes, used for visualizing dense dynamic networks. We first identify use cases for ontology alignment evaluation that can benefit from interactive visualization, and then detail how our </td></tr><tr><td>204</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_14">Practical Update Management in Ontology-Based Data Access</a></td></tr><tr><td colspan=3>Ontology-based Data Access (OBDA) is gaining importance both scientifically and practically. However, little attention has been paid so far to the problem of updating OBDA systems. This is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level, while maintaining the independence of the data sources. In this paper, we propose mechanisms to properly handle updates in this context. We show that updating data both at the ontology and source level is first-order rewritable. We also provide a practical implementation of such updating mechanisms based on non-recursive Datalog.</td></tr><tr><td>205</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_16">Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings</a></td></tr><tr><td colspan=3>Web tables constitute valuable sources of information for various applications, ranging from Web search to Knowledge Base (KB) augmentation. An underlying common requirement is to annotate the rows of Web tables with semantically rich descriptions of entities published in Web KBs. In this paper, we evaluate three unsupervised annotation methods: (a) a </td></tr><tr><td>206</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_13">LDScript: A Linked Data Script Language</a></td></tr><tr><td colspan=3>In addition to the existing standards dedicated to representation or querying, Semantic Web programmers could really benefit from a dedicated programming language enabling them to directly define functions on RDF terms, RDF graphs or SPARQL results. This is especially the case, for instance, when defining SPARQL extension functions. The ability to capitalize complex SPARQL filter expressions into extension functions or to define and reuse dedicated aggregates are real cases where a dedicated language can support modularity and maintenance of the code. Other families of use cases include the definition of </td></tr><tr><td>207</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_15">Computing Authoring Tests from Competency Questions: Experimental Validation</a></td></tr><tr><td colspan=3>This paper explores whether Authoring Tests derived from Competency Questions accurately represent the expectations of ontology authors. In earlier work we proposed that an ontology authoring interface can be improved by allowing the interface to test whether a given Competency Question (CQ) is able to be answered by the ontology at a given stage of its construction, an approach known as CQ-driven Ontology Authoring (CQOA). The experiments presented in the present paper suggest that CQOA’s understanding of CQs matches users’ understanding quite well, especially for inexperienced ontology authors.</td></tr><tr><td>208</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_10">Tractable Query Answering for Expressive Ontologies and Existential Rules</a></td></tr><tr><td colspan=3>The disjunctive skolem chase is a sound and complete (albeit non-terminating) algorithm that can be used to solve conjunctive query answering over DL ontologies and programs with disjunctive existential rules. Even though acyclicity notions can be used to ensure chase termination for a large subset of real-world knowledge bases, the complexity of reasoning over acyclic theories still remains high. Hence, we study several restrictions which not only guarantee chase termination but also ensure polynomiality. We include an evaluation that shows that almost all acyclic DL ontologies do indeed satisfy these general restrictions.</td></tr><tr><td>209</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_11">Zooming in on Ontologies: Minimal Modules and Best Excerpts</a></td></tr><tr><td colspan=3>Ensuring access to the most relevant knowledge contained in large ontologies has been identified as an important challenge. To this end, minimal modules (sub-ontologies that preserve all entailments over a given vocabulary) and excerpts (certain, small number of axioms that best capture the knowledge regarding the vocabulary by allowing for a degree of semantic loss) have been proposed. In this paper, we introduce the notion of subsumption justification as an extension of justification (a minimal set of axioms needed to preserve a logical consequence) to capture the subsumption knowledge between a term and all other terms in the vocabulary. We present algorithms for computing subsumption justifications based on a simulation notion developed for the problem of deciding the logical difference between ontologies. We show how subsumption justifications can be used to obtain minimal modules and to compute best excerpts by additionally employing a partial Max-SAT solver. This yields two state-of-the-art methods for computing all minimal modules and all best excerpts, which we evaluate over large biomedical ontologies.</td></tr><tr><td>210</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_12">Global RDF Vector Space Embeddings</a></td></tr><tr><td colspan=3>Vector space embeddings have been shown to perform well when using RDF data in data mining and machine learning tasks. Existing approaches, such as RDF2Vec, use </td></tr><tr><td>211</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_7">Semantics and Validation of Shapes Schemas for RDF</a></td></tr><tr><td colspan=3>We present a formal semantics and proof of soundness for shapes schemas, an expressive schema language for RDF graphs that is the foundation of Shape Expressions Language 2.0. It can be used to describe the vocabulary and the structure of an RDF graph, and to constrain the admissible properties and values for nodes in that graph. The language defines a typing mechanism called shapes against which nodes of the graph can be checked. It includes an algebraic grouping operator, a choice operator and cardinality constraints for the number of allowed occurrences of a property. Shapes can be combined using Boolean operators, and can use possibly recursive references to other shapes.</td></tr><tr><td>212</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_8">Temporal Query Answering in DL-Lite over Inconsistent Data</a></td></tr><tr><td colspan=3>In ontology-based systems that process data stemming from different sources and that is received over time, as in context-aware systems, reasoning needs to cope with the temporal dimension and should be resilient against inconsistencies in the data. Motivated by such settings, this paper addresses the problem of handling inconsistent data in a temporal version of ontology-based query answering. We consider a recently proposed temporal query language that combines conjunctive queries with operators of propositional linear temporal logic and extend to this setting three inconsistency-tolerant semantics that have been introduced for querying inconsistent description logic knowledge bases. We investigate their complexity for DL-Lite</td></tr><tr><td>213</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_9">Semantic Wide and Deep Learning for Detecting Crisis-Information Categories on Social Media</a></td></tr><tr><td colspan=3>When crises hit, many flog to social media to share or consume information related to the event. Social media posts during crises tend to provide valuable reports on affected people, donation offers, help requests, advice provision, etc. Automatically identifying the category of information (e.g., </td></tr><tr><td>214</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_1">Multi-label Based Learning for Better Multi-criteria Ranking of Ontology Reasoners</a></td></tr><tr><td colspan=3>A growing number of highly optimized reasoning algorithms have been developed to allow inference tasks on expressive ontology languages such as OWL(DL). Nevertheless, there is broad agreement that a reasoner could be optimized for some, but not all the ontologies. This particular fact makes it hard to select the best performing reasoner to handle a given ontology, especially for novice users. In this paper, we present a novel method to support the selection ontology reasoners. Our method generates a recommendation in the form of reasoner ranking. The efficiency as well as the correctness are our main ranking criteria. Our solution combines and adjusts multi-label classification and multi-target regression techniques. A large collection of ontologies and 10 well-known reasoners are studied. The experimental results show that the proposed method performs significantly better than several state-of-the-art ranking solutions. Furthermore, it proves that our introduced ranking method could effectively be evolved to a competitive meta-reasoner.</td></tr><tr><td>215</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_2">The Efficacy of OWL and DL on User Understanding of Axioms and Their Entailments</a></td></tr><tr><td colspan=3>OWL is recognized as the de facto standard notation for ontology engineering. The Manchester OWL Syntax (MOS) was developed as an alternative to symbolic description logic (DL) and it is believed to be more effective for users. This paper sets out to test that belief from two perspectives by evaluating how accurately and quickly people </td></tr><tr><td>216</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_3">A Decidable Very Expressive Description Logic for Databases</a></td></tr><tr><td colspan=3>We introduce </td></tr><tr><td>217</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_6">Expressive Stream Reasoning with Laser</a></td></tr><tr><td colspan=3>An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases.</td></tr><tr><td>218</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_4">Improving Visual Relationship Detection Using Semantic Modeling of Scene Descriptions</a></td></tr><tr><td colspan=3>Structured scene descriptions of images are useful for the automatic processing and querying of large image databases. We show how the combination of a statistical semantic model and a visual model can improve on the task of mapping images to their associated scene description. In this paper we consider scene descriptions which are represented as a set of triples (</td></tr><tr><td>219</td><td>2017</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68288-4_5">An Empirical Study on How the Distribution of Ontologies Affects Reasoning on the Web</a></td></tr><tr><td colspan=3>The Web of Data is an inherently distributed environment where ontologies are located in (physically) remote locations and are subject to constant changes. Reasoning is affected by these changes, but the extent and significance of this dependency is not well-studied yet. To address this problem, this paper presents an empirical study on how the distribution of ontological data on the Web affects the outcome of reasoning. We study (1) to what degree datasets depend on external ontologies and (2) to what extent the inclusion of additional ontological information via IRI de-referencing and the </td></tr><tr><td>220</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_39">Unsupervised Entity Resolution on Multi-type Graphs</a></td></tr><tr><td colspan=3>Entity resolution is the task of identifying all mentions that represent the same real-world entity within a knowledge base or across multiple knowledge bases. We address the problem of performing entity resolution on RDF graphs containing multiple types of nodes, using the links between instances of different types to improve the accuracy. For example, in a graph of products and manufacturers the goal is to resolve all the products and all the manufacturers. We formulate this problem as a multi-type graph summarization problem, which involves clustering the nodes in each type that refer to the same entity into one super node and creating weighted links among super nodes that summarize the inter-cluster links in the original graph. Experiments show that the proposed approach outperforms several state-of-the-art generic entity resolution approaches, especially in data sets with missing values and one-to-many, many-to-many relations.</td></tr><tr><td>221</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_38">Context-Free Path Queries on RDF Graphs</a></td></tr><tr><td colspan=3>Navigational graph queries are an important class of queries that can extract implicit binary relations over the nodes of input graphs. Most of the navigational query languages used in the RDF community, e.g. property paths in W3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the regular expressions. It is known that regular expressions have limited expressivity; for instance, some natural queries, like </td></tr><tr><td>222</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_37">A Knowledge Base Approach to Cross-Lingual Keyword Query Interpretation</a></td></tr><tr><td colspan=3>The amount of entities in large knowledge bases available on the Web has been increasing rapidly, making it possible to propose new ways of intelligent information access. In addition, there is an impending need for technologies that can enable cross-lingual information access. As a simple and intuitive way of specifying information needs, keyword queries enjoy widespread usage, but suffer from the challenges including </td></tr><tr><td>223</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_36">A Probabilistic Model for Time-Aware Entity Recommendation</a></td></tr><tr><td colspan=3>In recent years, there has been an increasing effort to develop techniques for related entity recommendation, where the task is to retrieve a ranked list of related entities given a keyword query. Another trend in the area of information retrieval (IR) is to take temporal aspects of a given query into account when assessing the relevance of documents. However, while this has become an established functionality in document search engines, the significance of time has not yet been recognized for entity recommendation. In this paper, we address this gap by introducing the task of </td></tr><tr><td>224</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_34">Integrating Medical Scientific Knowledge with the Semantically Quantified Self</a></td></tr><tr><td colspan=3>The assessment of risk in medicine is a crucial task, and depends on scientific knowledge derived by systematic clinical studies on factors affecting health, as well as on particular knowledge about the current status of a particular patient. Existing non-semantic risk prediction tools are typically based on hardcoded scientific knowledge, and only cover a very limited range of patient states. This makes them rapidly out of date, and limited in application, particularly for patients with multiple co-occurring conditions. In this work we propose an integration of Semantic Web and Quantified Self technologies to create a framework for calculating clinical risk predictions for patients based on self-gathered biometric data. This framework relies on generic, reusable ontologies for representing clinical risk, and sensor readings, and reasoning to support the integration of data represented according to these ontologies. The implemented framework shows a wide range of advantages over existing risk calculation.</td></tr><tr><td>225</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_35">Learning to Assess Linked Data Relationships Using Genetic Programming</a></td></tr><tr><td colspan=3>The goal of this work is to learn a measure supporting the detection of strong relationships between Linked Data entities. Such relationships can be represented as paths of entities and properties, and can be obtained through a blind graph search process traversing Linked Data. The challenge here is therefore the design of a cost-function that is able to detect the strongest relationship between two given entities, by objectively assessing the value of a given path. To achieve this, we use a Genetic Programming approach in a supervised learning method to generate path evaluation functions that compare well with human evaluations. We show how such a cost-function can be generated only using basic topological features of the nodes of the paths as they are being traversed (i.e. without knowledge of the whole graph), and how it can be improved through introducing a very small amount of knowledge about the vocabularies of the properties that connect nodes in the graph.</td></tr><tr><td>226</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_32">Can You Imagine... A Language for Combinatorial Creativity?</a></td></tr><tr><td colspan=3>Combinatorial creativity combines existing concepts in a novel way in order to produce new concepts. For example, we can imagine jewelry that measures blood pressure. For this, we would combine the concept of jewelry with the capabilities of medical devices. In this paper, we concentrate on creating new concepts in the description logic </td></tr><tr><td>227</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_31">SPARQL-to-SQL on Internet of Things Databases and Streams</a></td></tr><tr><td colspan=3>To realise a semantic Web of Things, the challenge of achieving efficient Resource Description Format (RDF) storage and SPARQL query performance on Internet of Things (IoT) devices with limited resources has to be addressed. State-of-the-art SPARQL-to-SQL engines have been shown to outperform RDF stores on some benchmarks. In this paper, we describe an optimisation to the SPARQL-to-SQL approach, based on a study of time-series IoT data structures, that employs metadata abstraction and efficient translation by reusing existing SPARQL engines to produce Linked Data ‘just-in-time’. We evaluate our approach against RDF stores, state-of-the-art SPARQL-to-SQL engines and streaming SPARQL engines, in the context of IoT data and scenarios. We show that storage efficiency, with succinct row storage, and query performance can be improved from 2 times to 3 orders of magnitude.</td></tr><tr><td>228</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_33">Leveraging Linked Data to Discover Semantic Relations Within Data Sources</a></td></tr><tr><td colspan=3>Mapping data to a shared domain ontology is a key step in publishing semantic content on the Web. Most of the work on automatically mapping structured and semi-structured sources to ontologies focuses on semantic labeling, i.e., annotating data fields with ontology classes and/or properties. However, a precise mapping that fully recovers the intended meaning of the data needs to describe the semantic relations between the data fields too. We present a novel approach to automatically discover the semantic relations within a given data source. We mine the small graph patterns occurring in Linked Open Data and combine them to build a graph that will be used to infer semantic relations. We evaluated our approach on datasets from different domains. Mining patterns of maximum length five, our method achieves an average precision of 75 % and recall of 77 % for a dataset with very complex mappings to the domain ontology, increasing up to 86 % and 82 %, respectively, for simpler ontologies and mappings.</td></tr><tr><td>229</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_30">RDF2Vec: RDF Graph Embeddings for Data Mining</a></td></tr><tr><td colspan=3>Linked Open Data has been recognized as a valuable source for background information in data mining. However, most data mining tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. Our evaluation shows that such vector representations outperform existing techniques for the propositionalization of RDF graphs on a variety of different predictive machine learning tasks, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.</td></tr><tr><td>230</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_29">Distributed RDF Query Answering with Dynamic Data Exchange</a></td></tr><tr><td colspan=3>Evaluating joins over RDF data stored in a shared-nothing server cluster is key to processing truly large RDF datasets. To the best of our knowledge, the existing approaches use a variant of the </td></tr><tr><td>231</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_27">Semantic Labeling: A Domain-Independent Approach</a></td></tr><tr><td colspan=3>Semantic labeling is the process of mapping attributes in data sources to classes in an ontology and is a necessary step in heterogeneous data integration. Variations in data formats, attribute names and even ranges of values of data make this a very challenging task. In this paper, we present a novel domain-independent approach to automatic semantic labeling that uses machine learning techniques. Previous approaches use machine learning to learn a model that extracts features related to the data of a domain, which requires the model to be re-trained for every new domain. Our solution uses similarity metrics as features to compare against labeled domain data and learns a matching function to infer the correct semantic labels for data. Since our approach depends on the learned similarity metrics but not the data itself, it is domain-independent and only needs to be trained once to work effectively across multiple domains. In our evaluation, our approach achieves higher accuracy than other approaches, even when the learned models are trained on domains other than the test domain.</td></tr><tr><td>232</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_23">Ontologies for Knowledge Graphs: Breaking the Rules</a></td></tr><tr><td colspan=3>Large-scale knowledge graphs (KGs) are widely used in industry and academia, and provide excellent use-cases for ontologies. We find, however, that popular ontology languages, such as OWL and Datalog, cannot express even the most basic relationships on the normalised data format of KGs. Existential rules are more powerful, but may make reasoning undecidable. Normalising them to suit KGs often also destroys syntactic restrictions that ensure decidability and low complexity. We study this issue for several classes of existential rules and derive new syntactic criteria to recognise well-behaved rule-based ontologies over KGs.</td></tr><tr><td>233</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_25">Semantic Sensitive Simultaneous Tensor Factorization</a></td></tr><tr><td colspan=3>The semantics distributed over large-scale knowledge bases can be used to intermediate heterogeneous users’ activity logs created in services; such information can be used to improve applications that can help users to decide the next activities/services. Since user activities can be represented in terms of relationships involving three or more things (e.g. a user tags movie items on a webpage), tensors are an attractive approach to represent them. The recently introduced Semantic Sensitive Tensor Factorization (SSTF) is promising as it achieves high accuracy in predicting users’ activities by basing tensor factorization on the semantics behind objects (e.g. item categories). However, SSTF currently focuses on the factorization of a tensor for a single service and thus has two problems: (1) </td></tr><tr><td>234</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_22">Algebraic Calculi for Weighted Ontology Alignments</a></td></tr><tr><td colspan=3>Alignments between ontologies usually come with numerical attributes expressing the confidence of each correspondence. Semantics supporting such confidences must generalise the semantics of alignments without confidence. There exists a semantics which satisfies this but introduces a discontinuity between weighted and non-weighted interpretations. Moreover, it does not provide a calculus for reasoning with weighted ontology alignments. This paper introduces a calculus for such alignments. It is given by an infinite relation-type algebra, the elements of which are weighted taxonomic relations. In addition, it approximates the non-weighted case in a continuous manner.</td></tr><tr><td>235</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_24">An Extensible Linear Approach for Holistic Ontology Matching</a></td></tr><tr><td colspan=3>Resolving the semantic heterogeneity in the semantic web requires finding correspondences between ontologies describing resources. In particular, with the explosive growth of data sets in the Linked Open Data, linking multiple vocabularies and ontologies simultaneously, known as holistic matching problem, becomes necessary. Currently, most state-of-the-art matching approaches are limited to pairwise matching. In this paper, we propose a holistic ontology matching approach that is modeled through a linear program extending the maximum-weighted graph matching problem with linear constraints (cardinality, structural, and coherence constraints). Our approach guarantees the optimal solution with mostly coherent alignments. To evaluate our proposal, we discuss the results of experiments performed on the Conference track of the OAEI 2015, under both holistic and pairwise matching settings.</td></tr><tr><td>236</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_26">Multi-level Semantic Labelling of Numerical Values</a></td></tr><tr><td colspan=3>With the success of Open Data a huge amount of tabular data sources became available that could potentially be mapped and linked into the Web of (Linked) Data. Most existing approaches to “semantically label” such tabular data rely on mappings of textual information to classes, properties, or instances in RDF knowledge bases in order to link – and eventually transform – tabular data into RDF. However, as we will illustrate, Open Data tables typically contain a large portion of numerical columns and/or non-textual headers; therefore solutions that solely focus on textual “cues” are only partially applicable for mapping such data sources. We propose an approach to find and rank candidates of semantic labels and context descriptions for a given bag of numerical values. To this end, we apply a hierarchical clustering over information taken from DBpedia to build a background knowledge graph of possible “semantic contexts” for bags of numerical values, over which we perform a nearest neighbour search to rank the most likely candidates. Our evaluation shows that our approach can assign fine-grained semantic labels, when there is enough supporting evidence in the background knowledge graph. In other cases, our approach can nevertheless assign high level contexts to the data, which could potentially be used in combination with other approaches to narrow down the search space of possible labels.</td></tr><tr><td>237</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_28">Exploiting Emergent Schemas to Make RDF Systems More Efficient</a></td></tr><tr><td colspan=3>We build on our earlier finding that more than 95 % of the triples in actual RDF triple graphs have a remarkably tabular structure, whose schema does not necessarily follow from explicit metadata such as ontologies, but for which an RDF store can automatically derive by looking at the data using so-called “emergent schema” detection techniques. In this paper we investigate how computers and in particular RDF stores can take advantage from this emergent schema to more compactly store RDF data and more efficiently optimize and execute SPARQL queries. To this end, we contribute techniques for efficient emergent schema aware RDF storage and new query operator algorithms for emergent schema aware scans and joins. In all, these techniques allow RDF schema processors fully catch up with relational database techniques in terms of rich physical database design options and efficiency, without requiring a rigid upfront schema structure definition.</td></tr><tr><td>238</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_21">Optimizing Aggregate SPARQL Queries Using Materialized RDF Views</a></td></tr><tr><td colspan=3>During recent years, more and more data has been published as native RDF datasets. In this setup, both the size of the datasets and the need to process aggregate queries represent challenges for standard SPARQL query processing techniques. To overcome these limitations, materialized views can be created and used as a source of precomputed partial results during query processing. However, materialized view techniques as proposed for relational databases do not support RDF specifics, such as incompleteness and the need to support implicit (derived) information. To overcome these challenges, this paper proposes MARVEL (MAterialized Rdf Views with Entailment and incompLetness). The approach consists of a view selection algorithm based on an associated RDF-specific cost model, a view definition syntax, and an algorithm for rewriting SPARQL queries using materialized RDF views. The experimental evaluation shows that MARVEL can improve query response time by more than an order of magnitude while effectively handling RDF specifics.</td></tr><tr><td>239</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_19">Walking Without a Map: Ranking-Based Traversal for Querying Linked Data</a></td></tr><tr><td colspan=3>The traversal-based approach to execute queries over Linked Data on the WWW fetches data by traversing data links and, thus, is able to make use of up-to-date data from initially unknown data sources. While the downside of this approach is the delay before the query engine completes a query execution, user perceived response time may be improved significantly by returning as many elements of the result set as soon as possible. To this end, the query engine requires a traversal strategy that enables the engine to fetch result-relevant data as early as possible. The challenge for such a strategy is that the query engine does not know a priori which of the data sources discovered during the query execution will contain result-relevant data. In this paper, we investigate 14 different approaches to rank traversal steps and achieve a variety of traversal strategies. We experimentally study their impact on response times and compare them to a baseline that resembles a breadth-first traversal. While our experiments show that some of the approaches can achieve noteworthy improvements over the baseline in a significant number of cases, we also observe that for every approach, there is a non-negligible chance to achieve response times that are worse than the baseline.</td></tr><tr><td>240</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_20">CubeQA—Question Answering on RDF Data Cubes</a></td></tr><tr><td colspan=3>Statistical data in the form of RDF Data Cubes is becoming increasingly valuable as it influences decisions in areas such as health care, policy and finance. While a growing amount is becoming freely available through the open data movement, this data is opaque to laypersons. Semantic Question Answering (SQA) technologies provide intuitive access via free-form natural language queries but general SQA systems cannot process RDF Data Cubes. On the intersection between RDF Data Cubes and SQA, we create a new subfield of SQA, called RDCQA. We create an RDQCA benchmark as task 3 of the QALD-6 evaluation challenge, to stimulate further research and enable quantitative comparison between RDCQA systems. We design and evaluate the domain independent CubeQA algorithm, which is the first RDCQA system and achieves a global </td></tr><tr><td>241</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_18">Predicting Energy Consumption of Ontology Reasoning over Mobile Devices</a></td></tr><tr><td colspan=3>The unprecedented growth in mobile devices, combined with advances in Semantic Web (SW) Technologies, has given birth to opportunities for more intelligent systems on-the-go. Limited resources of mobile devices demand approaches that make mobile reasoning more applicable. While Mobile-Cloud integration is a promising method for harnessing the power of semantic technologies in the mobile infrastructure, it is an open question how to decide when to reason over ontologies on mobile devices. In this paper, we introduce an energy consumption prediction mechanism for ontology reasoning on mobile devices that allows an analysis of the feasibility of performing an ontology reasoning on a mobile device with respect to energy consumption. The developed prediction model contributes to mobile–cloud integration and helps to improve further developments in semantic reasoning in general.</td></tr><tr><td>242</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_16">Planning Ahead: Stream-Driven Linked-Data Access Under Update-Budget Constraints</a></td></tr><tr><td colspan=3>Data stream applications are becoming increasingly popular on the web. In these applications, one query pattern is especially prominent: a join between a continuous data stream and some background data (BGD). Oftentimes, the target BGD is large, maintained externally, changing slowly, and costly to query (both in terms of time and money). Hence, practical applications usually maintain a local (cached) view of the relevant BGD. Given that these caches are not updated as the original BGD, they should be refreshed under realistic budget constraints (in terms of latency, computation time, and possibly financial cost) to avoid stale data leading to wrong answers. This paper proposes to model the join between streams and the BGD as a bipartite graph. By exploiting the graph structure, we keep the quality of results good enough without refreshing the entire cache for each evaluation. We also introduce two extensions to this method: first, we consider a continuous join between recent portions of a data stream and some BGD to focus on updates that have the longest effect. Second, we consider the future impact of a query to the BGD by proposing to delay some updates to provide fresher answers in future. By extending an existing stream processor with the proposed policies, we empirically show that we can improve result freshness by 93 % over baseline algorithms such as Random Selection or Least Recently Updated.</td></tr><tr><td>243</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_17">Explicit Query Interpretation and Diversification for Context-Driven Concept Search Across Ontologies</a></td></tr><tr><td colspan=3>Finding relevant concepts from a corpus of ontologies is useful in many scenarios, such as document classification, web page annotation, and automatic ontology population. Many millions of concepts are contained in a large number of ontologies across diverse domains. A SPARQL-based query demands the knowledge of the structure of ontologies and the query language, whereas user-friendlier and, simpler keyword-based approaches suffer from false positives. This is because concept descriptions in ontologies may be ambiguous and may overlap. In this paper, we propose a keyword-based concept search framework, which (1) exploits the structure and semantics in ontologies, by constructing </td></tr><tr><td>244</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_12">Are Names Meaningful? Quantifying Social Meaning on the Semantic Web</a></td></tr><tr><td colspan=3>According to its model-theoretic semantics, Semantic Web IRIs are individual constants or predicate letters whose names are chosen arbitrarily and carry no formal meaning. At the same time it is a well-known aspect of Semantic Web </td></tr><tr><td>245</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_14">Seed, an End-User Text Composition Tool for the Semantic Web</a></td></tr><tr><td colspan=3>Despite developments of Semantic Web-enabling technologies, the gap between non-expert end-users and the Semantic Web still exists. In the field of semantic content authoring, tools for interacting with semantic content remain directed at highly trained individuals. This adds to the challenges of bringing user-generated content into the Semantic Web. In this paper, we present </td></tr><tr><td>246</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_15">Exception-Enriched Rule Learning from Knowledge Graphs</a></td></tr><tr><td colspan=3>Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. To overcome this problem, we present a method for effective revision of learned Horn rules by adding exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction.</td></tr><tr><td>247</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_11">Updating DL-Lite Ontologies Through First-Order Queries</a></td></tr><tr><td colspan=3>In this paper we study instance-level update in </td></tr><tr><td>248</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_13">User Validation in Ontology Alignment</a></td></tr><tr><td colspan=3>User validation is one of the challenges facing the ontology alignment community, as there are limits to the quality of automated alignment algorithms. In this paper we present a broad study on user validation of ontology alignments that encompasses three distinct but interrelated aspects: the profile of the user, the services of the alignment system, and its user interface. We discuss key issues pertaining to the alignment validation process under each of these aspects, and provide an overview of how current systems address them. Finally, we use experiments from the Interactive Matching track of the Ontology Alignment Evaluation Initiative (OAEI) 2015 to assess the impact of errors in alignment validation, and how systems cope with them as function of their services.</td></tr><tr><td>249</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_8">Efficient Algorithms for Association Finding and Frequent Association Pattern Mining</a></td></tr><tr><td colspan=3>Finding associations between entities is a common information need in many areas. It has been facilitated by the increasing amount of graph-structured data on the Web describing relations between entities. In this paper, we define an association connecting multiple entities in a graph as a minimal connected subgraph containing all of them. We propose an efficient graph search algorithm for finding associations, which prunes the search space by exploiting distances between entities computed based on a distance oracle. Having found a possibly large group of associations, we propose to mine frequent association patterns as a conceptual abstract summarizing notable subgroups to be explored, and present an efficient mining algorithm based on canonical codes and partitions. Extensive experiments on large, real RDF datasets demonstrate the efficiency of the proposed algorithms.</td></tr><tr><td>250</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_10">Knowledge Representation on the Web Revisited: The Case for Prototypes</a></td></tr><tr><td colspan=3>Recently, RDF and OWL have become the most common knowledge representation languages in use on the Web, propelled by the recommendation of the W3C. In this paper we examine an alternative way to represent knowledge based on Prototypes. This Prototype-based representation has different properties, which we argue to be more suitable for data sharing and reuse on the Web. Prototypes avoid the distinction between classes and instances and provide a means for object-based data sharing and reuse.</td></tr><tr><td>251</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_9">A Reuse-Based Annotation Approach for Medical Documents</a></td></tr><tr><td colspan=3>Annotations are useful to semantically enrich documents and other datasets with concepts of standardized vocabularies and ontologies. In the medical domain, many documents are not annotated at all and manual annotation is a difficult process making automatic annotation methods highly desirable to support human annotators. We propose a reuse-based annotation approach that utilizes previous annotations to annotate similar medical documents. The approach clusters items in documents such as medical forms according to previous ontology-based annotations and uses these clusters to determine candidate annotations for new items. The final annotations are selected according to a new context-based strategy that considers the co-occurrence and semantic relatedness of annotating concepts. The evaluation based on previous UMLS annotations of medical forms shows that the new approaches outperform a baseline approach as well as the use of the MetaMap tool for finding UMLS concepts in medical documents.</td></tr><tr><td>252</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_7">WebBrain: Joint Neural Learning of Large-Scale Commonsense Knowledge</a></td></tr><tr><td colspan=3>Despite the emergence and growth of numerous large knowledge graphs, many basic and important facts about our everyday world are not readily available on the Web. To address this, we present WebBrain, a new approach for harvesting commonsense knowledge that relies on joint learning from Web-scale data to fill gaps in the knowledge acquisition. We train a neural network model to learn relations based on large numbers of textual patterns found on the Web. At the same time, the model learns vector representations of general word semantics. This joint approach allows us to generalize beyond the explicitly extracted information. Experiments show that we can obtain representations of words that reflect their semantics, yet also allow us to capture conceptual relationships and commonsense knowledge.</td></tr><tr><td>253</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_6">Containment of Expressive SPARQL Navigational Queries</a></td></tr><tr><td colspan=3>Query containment is one of the building block of query optimization techniques. In the relational world, query containment is a well-studied problem. At the same time it is well-understood that relational queries are not enough to cope with graph-structured data, where one is interested in expressing queries that capture </td></tr><tr><td>254</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_3">Ontop of Geospatial Databases</a></td></tr><tr><td colspan=3>We propose an OBDA approach for accessing geospatial data stored in relational databases, using the OGC standard GeoSPARQL and R2RML or OBDA mappings. We introduce extensions to an existing SPARQL-to-SQL translation method to support GeoSPARQL features. We describe the implementation of our approach in the system ontop-spatial, an extension of the OBDA system Ontop for creating virtual geospatial RDF graphs on top of geospatial relational databases. We present an experimental evaluation of our system using and extending a state-of-the-art benchmark. To measure the performance of our system, we compare it to a state-of-the-art geospatial RDF store and confirm its efficiency.</td></tr><tr><td>255</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_4">Expressive Multi-level Modeling for the Semantic Web</a></td></tr><tr><td colspan=3>In several subject domains, classes themselves may be subject to categorization, resulting in classes of classes (or “metaclasses”). When representing these domains, one needs to capture not only entities of different classification levels, but also their (intricate) relations. We observe that this is challenging in current Semantic Web languages, as there is little support to guide the modeler in producing correct multi-level ontologies, especially because of the nuances in the constraints that apply to entities of different classification levels and their relations. In order to address these representation challenges, we propose a vocabulary that can be used as a basis for multi-level ontologies in OWL along with a number of integrity constraints to prevent the construction of inconsistent models. In this process we employ an axiomatic theory called MLT (a Multi-Level Modeling Theory).</td></tr><tr><td>256</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_5">A Practical Acyclicity Notion for Query Answering Over </a></td></tr><tr><td colspan=3>Conjunctive query answering over expressive Horn Description Logic ontologies is a relevant and challenging problem which, in some cases, can be addressed by application of the chase algorithm. In this paper, we define a novel acyclicity notion which provides a sufficient condition for termination of the restricted chase over </td></tr><tr><td>257</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_1">Structuring Linked Data Search Results Using Probabilistic Soft Logic</a></td></tr><tr><td colspan=3>On-the-fly generation of integrated representations of Linked Data (LD) search results is challenging because it requires successfully automating a number of complex subtasks, such as structure inference and matching of both instances and concepts, each of which gives rise to uncertain outcomes. Such uncertainty is unavoidable given the semantically heterogeneous nature of web sources, including LD ones. This paper approaches the problem of structuring LD search results as an evidence-based one. In particular, the paper shows how one formalism (viz., probabilistic soft logic (PSL)) can be exploited to assimilate different sources of evidence in a principled way and to beneficial effect for users. The paper considers syntactic evidence derived from matching algorithms, semantic evidence derived from LD vocabularies, and user evidence, in the form of feedback. The main contributions are: sets of PSL rules that model the uniform assimilation of diverse kinds of evidence, an empirical evaluation of how the resulting PSL programs perform in terms of their ability to infer structure for integrating LD search results, and, finally, a concrete example of how populating such inferred structures for presentation to the end user is beneficial, besides enabling the collection of feedback whose assimilation further improves search result presentation.</td></tr><tr><td>258</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-46523-4_2">The Multiset Semantics of SPARQL Patterns</a></td></tr><tr><td colspan=3>The paper determines the algebraic and logic structure of the multiset semantics of the core patterns of SPARQL. We prove that the fragment formed by AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both, the intuitive multiset relational algebra (projection, selection, natural join, arithmetic union and except), and the multiset non-recursive Datalog with safe negation.</td></tr><tr><td>259</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_6">Identifying Poorly-Defined Concepts in WordNet with Graph Metrics</a></td></tr><tr><td colspan=3>Princeton WordNet is the most widely-used lexical resource in natural language processing and continues to provide a gold standard model of semantics. However, there are still significant quality issues with the resource and these affect the performance of all NLP systems built on this resource. One major issue is that many nodes are insufficiently defined and new links need to be added to increase performance in NLP. We combine the use of graph-based metrics with measures of ambiguity in order to predict which synsets are difficult for word sense disambiguation, a major NLP task, which is dependent on good lexical information. We show that this method allows use to find poorly defined nodes with a 89.9% precision, which would assist manual annotators to focus on improving the most in-need parts of the WordNet graph.</td></tr><tr><td>260</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_8">Chainable and Extendable Knowledge Integration Web Services</a></td></tr><tr><td colspan=3>This paper introduces the current state of the FREME framework. The paper puts FREME into the context of linguistic linked data and related approaches of multilingual and semantic processing. In addition, we focus on two specific aspects of FREME: the FREME NER e-Service, and chaining of FREME e-Services. We believe that the flexible and distributed combination of e-Services bears a potential for their mutual improvement. The FREME framework is an open source software available for free download (</td></tr><tr><td>261</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_7">Extracting Process Graphs from Medical Text Data</a></td></tr><tr><td colspan=3>In this paper a natural language processing workflow to extract sequential activities from large collections of medical text documents is developed. A graph-based data structure is introduced to merge extracted sequences which contain similar activities in order to build a global graph on procedures which are described in documents on similar topics or tasks. The method describes an information extraction process which will, in the future, enrich or create knowledge bases for process models or activity sequences for the medical domain.</td></tr><tr><td>262</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_4">Wikipedia and DBpedia for Media - Managing Audiovisual Resources in Their Semantic Context</a></td></tr><tr><td colspan=3>
The EBU, NRK and VRT are three European media companies. The EBU is the largest association of broadcasters. The NRK and VRT are the national public broadcasters in Norway and in Belgium (Flemish). The EBU, NRK and VRT are known in the media community for striving innovation. They have developed recognised expertise in engineering solutions and standards around the management of information for the audiovisual industry in a multi-lingual environment. They promote the use of semantic technologies for the production and distribution of content across a variety of media and platforms. In this context, Wikipedia, DBpedia, automatic metadata extraction and other tools are important information sources. This is not an academic paper but a report on the operational use of such information (access, usability, long term availability of information, editorial quality) and needs by the media industry. For broadcasters, minimizing cost and complexity is of the essence!
</td></tr><tr><td>263</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_3">Statistical Induction of Coupled Domain/Range Restrictions from RDF Knowledge Bases</a></td></tr><tr><td colspan=3>Statistical Schema Induction can be applied on an RDF dataset to induce domain and range restrictions. We extend an existing approach that derives independent domain and range restrictions to derive coupled domain/range restrictions, which may be beneficial in the context of Natural Language Processing tasks such as Semantic Parsing and Entity Classification. We provide results from an experiment on the DBpedia graph. An evaluation shows that high precision can be achieved. Code and data are available at </td></tr><tr><td>264</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_1">Knowledge Graphs: Venturing Out into the Wild</a></td></tr><tr><td colspan=3>While we now have vast collections of knowledge at our disposal, it appears that our systems often need further kinds of knowledge that are still missing in most knowledge graphs. This paper argues that we need keep moving further beyond simple collections of encyclopedic facts. Three key directions are (1) aiming at more tightly integrated knowledge, (2) distilling knowledge from text and other unstructured data, and (3) moving towards cognitive and neural approaches to better exploit the available knowledge in intelligent applications.</td></tr><tr><td>265</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_5">Identifying Global Representative Classes of DBpedia Ontology Through Multilingual Analysis: A Rank Aggregation Approach</a></td></tr><tr><td colspan=3>Identifying the global representative parts from the multilingual pivotal ontology is important for integrating local language resources into Linked Data. We present a novel method of identifying global representative classes of DBpedia ontology based on the collective popularity, calculated by the aggregation of ranking orders from Wikipedia’s local language editions. We publish the contents of this paper on </td></tr><tr><td>266</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_37">Schema-Agnostic Query Rewriting in SPARQL 1.1</a></td></tr><tr><td colspan=3>SPARQL 1.1 supports the use of ontologies to enrich query results with logical entailments, and OWL 2 provides a dedicated fragment OWL QL for this purpose. Typical implementations use the OWL QL schema to rewrite a conjunctive query into an equivalent set of queries, to be answered against the non-schema part of the data. With the adoption of the recent SPARQL 1.1 standard, however, RDF databases are capable of answering much more expressive queries directly, and we ask how this can be exploited in query rewriting. We find that SPARQL 1.1 is powerful enough to “implement” a full-fledged OWL QL reasoner in a single query. Using additional SPARQL 1.1 features, we develop a new method of schema-agnostic query rewriting, where arbitrary conjunctive queries over OWL QL are rewritten into equivalent SPARQL 1.1 queries in a way that is fully independent of the actual schema. This allows us to query RDF data under OWL QL entailment without extracting or preprocessing OWL axioms.</td></tr><tr><td>267</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_33">Ensemble Learning for Named Entity Recognition</a></td></tr><tr><td colspan=3>A considerable portion of the information on the Web is still only available in unstructured form. Implementing the vision of the Semantic Web thus requires transforming this unstructured data into structured data. One key step during this process is the recognition of named entities. Previous works suggest that ensemble learning can be used to improve the performance of named entity recognition tools. However, no comparison of the performance of existing supervised machine learning approaches on this task has been presented so far. We address this research gap by presenting a thorough evaluation of named entity recognition based on ensemble learning. To this end, we combine four different state-of-the approaches by using 15 different algorithms for ensemble learning and evaluate their performace on five different datasets. Our results suggest that ensemble learning can reduce the error rate of state-of-the-art named entity recognition systems by 40%, thereby leading to over 95% f-score in our best run.</td></tr><tr><td>268</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_36">kyrie2: Query Rewriting under Extensional Constraints in </a></td></tr><tr><td colspan=3>In this paper we study query answering and rewriting in ontology-based data access. Specifically, we present an algorithm for computing a perfect rewriting of unions of conjunctive queries posed over ontologies expressed in the description logic </td></tr><tr><td>269</td><td>2016</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-68723-0_2">Information Extraction from the Web by Matching Visual Presentation Patterns</a></td></tr><tr><td colspan=3>The documents available in the World Wide Web contain large amounts of information presented in tables, lists or other visually regular structures. The published information is however usually not annotated explicitly or implicitly and its interpretation is left on a human reader. This makes the information extraction from web documents a challenging problem. Most existing approaches are based on a top-down approach that proceeds from the larger page regions to individual data records, which depends on different heuristics. We present an opposite bottom-up approach. We roughly identify the smallest data fields in the document and later, we refine this approximation by matching the discovered visual presentation patterns with the expected semantic structure of the extracted information. This approach allows to efficiently extract structured data from heterogeneous documents without any kind of additional annotations as we demonstrate experimentally on various application domains.</td></tr><tr><td>270</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_34">OBDA: Query Rewriting or Materialization? In Practice, Both!</a></td></tr><tr><td colspan=3>Given a source relational database, a target OWL ontology and a mapping from the source database to the target ontology, Ontology-Based Data Access (OBDA) concerns answering queries over the target ontology using these three components. This paper presents the development of Ultrawrap</td></tr><tr><td>271</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_30">M-ATOLL: A Framework for the Lexicalization of Ontologies in Multiple Languages</a></td></tr><tr><td colspan=3>Many tasks in which a system needs to mediate between natural language expressions and elements of a vocabulary in an ontology or dataset require knowledge about how the elements of the vocabulary (i.e. classes, properties, and individuals) are expressed in natural language. In a multilingual setting, such knowledge is needed for each of the supported languages. In this paper we present M-ATOLL, a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus. The framework exploits a set of language-specific dependency patterns which are formalized as SPARQL queries and run over a parsed corpus. We have instantiated the system for two languages: German and English. We evaluate it in terms of precision, recall and F-measure for English and German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia. In particular, we investigate the contribution of each single dependency pattern and perform an analysis of the impact of different parameters.</td></tr><tr><td>272</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_31">Towards Efficient and Effective Semantic Table Interpretation</a></td></tr><tr><td colspan=3>This paper describes TableMiner, the first semantic Table Interpretation method that adopts an incremental, mutually recursive and bootstrapping learning approach seeded by automatically selected ‘partial’ data from a table. TableMiner labels columns containing named entity mentions with semantic concepts that best describe data in columns, and disambiguates entity content cells in these columns. TableMiner is able to use various types of contextual information outside tables for Table Interpretation, including semantic markups (e.g., RDFa/microdata annotations) that to the best of our knowledge, have never been used in Natural Language Processing tasks. Evaluation on two datasets shows that compared to two baselines, TableMiner consistently obtains the best performance. In the classification task, it achieves significant improvements of between 0.08 and 0.38 F1 depending on different baseline methods; in the disambiguation task, it outperforms both baselines by between 0.19 and 0.37 in Precision on one dataset, and between 0.02 and 0.03 F1 on the other dataset. Observation also shows that the bootstrapping learning approach adopted by TableMiner can potentially deliver computational savings of between 24 and 60% against classic methods that ‘exhaustively’ processes the entire table content to build features for interpretation.</td></tr><tr><td>273</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_35">Answering SPARQL Queries over Databases under OWL 2 QL Entailment Regime</a></td></tr><tr><td colspan=3>We present an extension of the ontology-based data access platform </td></tr><tr><td>274</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_32">Semano: Semantic Annotation Framework for Natural Language Resources</a></td></tr><tr><td colspan=3>In this paper, we present Semano — a generic framework for annotating natural language texts with entities of OWL 2 DL ontologies. Semano generalizes the mechanism of JAPE transducers that has been introduced within the General Architecture for Text Engineering (GATE) to enable modular development of annotation rule bases. The core of the Semano rule base model are rule templates called </td></tr><tr><td>275</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_29">AGDISTIS - Graph-Based Disambiguation of Named Entities Using Linked Data</a></td></tr><tr><td colspan=3>Over the last decades, several billion Web pages have been made available on the Web. The ongoing transition from the current Web of unstructured data to the Web of Data yet requires scalable and accurate approaches for the extraction of structured data in RDF (Resource Description Framework) from these websites. One of the key steps towards extracting RDF from text is the disambiguation of named entities. While several approaches aim to tackle this problem, they still achieve poor accuracy. We address this drawback by presenting AGDISTIS, a novel knowledge-base-agnostic approach for named entity disambiguation. Our approach combines the Hypertext-Induced Topic Search (HITS) algorithm with label expansion strategies and string similarity measures. Based on this combination, AGDISTIS can efficiently detect the correct URIs for a given set of named entities within an input text. We evaluate our approach on eight different datasets against state-of-the-art named entity disambiguation frameworks. Our results indicate that we outperform the state-of-the-art approach by up to 29% F-measure.</td></tr><tr><td>276</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_28">Updating RDFS ABoxes and TBoxes in SPARQL</a></td></tr><tr><td colspan=3>Updates in RDF stores have recently been standardised in the SPARQL 1.1 Update specification. However, computing entailed answers by ontologies is usually treated orthogonally to updates in triple stores. Even the W3C SPARQL 1.1 Update and SPARQL 1.1 Entailment Regimes specifications explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates. In this paper, we take a first step to close this gap. We define a fragment of SPARQL basic graph patterns corresponding to (the RDFS fragment of) </td></tr><tr><td>277</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_26">A Power Consumption Benchmark for Reasoners on Mobile Devices</a></td></tr><tr><td colspan=3>We introduce a new methodology for benchmarking the performance per watt of semantic web reasoners and rule engines on smartphones to provide developers with information critical for deploying semantic web tools on power-constrained devices. We validate our methodology by applying it to three well-known reasoners and rule engines answering queries on two ontologies with expressivities in RDFS and OWL DL. While this validation was conducted on smartphones running Google’s Android operating system, our methodology is general and may be applied to different hardware platforms, reasoners, ontologies, and entire applications to determine performance relevant to power consumption. We discuss the implications of our findings for balancing tradeoffs of local computation versus communication costs for semantic technologies on mobile platforms, sensor networks, the Internet of Things, and other power-constrained environments.</td></tr><tr><td>278</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_27">Dynamic Provenance for SPARQL Updates</a></td></tr><tr><td colspan=3>While the Semantic Web currently can exhibit provenance information by using the W3C PROV standards, there is a “missing link” in connecting PROV to storing and querying for dynamic changes to RDF graphs using SPARQL. Solving this problem would be required for such clear use-cases as the creation of version control systems for RDF. While some provenance models and annotation techniques for storing and querying provenance data originally developed with databases or workflows in mind transfer readily to RDF and SPARQL, these techniques do not readily adapt to describing changes in dynamic RDF datasets over time. In this paper we explore how to adapt the dynamic copy-paste provenance model of Buneman et al.[2] to RDF datasets that change over time in response to SPARQL updates, how to represent the resulting provenance records themselves as RDF in a manner compatible with W3C PROV, and how the provenance information can be defined by reinterpreting SPARQL updates. The primary contribution of this paper is a semantic framework that enables the semantics of SPARQL Update to be used as the basis for a ‘cut-and-paste’ provenance model in a principled manner.</td></tr><tr><td>279</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_25">A Cross-Platform Benchmark Framework for Mobile Semantic Web Reasoning Engines</a></td></tr><tr><td colspan=3>Semantic Web technologies are used in a variety of domains for their ability to facilitate data integration, as well as enabling expressive, standards-based reasoning. Deploying Semantic Web reasoning processes directly on mobile devices has a number of advantages, including robustness to connectivity loss, more timely results, and reduced infrastructure requirements. At the same time, a number of challenges arise as well, related to mobile platform heterogeneity and limited computing resources. To tackle these challenges, it should be possible to benchmark mobile reasoning performance across different mobile platforms, with rule- and datasets of varying scale and complexity and existing reasoning process flows. To deal with the current heterogeneity of rule formats, a uniform rule- and data-interface on top of mobile reasoning engines should be provided as well. In this paper, we present a cross-platform benchmark framework that supplies 1) a generic, standards-based Semantic Web layer on top of existing mobile reasoning engines; and 2) a benchmark engine to investigate and compare mobile reasoning performance.</td></tr><tr><td>280</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_24">Noisy Type Assertion Detection in Semantic Datasets</a></td></tr><tr><td colspan=3>Semantic datasets provide support to automate many tasks such as decision-making and question answering. However, their performance is always decreased by the noises in the datasets, among which, noisy type assertions play an important role. This problem has been mainly studied in the domain of data mining but not in the semantic web community. In this paper, we study the problem of noisy type assertion detection in semantic web datasets by making use of concept disjointness relationships hidden in the datasets. We transform noisy type assertion detection into multiclass classification of pairs of type assertions which type an individual to two potential disjoint concepts. The multiclass classification is solved by Adaboost with C4.5 as the base classifier. Furthermore, we propose instance-concept compatability metrics based on instance-instance relationships and instance-concept assertions. We evaluate the approach on both synthetic datasets and DBpedia. Our approach effectively detect noisy type assertions in DBpedia with a high precision of 95%.</td></tr><tr><td>281</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_20">Discovery and Visual Analysis of Linked Data for Humans</a></td></tr><tr><td colspan=3>Linked Data has grown to become one of the largest available knowledge bases. Unfortunately, this wealth of data remains inaccessible to those without in-depth knowledge of semantic technologies. We describe a toolchain enabling users without semantic technology background to explore and visually analyse Linked Data. We demonstrate its applicability in scenarios involving data from the Linked Open Data Cloud, and research data extracted from scientific publications. Our focus is on the Web-based front-end consisting of querying and visualisation tools. The performed usability evaluations unveil mainly positive results confirming that the Query Wizard simplifies searching, refining and transforming Linked Data and, in particular, that people using the Visualisation Wizard quickly learn to perform interactive analysis tasks on the resulting Linked Data sets. In making Linked Data analysis effectively accessible to the general public, our tool has been integrated in a number of live services where people use it to analyse, discover and discuss facts with Linked Data.</td></tr><tr><td>282</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_22">Transferring Semantic Categories with Vertex Kernels: Recommendations with SemanticSVD++</a></td></tr><tr><td colspan=3>Matrix Factorisation is a recommendation approach that tries to understand what factors interest a user, based on his past ratings for items (products, movies, songs), and then use this factor information to predict future item ratings. A central limitation of this approach however is that it cannot capture how a user’s tastes have evolved beforehand; thereby ignoring if a user’s preference for a factor is likely to change. One solution to this is to include users’ preferences for semantic (i.e. linked data) categories, however this approach is limited should a user be presented with an item for which he has not rated the semantic categories previously; so called </td></tr><tr><td>283</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_21">Col-Graph: Towards Writable and Scalable Linked Open Data</a></td></tr><tr><td colspan=3>Linked Open Data faces severe issues of scalability, availability and data quality. These issues are observed by data consumers performing federated queries; SPARQL endpoints do not respond and results can be wrong or out-of-date. If a data consumer finds an error, how can she fix it? This raises the issue of the </td></tr><tr><td>284</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_23">Detecting Errors in Numerical Linked Data Using Cross-Checked Outlier Detection</a></td></tr><tr><td colspan=3>Outlier detection used for identifying wrong values in data is typically applied to single datasets to search them for values of unexpected behavior. In this work, we instead propose an approach which combines the outcomes of two independent outlier detection runs to get a more reliable result and to also prevent problems arising from natural outliers which are exceptional values in the dataset but nevertheless correct. Linked Data is especially suited for the application of such an idea, since it provides large amounts of data enriched with hierarchical information and also contains explicit links between instances. In a first step, we apply outlier detection methods to the property values extracted from a single repository, using a novel approach for splitting the data into relevant subsets. For the second step, we exploit </td></tr><tr><td>285</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_18">The WebDataCommons Microdata, RDFa and Microformat Dataset Series</a></td></tr><tr><td colspan=3>In order to support web applications to understand the content of HTML pages an increasing number of websites have started to annotate structured data within their pages using markup formats such as Microdata, RDFa, Microformats. The annotations are used by Google, Yahoo!, Yandex, Bing and Facebook to enrich search results and to display entity descriptions within their applications. In this paper, we present a series of publicly accessible Microdata, RDFa, Microformats datasets that we have extracted from three large web corpora dating from 2010, 2012 and 2013. Altogether, the datasets consist of almost 30 billion RDF quads. The most recent of the datasets contains amongst other data over 211 million product descriptions, 54 million reviews and 125 million postal addresses originating from thousands of websites. The availability of the datasets lays the foundation for further research on integrating and cleansing the data as well as for exploring its utility within different application contexts. As the dataset series covers four years, it can also be used to analyze the evolution of the adoption of the markup formats.</td></tr><tr><td>286</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_16">Adoption of the Linked Data Best Practices in Different Topical Domains</a></td></tr><tr><td colspan=3>The central idea of Linked Data is that data publishers support applications in discovering and integrating data by complying to a set of best practices in the areas of linking, vocabulary usage, and metadata provision. In 2011, the </td></tr><tr><td>287</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_17">Analyzing Schema.org</a></td></tr><tr><td colspan=3>Schema.org is a way to add machine-understandable information to web pages that is processed by the major search engines to improve search performance. The definition of schema.org is provided as a set of web pages plus a partial mapping into RDF triples with unusual properties, and is incomplete in a number of places. This analysis of and formal semantics for schema.org provides a complete basis for a plausible version of what schema.org should be.</td></tr><tr><td>288</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_19">On Publishing Chinese Linked Open Schema</a></td></tr><tr><td colspan=3>Linking Open Data (LOD) is the largest community effort for semantic data publishing which converts the Web from a Web of document to a Web of interlinked knowledge. While the state of the art LOD contains billion of triples describing millions of entities, it has only a limited number of schema information and is lack of schema-level axioms. To close the gap between the lightweight LOD and the expressive ontologies, we contribute to the complementary part of the LOD, that is, Linking Open Schema (LOS). In this paper, we introduce Zhishi.schema, the first effort to publish Chinese linked open schema. We collect navigational categories as well as dynamic tags from more than 50 various most popular social Web sites in China. We then propose a two-stage method to capture equivalence, subsumption and relate relationships between the collected categories and tags, which results in an integrated concept taxonomy and a large semantic network. Experimental results show the high quality of Zhishi.schema. Compared with category systems of DBpedia, Yago, BabelNet, and Freebase, Zhishi.schema has wide coverage of categories and contains the largest number of subsumptions between categories. When substituting Zhishi.schema for the original category system of Zhishi.me, we not only filter out incorrect category subsumptions but also add more finer-grained categories.</td></tr><tr><td>289</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_14">LOD Laundromat: A Uniform Way of Publishing Other People’s Dirty Data</a></td></tr><tr><td colspan=3>It is widely accepted that </td></tr><tr><td>290</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_15">Dutch Ships and Sailors Linked Data</a></td></tr><tr><td colspan=3>We present the Dutch Ships and Sailors Linked Data Cloud. This heterogeneous dataset brings together four curated datasets on Dutch Maritime history as five-star linked data. The individual datasets use separate datamodels, designed in close collaboration with maritime historical researchers. The individual models are mapped to a common interoperability layer, allowing for analysis of the data on the general level. We present the datasets, modeling decisions, internal links and links to external data sources. We show ways of accessing the data and present a number of examples of how the dataset can be used for historical research. The Dutch Ships and Sailors Linked Data Cloud is a potential hub dataset for digital history research and a prime example of the benefits of Linked Data for this field.</td></tr><tr><td>291</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_11">Sempala: Interactive SPARQL Query Processing on Hadoop</a></td></tr><tr><td colspan=3>Driven by initiatives like Schema.org, the amount of semantically annotated data is expected to grow steadily towards massive scale, requiring cluster-based solutions to query it. At the same time, Hadoop has become dominant in the area of Big Data processing with large infrastructures being already deployed and used in manifold application fields. For Hadoop-based applications, a common data pool (HDFS) provides many synergy benefits, making it very attractive to use these infrastructures for semantic data processing as well. Indeed, existing SPARQL-on- Hadoop (MapReduce) approaches have already demonstrated very good scalability, however, query runtimes are rather slow due to the underlying batch processing framework. While this is acceptable for data-intensive queries, it is not satisfactory for the majority of SPARQL queries that are typically much more selective requiring only small subsets of the data. In this paper, we present </td></tr><tr><td>292</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_12">Querying Datasets on the Web with High Availability</a></td></tr><tr><td colspan=3>As the Web of Data is growing at an ever increasing speed, the lack of reliable query solutions for live public data becomes apparent. </td></tr><tr><td>293</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_13">Diversified Stress Testing of RDF Data Management Systems</a></td></tr><tr><td colspan=3>The Resource Description Framework (RDF) is a standard for conceptually describing data on the Web, and SPARQL is the query language for RDF. As RDF data continue to be published across heterogeneous domains and integrated at Web-scale such as in the Linked Open Data (LOD) cloud, RDF data management systems are being exposed to queries that are far more diverse and workloads that are far more varied. The first contribution of our work is an in-depth experimental analysis that shows existing SPARQL benchmarks are not suitable for testing systems for diverse queries and varied workloads. To address these shortcomings, our second contribution is the Waterloo SPARQL Diversity Test Suite (WatDiv) that provides stress testing tools for RDF data management systems. Using WatDiv, we have been able to reveal issues with existing systems that went unnoticed in evaluations using earlier benchmarks. Specifically, our experiments with five popular RDF data management systems show that they cannot deliver good performance uniformly across workloads. For some queries, there can be as much as five orders of magnitude difference between the query execution time of the fastest and the slowest system while the fastest system on one query may unexpectedly time out on another query. By performing a detailed analysis, we pinpoint these problems to specific types of queries and workloads.</td></tr><tr><td>294</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_10">SYRql: A Dataflow Language for Large Scale Processing of RDF Data</a></td></tr><tr><td colspan=3>The recent big data movement resulted in a surge of activity on layering declarative languages on top of distributed computation platforms. In the Semantic Web realm, this surge of analytics languages was not reflected despite the significant growth in the available RDF data. Consequently, when analysing large RDF datasets, users are left with two main options: using SPARQL or using an existing non-RDF-specific big data language, both with its own limitations. The pure declarative nature of SPARQL and the high cost of evaluation can be limiting in some scenarios. On the other hand, existing big data languages are designed mainly for tabular data and, therefore, applying them to RDF data results in verbose, unreadable, and sometimes inefficient scripts. In this paper, we introduce </td></tr><tr><td>295</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_4">Introducing Wikidata to the Linked Data Web</a></td></tr><tr><td colspan=3>Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.</td></tr><tr><td>296</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_5">Web-Scale Extension of RDF Knowledge Bases from Templated Websites</a></td></tr><tr><td colspan=3>Only a small fraction of the information on the Web is represented as Linked Data. This lack of coverage is partly due to the paradigms followed so far to extract Linked Data. While converting structured data to RDF is well supported by tools, most approaches to extract RDF from semi-structured data rely on extraction methods based on ad-hoc solutions. In this paper, we present a holistic and open-source framework for the extraction of RDF from templated websites. We discuss the architecture of the framework and the initial implementation of each of its components. In particular, we present a novel wrapper induction technique that does not require any human supervision to detect wrappers for web sites. Our framework also includes a consistency layer with which the data extracted by the wrappers can be checked for logical consistency. We evaluate the initial version of REX on three different datasets. Our results clearly show the potential of using templated Web pages to extend the Linked Data Cloud. Moreover, our results indicate the weaknesses of our current implementations and how they can be extended.</td></tr><tr><td>297</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_3">SAKey: Scalable Almost Key Discovery in RDF Data</a></td></tr><tr><td colspan=3>Exploiting identity links among RDF resources allows applications to efficiently integrate data. Keys can be very useful to discover these identity links. A set of properties is considered as a key when its values uniquely identify resources. However, these keys are usually not available. The approaches that attempt to automatically discover keys can easily be overwhelmed by the size of the data and require clean data. We present SAKey, an approach that discovers keys in RDF data in an efficient way. To prune the search space, SAKey exploits characteristics of the data that are dynamically detected during the process. Furthermore, our approach can discover keys in datasets where erroneous data or duplicates exist (i.e., almost keys). The approach has been evaluated on different synthetic and real datasets. The results show both the relevance of almost keys and the efficiency of discovering them.</td></tr><tr><td>298</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_6">EPCIS Event-Based Traceability in Pharmaceutical Supply Chains via Automated Generation of Linked Pedigrees</a></td></tr><tr><td colspan=3>In this paper we show how event processing over semantically annotated streams of events can be exploited, for implementing tracing and tracking of products in supply chains through the automated generation of linked pedigrees. In our abstraction, events are encoded as spatially and temporally oriented named graphs, while linked pedigrees as RDF datasets are their specific compositions. We propose an algorithm that operates over streams of RDF annotated EPCIS events to generate linked pedigrees. We exemplify our approach using the pharmaceuticals supply chain and show how counterfeit detection is an implicit part of our pedigree generation. Our evaluation results show that for fast moving supply chains, smaller window sizes on event streams provide significantly higher efficiency in the generation of pedigrees as well as enable early counterfeit detection.</td></tr><tr><td>299</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_9">Drug-Target Interaction Prediction Using Semantic Similarity and Edge Partitioning</a></td></tr><tr><td colspan=3>The ability to integrate a wealth of human-curated knowledge from scientific datasets and ontologies can benefit drug-target interaction prediction. The hypothesis is that similar drugs interact with the same targets, and similar targets interact with the same drugs. The similarities between drugs reflect a chemical semantic space, while similarities between targets reflect a genomic semantic space. In this paper, we present a novel method that combines a data mining framework for link prediction, semantic knowledge (similarities) from ontologies or semantic spaces, and an algorithmic approach to partition the edges of a heterogeneous graph that includes drug-target interaction edges, and drug-drug and target-target similarity edges. Our semantics based edge partitioning approach, semEP, has the advantages of edge based community detection which allows a node to participate in more than one cluster or community. The semEP problem is to create a minimal partitioning of the edges such that the cluster density of each subset of edges is maximal. We use semantic knowledge (similarities) to specify edge constraints, i.e., specific drug-target interaction edges that should not participate in the same cluster. Using a well-known dataset of drug-target interactions, we demonstrate the benefits of using semEP predictions to improve the performance of a range of state-of-the-art machine learning based prediction methods. Validation of the novel best predicted interactions of semEP against the STITCH interaction resource reflect both accurate and diverse predictions.</td></tr><tr><td>300</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_8">Linked Biomedical Dataspace: Lessons Learned Integrating Data for Drug Discovery</a></td></tr><tr><td colspan=3>The increase in the volume and heterogeneity of biomedical data sources has motivated researchers to embrace Linked Data (LD) technologies to solve the ensuing integration challenges and enhance information discovery. As an integral part of the EU GRANATUM project, a Linked Biomedical Dataspace (LBDS) was developed to semantically interlink data from multiple sources and augment the design of </td></tr><tr><td>301</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_7">Scientific Lenses to Support Multiple Views over Linked Chemistry Data</a></td></tr><tr><td colspan=3>When are two entries about a small molecule in different datasets the same? If they have the same drug name, chemical structure, or some other criteria? The choice depends upon the application to which the data will be put. However, existing Linked Data approaches provide a single global view over the data with no way of varying the notion of equivalence to be applied.</td></tr><tr><td>302</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_2">HELIOS – Execution Optimization for Link Discovery</a></td></tr><tr><td colspan=3>Links between knowledge bases build the backbone of the Linked Data Web. In previous works, the combination of the results of time-efficient algorithms through set-theoretical operators has been shown to be very time-efficient for Link Discovery. However, the further optimization of such link specifications has not been paid much attention to. We address the issue of further optimizing the runtime of link specifications by presenting </td></tr><tr><td>303</td><td>2014</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-11964-9_1">CAMO: Integration of Linked Open Data for Multimedia Metadata Enrichment</a></td></tr><tr><td colspan=3>Metadata is a vital factor for effective management, organization and retrieval of multimedia content. In this paper, we introduce CAMO, a new system developed jointly with Samsung to enrich multimedia metadata by integrating Linked Open Data (LOD). Large-scale, heterogeneous LOD sources, e.g., DBpedia, LinkMDB and MusicBrainz, are integrated using ontology matching and instance linkage techniques. A mobile app for Android devices is built on top of the LOD to improve multimedia content browsing. An empirical evaluation is conducted to demonstrate the effectiveness and accuracy of the system in the multimedia domain.</td></tr><tr><td>304</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_36">Explaining and Suggesting Relatedness in Knowledge Graphs</a></td></tr><tr><td colspan=3>Knowledge graphs (KGs) are a key ingredient for searching, browsing and knowledge discovery activities. Motivated by the need to harness knowledge available in a variety of KGs, we face the following two problems. First, given a pair of entities defined in some KG, find an explanation of their relatedness. We formalize the notion of relatedness explanation and introduce different criteria to build explanations based on information-theory, diversity and their combinations. Second, given a pair of entities, find other (pairs of) entities sharing a similar relatedness perspective. We describe an implementation of our ideas in a tool, called </td></tr><tr><td>305</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_37">Type-Constrained Representation Learning in Knowledge Graphs</a></td></tr><tr><td colspan=3>Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data.</td></tr><tr><td>306</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_38">Publishing Without Publishers: A Decentralized Approach to Dissemination, Retrieval, and Archiving of Data</a></td></tr><tr><td colspan=3>Making available and archiving scientific results is for the most part still considered the task of classical publishing companies, despite the fact that classical forms of publishing centered around printed narrative articles no longer seem well-suited in the digital age. In particular, there exist currently no efficient, reliable, and agreed-upon methods for publishing scientific datasets, which have become increasingly important for science. Here we propose to design scientific data publishing as a Web-based bottom-up process, without top-down control of central authorities such as publishing companies. Based on a novel combination of existing concepts and technologies, we present a server network to decentrally store and archive data in the form of nanopublications, an RDF-based format to represent scientific data. We show how this approach allows researchers to publish, retrieve, verify, and recombine datasets of nanopublications in a reliable and trustworthy manner, and we argue that this architecture could be used for the Semantic Web in general. Evaluation of the current small network shows that this system is efficient and reliable.</td></tr><tr><td>307</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_34">Concept Forgetting in </a></td></tr><tr><td colspan=3>We present a method for forgetting concept symbols in ontologies specified in the description logic </td></tr><tr><td>308</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_35">Content-Based Recommendations via DBpedia and Freebase: A Case Study in the Music Domain</a></td></tr><tr><td colspan=3>The </td></tr><tr><td>309</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_30">Interest-Based RDF Update Propagation</a></td></tr><tr><td colspan=3>Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and process large amounts of requests from diverse applications. Many data products and services rely on full or partial local LOD replications to ensure faster querying and processing. Given the evolving nature of the original and authoritative datasets, to ensure consistent and up-to-date replicas frequent replacements are required at a great cost. In this paper, we introduce an approach for interest-based RDF update propagation, which propagates only interesting parts of updates from the source to the target dataset. Effectively, this enables remote applications to ‘subscribe’ to relevant datasets and consistently reflect the necessary changes locally without the need to frequently replace the entire dataset (or a relevant subset). Our approach is based on a formal definition for graph-pattern-based interest expressions that is used to filter interesting parts of updates from the source. We implement the approach in the iRap framework and perform a comprehensive evaluation based on DBpedia Live updates, to confirm the validity and value of our approach.</td></tr><tr><td>310</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_29">A Flexible Framework for Understanding the Dynamics of Evolving RDF Datasets</a></td></tr><tr><td colspan=3>The dynamic nature of Web data gives rise to a multitude of problems related to the description and analysis of the evolution of RDF datasets, which are important to a large number of users and domains, such as, the curators of biological information where changes are constant and interrelated. In this paper, we propose a framework that enables identifying, analysing and understanding these dynamics. Our approach is flexible enough to capture the peculiarities and needs of different applications on dynamic data, while being formally robust due to the satisfaction of the completeness and unambiguity properties. In addition, our framework allows the persistent representation of the detected changes between versions, in a manner that enables easy and efficient navigation among versions, automated processing and analysis of changes, cross-snapshot queries (spanning across different versions), as well as queries involving both changes and data. Our work is evaluated using real Linked Open Data, and exhibits good scalability properties.</td></tr><tr><td>311</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_26">Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation</a></td></tr><tr><td colspan=3>Semantic relatedness and disambiguation are fundamental problems for linking text documents to the Web of Data. There are many approaches dealing with both problems but most of them rely on word or concept distribution over Wikipedia. They are therefore not applicable to concepts that do not have a rich textual description. In this paper, we show that semantic relatedness can also be accurately computed by analysing only the graph structure of the knowledge base. In addition, we propose a joint approach to entity and word-sense disambiguation that makes use of graph-based relatedness. As opposed to the majority of state-of-the-art systems that target mainly named entities, we use our approach to disambiguate both entities and common nouns. In our experiments, we first validate our relatedness measure on multiple knowledge bases and ground truth datasets and show that it performs better than related state-of-the-art graph based measures. Afterwards, we evaluate the disambiguation algorithm and show that it also achieves superior disambiguation accuracy with respect to alternative state-of-the-art graph-based algorithms.</td></tr><tr><td>312</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_33">Next Step for NoHR: OWL 2 QL</a></td></tr><tr><td colspan=3>The Protégé plug-in NoHR allows the user to combine an OWL 2 EL ontology with a set of non-monotonic (logic programming) rules – suitable, e.g., to express defaults and exceptions – and query the combined knowledge base (KB). The formal approach realized in NoHR is polynomial (w.r.t. data complexity) and it has been shown that even very large health care ontologies, such as SNOMED CT, can be handled. As each of the tractable OWL profiles is motivated by different application cases, extending the tool to the other profiles is of particular interest, also because these preserve the polynomial data complexity of the combined formalism. Yet, a straightforward adaptation of the existing approach to OWL 2 QL turns out to not be viable. In this paper, we provide the non-trivial solution for the extension of NoHR to OWL 2 QL by directly translating the ontology into rules without any prior classification. We have implemented our approach and our evaluation shows encouraging results.</td></tr><tr><td>313</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_31">General Terminology Induction in OWL</a></td></tr><tr><td colspan=3>Automated acquisition, or learning, of ontologies has attracted research attention because it can help ontology engineers build ontologies and give domain experts new insights into their data. However, existing approaches to ontology learning are considerably limited, e.g. focus on learning descriptions for given classes, require intense supervision and human involvement, make assumptions about data, do not fully respect background knowledge. We investigate the problem of general terminology induction, i.e. learning sets of general class inclusions, GCIs, from data and background knowledge. We introduce measures that evaluate logical and statistical quality of a set of GCIs. We present methods to compute these measures and an anytime algorithm that induces sets of GCIs. Our experiments show that we can acquire interesting sets of GCIs and provide insights into the structure of the search space.</td></tr><tr><td>314</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_24">Klink-2: Integrating Multiple Web Sources to Generate Semantic Topic Networks</a></td></tr><tr><td colspan=3>The amount of scholarly data available on the web is steadily increasing, enabling different types of analytics which can provide important insights into the research activity. In order to make sense of and explore this large-scale body of knowledge we need an accurate, comprehensive and up-to-date ontology of research topics. Unfortunately, human crafted classifications do not satisfy these criteria, as they evolve too slowly and tend to be too coarse-grained. Current automated methods for generating ontologies of research areas also present a number of limitations, such as: i) they do not consider the rich amount of indirect statistical and semantic relationships, which can help to understand the relation between two topics – e.g., the fact that two research areas are associated with a similar set of venues or technologies; ii) they do not distinguish between different kinds of hierarchical relationships; and iii) they are not able to handle effectively ambiguous topics characterized by a noisy set of relationships. In this paper we present Klink-2, a novel approach which improves on our earlier work on automatic generation of semantic topic networks and addresses the aforementioned limitations by taking advantage of a variety of knowledge sources available on the web. In particular, Klink-2 analyses networks of research entities (including papers, authors, venues, and technologies) to infer three kinds of semantic relationships between topics. It also identifies ambiguous keywords (e.g., “ontology”) and separates them into the appropriate distinct topics – e.g., “ontology/philosophy” vs. “ontology/semantic web”. Our experimental evaluation shows that the ability of Klink-2 to integrate a high number of data sources and to generate topics with accurate contextual meaning yields significant improvements over other algorithms in terms of both precision and recall.</td></tr><tr><td>315</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_25">TabEL: Entity Linking in Web Tables</a></td></tr><tr><td colspan=3>Web tables form a valuable source of relational data. The Web contains an estimated 154 million HTML tables of relational data, with Wikipedia alone containing 1.6 million high-quality tables. Extracting the semantics of Web tables to produce machine-understandable knowledge has become an active area of research.</td></tr><tr><td>316</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_27">SANAPHOR: Ontology-Based Coreference Resolution</a></td></tr><tr><td colspan=3>We tackle the problem of resolving coreferences in textual content by leveraging Semantic Web techniques. Specifically, we focus on noun phrases that coreference identifiable entities that appear in the text; the challenge in this context is to improve the coreference resolution by leveraging potential semantic annotations that can be added to the identified mentions. Our system, </td></tr><tr><td>317</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_22"> Piercing to the Heart of Instance Matching Tools</a></td></tr><tr><td colspan=3>One of the main challenges in the Data Web is the identification of instances that refer to the same real-world entity. Choosing the right framework for this purpose remains tedious, as current instance matching benchmarks fail to provide end users and developers with the necessary insights pertaining to how current frameworks behave when dealing with real data. In this paper, we present </td></tr><tr><td>318</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_32">Understanding How Users Edit Ontologies: Comparing Hypotheses About Four Real-World Projects</a></td></tr><tr><td colspan=3>Ontologies are complex intellectual artifacts and creating them requires significant expertise and effort. While existing ontology-editing tools and methodologies propose ways of building ontologies in a normative way, empirical investigations of how experts </td></tr><tr><td>319</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_23">Decision-Making Bias in Instance Matching Model Selection</a></td></tr><tr><td colspan=3>Instance matching has emerged as an important problem in the Semantic Web, with machine learning methods proving especially effective. To enhance performance, task-specific knowledge is typically used to introduce bias in the model selection problem. Such biases tend to be exploited by practitioners in a piecemeal fashion. This paper introduces a framework where the model selection design process is represented as a factor graph. Nodes in this bipartite graphical model represent opportunities for explicitly introducing bias. The graph is first used to unify and visualize common biases in the design of existing instance matchers. As a direct application, we then use the graph to hypothesize about potential unexploited biases. The hypotheses are evaluated by training 1032 neural networks on three instance matching tasks on Microsoft Azure’s cloud-based platform. An analysis over 25 GB of experimental data indicates that the proposed biases can improve efficiency by over 65% over a baseline configuration, with effectiveness improving by a smaller margin. The findings lead to a promising set of four recommendations that can be integrated into existing supervised instance matchers.</td></tr><tr><td>320</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_21">Optimizing the Computation of Overriding</a></td></tr><tr><td colspan=3>We introduce optimization techniques for reasoning in </td></tr><tr><td>321</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_28">Improving Entity Retrieval on Structured Data</a></td></tr><tr><td colspan=3>The increasing amount of data on the Web, in particular of Linked Data, has led to a diverse landscape of datasets, which make entity retrieval a challenging task. Explicit cross-dataset links, for instance to indicate co-references or related entities can significantly improve entity retrieval. However, only a small fraction of entities are interlinked through explicit statements. In this paper, we propose a two-fold entity retrieval approach. In a first, offline preprocessing step, we cluster entities based on the </td></tr><tr><td>322</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_18">Adding </a></td></tr><tr><td colspan=3>Levesque’s proper knowledge bases (proper KBs) correspond to infinite sets of ground positive and negative facts, with the notable property that for FOL formulas in a certain normal form, which includes conjunctive queries and positive queries possibly extended with a controlled form of negation, entailment reduces to formula evaluation. However proper KBs represent extensional knowledge only. In description logic terms, they correspond to ABoxes. In this paper, we augment them with </td></tr><tr><td>323</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_20">Rewriting-Based Instance Retrieval for Negated Concepts in Description Logic Ontologies</a></td></tr><tr><td colspan=3>Instance retrieval computes all instances of a given concept in a consistent description logic (DL) ontology. Although it is a popular task for ontology reasoning, there is no scalable method for instance retrieval for negated concepts by now. This paper studies a new approach to instance retrieval for negated concepts based on query rewriting. A class of DL ontologies called the </td></tr><tr><td>324</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_19">R</a></td></tr><tr><td colspan=3>It has been shown, both theoretically and empirically, that performing core reasoning tasks on large and expressive ontologies in OWL 1 and OWL 2 is time-consuming and resource-intensive. Moreover, due to the different reasoning algorithms and optimisation techniques employed, each reasoner may be efficient for ontologies with different characteristics. In this paper, we present R</td></tr><tr><td>325</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_14">Towards Defeasible Mappings for Tractable Description Logics</a></td></tr><tr><td colspan=3>We present a novel approach to denote mappings between </td></tr><tr><td>326</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_15">An Algebra of Qualitative Taxonomical Relations for Ontology Alignments</a></td></tr><tr><td colspan=3>Algebras of relations were shown useful in managing ontology alignments. They make it possible to aggregate alignments disjunctively or conjunctively and to propagate alignments within a network of ontologies. The previously considered algebra of relations contains taxonomical relations between classes. However, compositional inference using this algebra is sound only if we assume that classes which occur in alignments have nonempty extensions. Moreover, this algebra covers relations only between classes. Here we introduce a new algebra of relations, which, first, solves the limitation of the previous one, and second, incorporates all qualitative taxonomical relations that occur between individuals and concepts, including the relations “is a” and “is not”. We prove that this algebra is coherent with respect to the simple semantics of alignments.</td></tr><tr><td>327</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_10">Facilitating Entity Navigation Through Top-K Link Patterns</a></td></tr><tr><td colspan=3>Entity navigation over Linked Data often follows semantic links by using Linked Data browsers. With the increasing volume of Linked Data, the rich and diverse links make it difficult for users to traverse the link graph and find target entities. Besides, there is a necessity for navigation paradigm to take into account not only single-entity-oriented transition, but also entity-set-oriented transition. To facilitate entity navigation, we propose a novel concept called link pattern, and introduce link pattern lattice to organize semantic links when browsing an entity or a set of entities. Furthermore, to help users quickly find target entities, top-K link patterns are selected for entity navigation. The proposed approach is implemented in a prototype system and then compared with two Linked Data browsers via a user study. Experimental results show that our approach is effective.</td></tr><tr><td>328</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_13">Mapping Analysis in Ontology-Based Data Access: Algorithms and Complexity</a></td></tr><tr><td colspan=3>Ontology-based data access (OBDA) is a recent paradigm for accessing data sources through an ontology that acts as a conceptual, integrated view of the data, and declarative mappings that connect the ontology to the data sources. We study the formal analysis of mappings in OBDA. Specifically, we focus on the problem of identifying mapping inconsistency and redundancy, two of the most important anomalies for mappings in OBDA. We consider a wide range of ontology languages that comprises OWL 2 and all its profiles, and examine mapping languages of different expressiveness over relational databases. We provide algorithms and establish tight complexity bounds for the decision problems associated with mapping inconsistency and redundancy. Our results prove that, in our general framework, such forms of mapping analysis enjoy nice computational properties, in the sense that they are not harder than standard reasoning tasks over the ontology or over the relational database schema.</td></tr><tr><td>329</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_12">Ontology-Based Integration of Cross-Linked Datasets</a></td></tr><tr><td colspan=3>In this paper we tackle the problem of answering SPARQL queries over </td></tr><tr><td>330</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_9">LinkDaViz – Automatic Binding of Linked Data to Visualizations</a></td></tr><tr><td colspan=3>As the Web of Data is growing steadily, the demand for user-friendly means for exploring, analyzing and visualizing Linked Data is also increasing. The key challenge for visualizing Linked Data consists in providing a clear overview of the data and supporting non-technical users in finding suitable visualizations while hiding technical details of Linked Data and visualization configuration. In order to accomplish this, we propose a largely automatic workflow which guides users through the process of creating visualizations by automatically categorizing and binding data to visualization parameters. The approach is based on a heuristic analysis of the structure of the input data and a comprehensive visualization model facilitating the automatic binding between data and visualization parameters. The resulting assignments are ranked and presented to the user. With LinkDaViz we provide a web-based implementation of the approach and demonstrate the feasibility by an extended user and performance evaluation.</td></tr><tr><td>331</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_11">Serving DBpedia with DOLCE – More than Just Adding a Cherry on Top</a></td></tr><tr><td colspan=3>Large knowledge bases, such as DBpedia, are most often created heuristically due to scalability issues. In the building process, both random as well as systematic errors may occur. In this paper, we focus on finding </td></tr><tr><td>332</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_8">Substring Filtering for Low-Cost Linked Data Interfaces</a></td></tr><tr><td colspan=3>Recently, Triple Pattern Fragments (</td></tr><tr><td>333</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_16">CogMap: A Cognitive Support Approach to Property and Instance Alignment</a></td></tr><tr><td colspan=3>The iterative user interaction approach for data integration proposed by Falconer and Noy can be generalized to consider interactions between integration tools (generators) that generate potential schema mappings and users or analysis tools (analyzers) that select the best mapping. Each such selection then provides high-confidence guidance for the next iteration of the integration tool. We have implemented this generalized approach in </td></tr><tr><td>334</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_7">Networks of Linked Data Eddies: An Adaptive Web Query Processing Engine for RDF Data</a></td></tr><tr><td colspan=3>Client-side query processing techniques that rely on the materialization of fragments of the original RDF dataset provide a promising solution for Web query processing. However, because of unexpected data transfers, the traditional optimize-then-execute paradigm, used by existing approaches, is not always applicable in this context, i.e., performance of client-side execution plans can be negatively affected by live conditions where rate at which data arrive from sources changes. We tackle adaptivity for client-side query processing, and present a network of Linked Data Eddies that is able to adjust query execution schedulers to data availability and runtime conditions. Experimental studies suggest that the network of Linked Data Eddies outperforms static Web query schedulers in scenarios with unpredictable transfer delays and data distributions.</td></tr><tr><td>335</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_3">Federated SPARQL Queries Processing with Replicated Fragments</a></td></tr><tr><td colspan=3>Federated query engines provide a unified query interface to federations of SPARQL endpoints. Replicating data fragments from different Linked Data sources facilitates data re-organization to better fit federated query processing needs of data consumers. However, existing federated query engines are not designed to support replication and replicated data can negatively impact their performance. In this paper, we formulate the source selection problem with fragment replication (SSP-FR). For a given set of endpoints with replicated fragments and a SPARQL query, the problem is to select the endpoints that minimize the number of tuples to be transferred. We devise the </td></tr><tr><td>336</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_2">Recursion in SPARQL</a></td></tr><tr><td colspan=3>In this paper we propose a general purpose recursion operator to be added to SPARQL, formalize its syntax and develop algorithms for evaluating it in practical scenarios. We also show how to implement recursion as a plug-in on top of existing systems and test its performance on several real world datasets.</td></tr><tr><td>337</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_6">Opportunistic Linked Data Querying Through Approximate Membership Metadata</a></td></tr><tr><td colspan=3>Between </td></tr><tr><td>338</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_5">LDQL: A Query Language for the Web of Linked Data</a></td></tr><tr><td colspan=3>The Web of Linked Data is composed of tons of RDF documents interlinked to each other forming a huge repository of distributed semantic data. Effectively querying this distributed data source is an important open problem in the Semantic Web area. In this paper, we propose LDQL, a declarative language to query Linked Data on the Web. One of the novelties of LDQL is that it expresses separately (i) patterns that describe the expected query result, and (ii)Web navigation paths that select the data sources to be used for computing the result. We present a formal syntax and semantics, prove equivalence rules, and study the expressiveness of the language. In particular, we show that LDQL is strictly more expressive than the query formalisms that have been proposed previously for Linked Data on the Web. The high expressiveness allows LDQL to define queries for which a complete execution is not computationally feasible over the Web. We formally study this issue and provide a syntactic sufficient condition to avoid this problem; queries satisfying this condition are ensured to have a procedure to be effectively evaluated over the Web of Linked Data.</td></tr><tr><td>339</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_4">FEASIBLE: A Feature-Based SPARQL Benchmark Generation Framework</a></td></tr><tr><td colspan=3>Benchmarking is indispensable when aiming to assess technologies with respect to their suitability for given tasks. While several benchmarks and benchmark generation frameworks have been developed to evaluate triple stores, they mostly provide a one-fits-all solution to the benchmarking problem. This approach to benchmarking is however unsuitable to evaluate the performance of a triple store for a given application with particular requirements. We address this drawback by presenting FEASIBLE, an automatic approach for the generation of benchmarks out of the query history of applications, i.e., query logs. The generation is achieved by selecting prototypical queries of a user-defined size from the input set of queries. We evaluate our approach on two query logs and show that the benchmarks it generates are accurate approximations of the input query logs. Moreover, we compare four different triple stores with benchmarks generated using our approach and show that they behave differently based on the data they contain and the types of queries posed. Our results suggest that FEASIBLE generates better sample queries than the state of the art. In addition, the better query selection and the larger set of query types used lead to triple store rankings which partly differ from the rankings generated by previous works.</td></tr><tr><td>340</td><td>2015</td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-25007-6_1">SPARQL with Property Paths</a></td></tr><tr><td colspan=3>The original SPARQL proposal was often criticized for its inability to navigate through the structure of RDF documents. For this reason property paths were introduced in SPARQL 1.1, but up to date there are no theoretical studies examining how their addition to the language affects main computational tasks such as query evaluation, query containment, and query subsumption. In this paper we tackle all of these problems and show that although the addition of property paths has no impact on query evaluation, they do make the containment and subsumption problems substantially more difficult.</td></tr></table>
      </body>
    </html>.
    